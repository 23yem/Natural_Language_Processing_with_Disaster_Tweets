{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"},{"sourceId":8616390,"sourceType":"datasetVersion","datasetId":5157167}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-11T22:52:06.332530Z","iopub.execute_input":"2024-06-11T22:52:06.333285Z","iopub.status.idle":"2024-06-11T22:52:06.743524Z","shell.execute_reply.started":"2024-06-11T22:52:06.333252Z","shell.execute_reply":"2024-06-11T22:52:06.742221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport os\nimport re\nfrom transformers import set_seed, BertTokenizer, TFBertForSequenceClassification, BertConfig\nimport tensorflow as tf\n%pip install evaluate\nimport evaluate\nimport optuna\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport json\n\n# Install necessary packages for Azure SQL connection\n%pip install mysql-connector-python \n%pip install PyMySQL\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\nrandom.seed(42)\ntf.random.set_seed(42)\nset_seed(42)\nos.environ['TF_DETERMINISTIC_OPS'] = '1'\n\n# Load the training data\ntrain_data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\nkaggle_test_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n\n# Split the data into 75% training and 25% validation sets\ntrain_data, val_data = train_test_split(train_data, test_size=0.25, random_state=42, stratify=train_data['target'])\n\n# Clean the text data\ndef clean_text(text):\n    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n    text = re.sub(r'@\\w+', '', text)     # Remove mentions\n    text = re.sub(r'\\d+', '', text)      # Remove numbers\n    text = re.sub(r'[^\\w\\s#]', '', text)  # Remove punctuation except hashtags\n    text = text.lower()                  # Convert to lowercase\n    return text\n\ntrain_data['clean_text'] = train_data['text'].apply(clean_text)\nval_data['clean_text'] = val_data['text'].apply(clean_text)\nkaggle_test_data['clean_text'] = kaggle_test_data['text'].apply(clean_text)\n\n# Function to combine keyword and text\ndef combine_keyword_and_text(row):\n    keyword = str(row['keyword']) if pd.notna(row['keyword']) else ''\n    text = row['clean_text']\n    return '[CLS] ' + keyword + ' [SEP] ' + text + ' [SEP]'\n\n# Apply the function to combine keyword and text\ntrain_data['combined_text'] = train_data.apply(combine_keyword_and_text, axis=1)\nval_data['combined_text'] = val_data.apply(combine_keyword_and_text, axis=1)\nkaggle_test_data['combined_text'] = kaggle_test_data.apply(combine_keyword_and_text, axis=1)\n\n# Tokenize the text data\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef tokenize_texts(texts):\n    return tokenizer(\n        texts.tolist(),\n        max_length=64,\n        padding=True,\n        truncation=True,\n        return_tensors='tf'\n    )\n\n# Encode the combined text data\ntrain_encodings = tokenize_texts(train_data['combined_text'])\nval_encodings = tokenize_texts(val_data['combined_text'])\nkaggle_test_encodings = tokenize_texts(kaggle_test_data['combined_text'])\n\ntrain_labels = tf.convert_to_tensor(train_data['target'].values)\nval_labels = tf.convert_to_tensor(val_data['target'].values)\n\n# Load the F1 metric from the evaluate library\nmetric = evaluate.load(\"f1\", trust_remote_code=True)\n\ndef compute_metrics(predictions, labels):\n    predictions = np.argmax(predictions, axis=1)\n    f1 = metric.compute(predictions=predictions, references=labels)['f1']\n    accuracy = accuracy_score(labels, predictions)\n    return {'f1': f1, 'accuracy': accuracy}\n\ndef create_tf_dataset(encodings, labels, batch_size):\n    dataset = tf.data.Dataset.from_tensor_slices((encodings, labels))\n    return dataset.shuffle(10000).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n\n# Define precision and recall metrics outside of the custom metric function\nprecision = tf.keras.metrics.Precision()\nrecall = tf.keras.metrics.Recall()\n\ndef f1_score(y_true, y_pred):\n    # Convert logits to predicted labels\n    y_pred = tf.argmax(y_pred, axis=1)\n    \n    # Ensure true labels are in integer format\n    y_true = tf.cast(y_true, tf.int64)\n    \n    # Update the state of precision and recall\n    precision.update_state(y_true, y_pred)\n    recall.update_state(y_true, y_pred)\n    \n    # Compute precision and recall values\n    precision_result = precision.result()\n    recall_result = recall.result()\n    \n    # Compute F1 score\n    f1 = 2 * ((precision_result * recall_result) / (precision_result + recall_result + tf.keras.backend.epsilon()))\n    \n    return f1\n\nstrategy = tf.distribute.MirroredStrategy()\n\n# Directory to save models\nmodel_save_dir = './saved_models'\nos.makedirs(model_save_dir, exist_ok=True)\n\n# Track top 3 models\ntop_n_models = []\n\ndef objective(trial):\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True)\n    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n    num_epochs = trial.suggest_int(\"num_epochs\", 3, 10)\n    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n    weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.1)\n    lr_scheduler_type = trial.suggest_categorical(\"lr_scheduler_type\", [\"constant\", \"linear\", \"cosine\", \"cosine_with_restarts\"])\n\n    num_gpus = strategy.num_replicas_in_sync\n\n    train_dataset = create_tf_dataset(dict(train_encodings), train_labels, batch_size // num_gpus)\n    val_dataset = create_tf_dataset(dict(val_encodings), val_labels, batch_size // num_gpus)\n    kaggle_test_dataset = tf.data.Dataset.from_tensor_slices(dict(kaggle_test_encodings)).batch(batch_size // num_gpus).prefetch(tf.data.experimental.AUTOTUNE)\n\n    with strategy.scope():\n        config = BertConfig.from_pretrained('bert-base-uncased', num_labels=2, hidden_dropout_prob=dropout_rate)\n        model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n\n        if lr_scheduler_type == \"linear\":\n            lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n                initial_learning_rate=learning_rate,\n                decay_steps=10000,\n                end_learning_rate=0.0,\n                power=1.0\n            )\n        elif lr_scheduler_type == \"cosine\":\n            lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n                initial_learning_rate=learning_rate,\n                decay_steps=10000\n            )\n        elif lr_scheduler_type == \"cosine_with_restarts\":\n            lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n                initial_learning_rate=learning_rate,\n                first_decay_steps=1000\n            )\n        else:\n            lr_schedule = learning_rate\n\n        optimizer = tf.keras.optimizers.experimental.AdamW(\n            learning_rate=lr_schedule,\n            weight_decay=weight_decay,\n            epsilon=1e-8\n        )\n\n        model.compile(optimizer=optimizer, \n                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n                      metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy'), f1_score])\n\n    model.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset, verbose=1)\n\n    # Evaluate on validation set\n    val_loss, val_accuracy, val_f1_score = model.evaluate(val_dataset, verbose=1)\n    print(f\"f1 score: {val_f1_score} and accuracy: {val_accuracy}\")\n    \n    avg_score = (val_accuracy + val_f1_score) / 2\n\n    if len(top_n_models) < 3 or avg_score > min(top_n_models, key=lambda x: x[1])[1]:  # Top-3 method\n        model_save_path = os.path.join(model_save_dir, f\"{studyName}_model_trial_{trial.number}_avg_score_{avg_score:.4f}\")\n        model.save(model_save_path, save_format=\"tf\")\n        top_n_models.append((trial.number, avg_score))\n        top_n_models.sort(key=lambda x: x[1], reverse=True)\n        if len(top_n_models) > 3:\n            top_n_models.pop()\n\n        # Fine-tune the model on the validation dataset\n        fine_tune_encodings = tokenize_texts(val_data['combined_text'])\n        fine_tune_labels = tf.convert_to_tensor(val_data['target'].values)\n        fine_tune_dataset = tf.data.Dataset.from_tensor_slices((\n            dict(fine_tune_encodings),\n            fine_tune_labels\n        )).batch(batch_size // num_gpus).prefetch(tf.data.experimental.AUTOTUNE)\n\n        # Calculate the ratio of training data size to epochs\n        training_data_size = len(train_data)\n        fine_tune_data_size = len(val_data)\n        fine_tune_epochs = max(1, round((fine_tune_data_size / training_data_size) * num_epochs))\n\n        model.fit(fine_tune_dataset, epochs=fine_tune_epochs, verbose=1)\n\n        # Make predictions on the Kaggle test dataset\n        kaggle_test_predictions = model.predict(kaggle_test_dataset).logits\n        kaggle_test_predicted_labels = tf.argmax(kaggle_test_predictions, axis=1).numpy()\n\n        # Create a submission DataFrame\n        submission = pd.DataFrame({'id': kaggle_test_data['id'], 'target': kaggle_test_predicted_labels})\n        \n        # Save the submission\n        submission_file = f\"{studyName}_model_trial_{trial.number}_avg_score_{avg_score:.4f}_f1_{val_f1_score}_accuracy_{val_accuracy}\" + '_submission.csv'  # Corrected naming convention\n        submission_path = os.path.join(model_save_dir, submission_file)\n        submission.to_csv(submission_path, index=False)\n        print(f\"Predictions saved for model: {model_save_path}\")\n\n    return avg_score\n\n# Define your Optuna study, using the MySQL connection string\noptuna_storage = 'mysql+pymysql://<username>:<password>@<host>/<database>?ssl_ca=<path_to_CA_cert>&ssl_verify_cert=true'\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\ndb_password = user_secrets.get_secret(\"DB_PASSWORD\")# This uses the secrets inside of Kaggle so I don't have to explicitly type my password out in code\n\n# Example with your details (replace '<password>' with your real password and '<database>' with your database name)\noptuna_storage = f'mysql+pymysql://MichaelAzure:{db_password}@kaggle-third-sql.mysql.database.azure.com/kaggle_disaster_database?ssl_ca=/kaggle/input/certification&ssl_verify_cert=true'\n\nstudyName = 'disaster_keyword_6'\nstudy = optuna.create_study(study_name=studyName, # name of the study\n                            storage=optuna_storage,  # URL for the mySQL schema\n                            direction='maximize', # maximize the log loss\n                            load_if_exists=True, # makes it so that if the study_name already exists in the schema, then it will append the new trials with the old trials and essentially resume the study. It will also remember the previous trials so it really is resuming the study\n                            )\n\nstudy.optimize(objective, n_trials=10)\n\nprint(\"Best trial:\")\ntrial = study.best_trial\nprint(f\"  Value: {trial.value}\")\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(f\"    {key}: {value}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-11T22:52:06.746370Z","iopub.execute_input":"2024-06-11T22:52:06.746901Z","iopub.status.idle":"2024-06-12T02:08:03.059053Z","shell.execute_reply.started":"2024-06-11T22:52:06.746865Z","shell.execute_reply":"2024-06-12T02:08:03.056387Z"},"trusted":true},"execution_count":null,"outputs":[]}]}