{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-07T23:32:55.902120Z","iopub.status.busy":"2024-06-07T23:32:55.901522Z","iopub.status.idle":"2024-06-07T23:32:56.266673Z","shell.execute_reply":"2024-06-07T23:32:56.265680Z","shell.execute_reply.started":"2024-06-07T23:32:55.902089Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-07T23:32:56.269147Z","iopub.status.busy":"2024-06-07T23:32:56.268693Z","iopub.status.idle":"2024-06-08T01:01:56.522743Z","shell.execute_reply":"2024-06-08T01:01:56.520761Z","shell.execute_reply.started":"2024-06-07T23:32:56.269113Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import random\n","import os\n","import re\n","from transformers import set_seed, BertTokenizer, TFBertForSequenceClassification, BertConfig\n","import tensorflow as tf\n","%pip install evaluate\n","import evaluate\n","import optuna\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.metrics import accuracy_score\n","import json\n","\n","# Install necessary packages for Azure SQL connection\n","%pip install mysql-connector-python \n","%pip install PyMySQL\n","\n","# Set random seeds for reproducibility\n","np.random.seed(42)\n","random.seed(42)\n","tf.random.set_seed(42)\n","set_seed(42)\n","os.environ['TF_DETERMINISTIC_OPS'] = '1'\n","\n","# Load the training data\n","train_data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n","kaggle_test_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n","\n","# Split the data into 70% training and 30% validation/test sets\n","train_data, val_test_data = train_test_split(train_data, test_size=0.3, random_state=42)\n","\n","# Split the 30% validation/test set into 50% validation and 50% test sets\n","val_data, split_test_data = train_test_split(val_test_data, test_size=0.5, random_state=42)\n","\n","# Clean the text data\n","def clean_text(text):\n","    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n","    text = re.sub(r'@\\w+', '', text)     # Remove mentions\n","    text = re.sub(r'\\d+', '', text)      # Remove numbers\n","    text = re.sub(r'[^\\w\\s#]', '', text)  # Remove punctuation except hashtags\n","    text = text.lower()                  # Convert to lowercase\n","    return text\n","\n","\n","train_data['clean_text'] = train_data['text'].apply(clean_text)\n","val_data['clean_text'] = val_data['text'].apply(clean_text)\n","split_test_data['clean_text'] = split_test_data['text'].apply(clean_text)\n","kaggle_test_data['clean_text'] = kaggle_test_data['text'].apply(clean_text)\n","\n","# Tokenize the text data\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","def tokenize_texts(texts):\n","    return tokenizer(\n","        texts.tolist(),\n","        max_length=64,\n","        padding=True,\n","        truncation=True,\n","        return_tensors='tf'\n","    )\n","\n","# train_encodings = tokenize_texts(train_data['clean_text'])\n","# val_encodings = tokenize_texts(val_data['clean_text'])\n","#split_test_encodings = tokenize_texts(split_test_data['clean_text'])\n","kaggle_test_encodings = tokenize_texts(kaggle_test_data['clean_text'])\n","\n","#train_labels = tf.convert_to_tensor(train_data['target'].values)\n","#val_labels = tf.convert_to_tensor(val_data['target'].values)\n","#split_test_labels = tf.convert_to_tensor(split_test_data['target'].values)\n","\n","# Load the F1 metric from the evaluate library\n","metric = evaluate.load(\"f1\", trust_remote_code=True)\n","\n","def compute_metrics(predictions, labels):\n","    predictions = np.argmax(predictions, axis=1)\n","    f1 = metric.compute(predictions=predictions, references=labels)['f1']\n","    accuracy = accuracy_score(labels, predictions)\n","    return {'f1': f1, 'accuracy': accuracy}\n","\n","def create_tf_dataset(encodings, labels, batch_size):\n","    dataset = tf.data.Dataset.from_tensor_slices((encodings, labels))\n","    return dataset.shuffle(10000).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","# Define precision and recall metrics outside of the custom metric function\n","precision = tf.keras.metrics.Precision()\n","recall = tf.keras.metrics.Recall()\n","\n","def f1_score(y_true, y_pred):\n","    # Convert logits to predicted labels\n","    y_pred = tf.argmax(y_pred, axis=1)\n","    \n","    # Ensure true labels are in integer format\n","    y_true = tf.cast(y_true, tf.int64)\n","    \n","    # Update the state of precision and recall\n","    precision.update_state(y_true, y_pred)\n","    recall.update_state(y_true, y_pred)\n","    \n","    # Compute precision and recall values\n","    precision_result = precision.result()\n","    recall_result = recall.result()\n","    \n","    # Compute F1 score\n","    f1 = 2 * ((precision_result * recall_result) / (precision_result + recall_result + tf.keras.backend.epsilon()))\n","    \n","    return f1\n","\n","strategy = tf.distribute.MirroredStrategy()\n","\n","# Directory to save models\n","model_save_dir = './saved_models'\n","os.makedirs(model_save_dir, exist_ok=True)\n","\n","# Track top 3 models\n","top_n_models = []\n","\n","def objective(trial):\n","    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True)\n","    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n","    num_epochs = trial.suggest_int(\"num_epochs\", 2, 8)\n","    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n","    weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.1)\n","    lr_scheduler_type = trial.suggest_categorical(\"lr_scheduler_type\", [\"constant\", \"linear\", \"cosine\", \"cosine_with_restarts\"])\n","\n","    # Ensure batch_size is evenly divisible by the number of GPUs\n","    num_gpus = strategy.num_replicas_in_sync\n","    if batch_size % num_gpus != 0:\n","        raise optuna.exceptions.TrialPruned(f\"Batch size {batch_size} not divisible by number of GPUs {num_gpus}\")\n","\n","    kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n","    fold_scores = []\n","\n","    combined_train_val_data = pd.concat([train_data, val_data])\n","    combined_encodings = tokenize_texts(combined_train_val_data['clean_text'])\n","    combined_labels = tf.convert_to_tensor(combined_train_val_data['target'].values)\n","    \n","\n","    for fold, (train_index, val_index) in enumerate(kfold.split(combined_train_val_data)):\n","        train_fold_data = combined_train_val_data.iloc[train_index]\n","        val_fold_data = combined_train_val_data.iloc[val_index]\n","\n","        train_encodings_fold = tokenize_texts(train_fold_data['clean_text'])\n","        val_encodings_fold = tokenize_texts(val_fold_data['clean_text'])\n","        split_test_encodings = tokenize_texts(split_test_data['clean_text'])\n","\n","        train_labels_fold = tf.convert_to_tensor(train_fold_data['target'].values)\n","        val_labels_fold = tf.convert_to_tensor(val_fold_data['target'].values)\n","        split_test_labels = tf.convert_to_tensor(split_test_data['target'].values)\n","\n","\n","        train_dataset_fold = tf.data.Dataset.from_tensor_slices((\n","            dict(train_encodings_fold),\n","            train_labels_fold\n","        )).batch(batch_size // num_gpus).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","        val_dataset_fold = tf.data.Dataset.from_tensor_slices((\n","            dict(val_encodings_fold),\n","            val_labels_fold\n","        )).batch(batch_size // num_gpus).prefetch(tf.data.experimental.AUTOTUNE)\n","        \n","        split_test_dataset = tf.data.Dataset.from_tensor_slices((\n","            dict(split_test_encodings),\n","            split_test_labels\n","        )).batch(batch_size // num_gpus).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","        with strategy.scope():\n","            config = BertConfig.from_pretrained('bert-base-uncased', num_labels=2, hidden_dropout_prob=dropout_rate)\n","            model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n","\n","            if lr_scheduler_type == \"linear\":\n","                lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n","                    initial_learning_rate=learning_rate,\n","                    decay_steps=10000,\n","                    end_learning_rate=0.0,\n","                    power=1.0\n","                )\n","            elif lr_scheduler_type == \"cosine\":\n","                lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n","                    initial_learning_rate=learning_rate,\n","                    decay_steps=10000\n","                )\n","            elif lr_scheduler_type == \"cosine_with_restarts\":\n","                lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n","                    initial_learning_rate=learning_rate,\n","                    first_decay_steps=1000\n","                )\n","            else:\n","                lr_schedule = learning_rate\n","\n","            optimizer = tf.keras.optimizers.experimental.AdamW(\n","                learning_rate=lr_schedule,\n","                weight_decay=weight_decay,\n","                epsilon=1e-8\n","            )\n","\n","            model.compile(optimizer=optimizer, \n","                          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n","                          metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy'), f1_score])\n","\n","        model.fit(train_dataset_fold, epochs=num_epochs, validation_data=val_dataset_fold, verbose=1)\n","\n","        predictions = model.predict(split_test_dataset).logits\n","        y_test_fold = np.concatenate([y.numpy() for _, y in split_test_dataset], axis=0)\n","        metrics = compute_metrics(predictions, y_test_fold)\n","        f1 = metrics['f1']\n","        accuracy = metrics['accuracy']\n","        \n","        avg_score = (f1 + accuracy) / 2\n","        fold_scores.append(avg_score)  # This is average from f1 and accuracy\n","\n","        if len(top_n_models) < 3 or avg_score > min(top_n_models, key=lambda x: x[1])[1]:  # Top-3 method\n","            model_save_path = os.path.join(model_save_dir, f\"{studyName}_model_trial_{trial.number}_fold_{fold}_avg_score_{avg_score:.4f}\")\n","            model.save(model_save_path, save_format=\"tf\")\n","            top_n_models.append((trial.number, avg_score))\n","            top_n_models.sort(key=lambda x: x[1], reverse=True)\n","            if len(top_n_models) > 3:\n","                top_n_models.pop()\n","\n","            # Save fold scores to a file\n","            scores_save_path = os.path.join(model_save_dir, f\"{studyName}_scores_trial_{trial.number}_fold_{fold}.json\")\n","            with open(scores_save_path, 'w') as f:\n","                json.dump({'fold_scores': fold_scores}, f, indent=4)\n","\n","    return np.mean(fold_scores)\n","\n","# Define your Optuna study, using the MySQL connection string\n","optuna_storage = 'mysql+pymysql://<username>:<password>@<host>/<database>?ssl_ca=<path_to_CA_cert>&ssl_verify_cert=true'\n","\n","from kaggle_secrets import UserSecretsClient\n","user_secrets = UserSecretsClient()\n","db_password = user_secrets.get_secret(\"DB_PASSWORD\")# This uses the secrets inside of Kaggle so I don't have to explicitly type my password out in code\n","\n","# Example with your details (replace '<password>' with your real password and '<database>' with your database name)\n","optuna_storage = f'mysql+pymysql://MichaelAzure:{db_password}@kaggle-third-sql.mysql.database.azure.com/kaggle_disaster_database?ssl_ca=/kaggle/input/certification&ssl_verify_cert=true'\n","\n","\n","studyName = 'disaster_test_mike_3'\n","study = optuna.create_study(study_name=studyName, # name of the study\n","                            storage=optuna_storage,  # URL for the mySQL schema\n","                            direction='maximize', # maximize the log loss\n","                            load_if_exists=True, # makes it so that if the study_name already exists in the schema, then it will append the new trials with the old trials and essentially resume the study. It will also remember the previous trials so it really is resuming the study\n","                            )\n","\n","study.optimize(objective, n_trials=6)\n","\n","print(\"Best trial:\")\n","trial = study.best_trial\n","print(f\"  Value: {trial.value}\")\n","print(\"  Params: \")\n","for key, value in trial.params.items():\n","    print(f\"    {key}: {value}\")\n","\n","\n","# # After hyperparameter tuning, use the saved top models to make predictions on the Kaggle test dataset\n","\n","# # Directory where models are saved\n","# saved_model_dir = './saved_models'\n","\n","# # Get the list of saved models\n","# saved_models = [f for f in os.listdir(saved_model_dir) if os.path.isdir(os.path.join(saved_model_dir, f))]\n","\n","# # Load the Kaggle test dataset\n","# kaggle_test_dataset = tf.data.Dataset.from_tensor_slices(dict(kaggle_test_encodings)).batch(study.best_trial.params['batch_size'])\n","\n","# # Iterate over the saved models\n","# for model_dir in saved_models:\n","#     model_path = os.path.join(saved_model_dir, model_dir)\n","#     loaded_model = tf.keras.models.load_model(model_path, custom_objects={\"f1_score\": f1_score})\n","    \n","#     # Make predictions\n","#     kaggle_test_predictions = loaded_model.predict(kaggle_test_dataset).logits\n","#     kaggle_test_predicted_labels = tf.argmax(kaggle_test_predictions, axis=1).numpy()\n","\n","#     # Create a submission DataFrame\n","#     submission = pd.DataFrame({'id': kaggle_test_data['id'], 'target': kaggle_test_predicted_labels})\n","    \n","#     # Save the submission\n","#     submission_file = model_dir + '_submission.csv'\n","#     submission_path = os.path.join(saved_model_dir, submission_file)\n","#     submission.to_csv(submission_path, index=False)\n","    \n","#     print(f\"Predictions saved for model: {model_dir}\")\n","\n","# # Train the final model on the combined Train and Validation sets\n","# final_train_data = pd.concat([train_data, val_data])\n","# final_train_encodings = tokenize_texts(final_train_data['clean_text'])\n","# final_train_labels = tf.convert_to_tensor(final_train_data['target'].values)\n","# final_train_dataset = tf.data.Dataset.from_tensor_slices((\n","#     dict(final_train_encodings),\n","#     final_train_labels\n","# )).batch(study.best_trial.params['batch_size']).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","# with strategy.scope():\n","#     final_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n","#     final_model.compile(optimizer=optimizer, \n","#                         loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n","#                         metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy'), f1_score])\n","    \n","# final_model.fit(final_train_dataset, epochs=study.best_trial.params['num_epochs'], verbose=1)\n","\n","# # Evaluate on the split test set\n","# split_test_dataset = tf.data.Dataset.from_tensor_slices((\n","#     dict(split_test_encodings),\n","#     split_test_labels\n","# )).batch(study.best_trial.params['batch_size']).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","# split_test_predictions = final_model.predict(split_test_dataset).logits\n","# split_test_y_val = np.concatenate([y.numpy() for _, y in split_test_dataset], axis=0)\n","# split_test_metrics = compute_metrics(split_test_predictions, split_test_y_val)\n","\n","# print(f\"Split Test F1 Score: {split_test_metrics['f1']}\")\n","# print(f\"Split Test Accuracy: {split_test_metrics['accuracy']}\")\n","\n","# # Predictions on the Kaggle test dataset\n","# kaggle_test_predictions = final_model.predict(kaggle_test_dataset).logits\n","# kaggle_test_predicted_labels = tf.argmax(kaggle_test_predictions, axis=1).numpy()\n","\n","# # Create a submission DataFrame\n","# submission = pd.DataFrame({'id': kaggle_test_data['id'], 'target': kaggle_test_predicted_labels})\n","# submission.to_csv('final_submission.csv', index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-08T01:03:48.289538Z","iopub.status.busy":"2024-06-08T01:03:48.288638Z","iopub.status.idle":"2024-06-08T01:05:14.426374Z","shell.execute_reply":"2024-06-08T01:05:14.425509Z","shell.execute_reply.started":"2024-06-08T01:03:48.289501Z"},"trusted":true},"outputs":[],"source":["# After hyperparameter tuning, use the saved top models to make predictions on the Kaggle test dataset\n","\n","# Directory where models are saved\n","saved_model_dir = './saved_models'\n","\n","# Get the list of saved models\n","saved_models = [f for f in os.listdir(saved_model_dir) if os.path.isdir(os.path.join(saved_model_dir, f))]\n","\n","# Load the Kaggle test dataset\n","kaggle_test_dataset = tf.data.Dataset.from_tensor_slices(dict(kaggle_test_encodings)).batch(study.best_trial.params['batch_size'])\n","\n","# Iterate over the saved models\n","for model_dir in saved_models:\n","    model_path = os.path.join(saved_model_dir, model_dir)\n","    loaded_model = tf.keras.models.load_model(model_path, custom_objects={\"f1_score\": f1_score})\n","    \n","    # Make predictions\n","    kaggle_test_predictions = loaded_model.predict(kaggle_test_dataset)\n","    kaggle_test_logits = kaggle_test_predictions['logits']\n","    kaggle_test_predicted_labels = tf.argmax(kaggle_test_logits, axis=1).numpy()\n","\n","    # Create a submission DataFrame\n","    submission = pd.DataFrame({'id': kaggle_test_data['id'], 'target': kaggle_test_predicted_labels})\n","    \n","    # Save the submission\n","    submission_file = 'Submission: ' + model_dir + '_.csv'\n","    submission_path = os.path.join(saved_model_dir, submission_file)\n","    submission.to_csv(submission_path, index=False)\n","    \n","    print(f\"Predictions saved for model: {model_dir}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-08T01:01:56.526363Z","iopub.status.idle":"2024-06-08T01:01:56.526813Z","shell.execute_reply":"2024-06-08T01:01:56.526598Z","shell.execute_reply.started":"2024-06-08T01:01:56.526580Z"},"trusted":true},"outputs":[],"source":["# # After hyperparameter tuning, train final model on combined training and validation sets\n","# final_train_data = pd.concat([train_data, val_data])\n","\n","# final_train_encodings = tokenize_texts(final_train_data['clean_text'])\n","# final_train_labels = tf.convert_to_tensor(final_train_data['target'].values)\n","# final_train_dataset = tf.data.Dataset.from_tensor_slices((\n","#     dict(final_train_encodings),\n","#     final_train_labels\n","# )).batch(study.best_trial.params['batch_size']).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","# with strategy.scope():\n","#     final_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n","#     final_model.compile(optimizer=optimizer, \n","#                         loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n","#                         metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy'), f1_score])\n","    \n","# final_model.fit(final_train_dataset, epochs=study.best_trial.params['num_epochs'], verbose=1)\n","\n","# # Evaluate on the split test set\n","# split_test_dataset = tf.data.Dataset.from_tensor_slices((\n","#     dict(split_test_encodings),\n","#     split_test_labels\n","# )).batch(study.best_trial.params['batch_size']).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","# split_test_predictions = final_model.predict(split_test_dataset).logits\n","# split_test_y_val = np.concatenate([y.numpy() for _, y in split_test_dataset], axis=0)\n","# split_test_metrics = compute_metrics(split_test_predictions, split_test_y_val)\n","\n","# print(f\"Split Test F1 Score: {split_test_metrics['f1']}\")\n","# print(f\"Split Test Accuracy: {split_test_metrics['accuracy']}\")\n","\n","# # Predictions on the Kaggle test dataset\n","# kaggle_test_dataset = tf.data.Dataset.from_tensor_slices(dict(kaggle_test_encodings)).batch(study.best_trial.params['batch_size'])\n","# kaggle_test_predictions = final_model.predict(kaggle_test_dataset).logits\n","# kaggle_test_predicted_labels = tf.argmax(kaggle_test_predictions, axis=1).numpy()\n","\n","# # Create a submission DataFrame\n","# submission = pd.DataFrame({'id': kaggle_test_data['id'], 'target': kaggle_test_predicted_labels})\n","# submission.to_csv('submission.csv', index=False)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":869809,"sourceId":17777,"sourceType":"competition"},{"datasetId":5157167,"sourceId":8616390,"sourceType":"datasetVersion"}],"dockerImageVersionId":30732,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
