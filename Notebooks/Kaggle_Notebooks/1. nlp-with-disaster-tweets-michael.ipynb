{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":30715,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Welcome to my first NLP Competition! This is my first notebook attempt for the NLP with Disaster Tweets (Kaggle) Competition!","metadata":{}},{"cell_type":"markdown","source":"# Step 0: Kaggle Set Up","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2024-06-04T19:58:01.448578Z","iopub.execute_input":"2024-06-04T19:58:01.449432Z","iopub.status.idle":"2024-06-04T19:58:01.931961Z","shell.execute_reply.started":"2024-06-04T19:58:01.449388Z","shell.execute_reply":"2024-06-04T19:58:01.930831Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Step 1: pip install dependencies and set global variables","metadata":{}},{"cell_type":"code","source":"# %pip install pandas numpy tensorflow transformers scikit-learn matplotlib\n\n# # #python.exe -m pip install --upgrade pip\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T19:58:01.934276Z","iopub.execute_input":"2024-06-04T19:58:01.934764Z","iopub.status.idle":"2024-06-04T19:58:01.939632Z","shell.execute_reply.started":"2024-06-04T19:58:01.934731Z","shell.execute_reply":"2024-06-04T19:58:01.938455Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Set Global random seed to make sure we can replicate any model that we create (no randomness)","metadata":{}},{"cell_type":"code","source":"import random\nimport tensorflow as tf\nimport numpy as np\nimport os\nfrom transformers import set_seed\n\n\n\nnp.random.seed(42)\nrandom.seed(42)\nnp.random.seed(42)\ntf.random.set_seed(42)\nset_seed(42)\n\nos.environ['TF_DETERMINISTIC_OPS'] = '1'","metadata":{"execution":{"iopub.status.busy":"2024-06-04T19:58:01.941171Z","iopub.execute_input":"2024-06-04T19:58:01.941595Z","iopub.status.idle":"2024-06-04T19:58:08.794218Z","shell.execute_reply.started":"2024-06-04T19:58:01.941551Z","shell.execute_reply":"2024-06-04T19:58:08.792899Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-06-04 19:58:02.385950: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-04 19:58:02.386025: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-04 19:58:02.387725: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Step 2: Exploring and Understanding the data","metadata":{}},{"cell_type":"markdown","source":"### Loading Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load the training data\ntrain_data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n\n# Display the first few rows of the training data\nprint(train_data.head())","metadata":{"execution":{"iopub.status.busy":"2024-06-04T19:58:08.795752Z","iopub.execute_input":"2024-06-04T19:58:08.796534Z","iopub.status.idle":"2024-06-04T19:58:08.846161Z","shell.execute_reply.started":"2024-06-04T19:58:08.796492Z","shell.execute_reply":"2024-06-04T19:58:08.844985Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  \n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Data Cleaning\n\nI had a hard choice of whether or not to delete hashtags, but after inspecting the data, I saw that there were so many hashtags and hashtags are a crucial part of tweets so I decided that I want to keep them and then do the extra work of preprecessing them later on when I preprocess the data.\n\nI might go and remove hashtags in the future, to see how it affects the performance. So, if you see that I decided to remove the hashtags, then now you know why!","metadata":{}},{"cell_type":"code","source":"import re # Regular Expression\n\ndef clean_text(text):\n    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n    text = re.sub(r'@\\w+', '', text)     # Remove mentions\n    text = re.sub(r'\\d+', '', text)      # Remove numbers\n    text = re.sub(r'[^\\w\\s#]', '', text)  # Remove punctuation except hashtags\n    text = text.lower()                  # Convert to lowercase\n    return text\n\ntrain_data['clean_text'] = train_data['text'].apply(clean_text) # Apply the data cleaning process to training data\ntest_data['clean_text'] = test_data['text'].apply(clean_text)# Apply the data cleaning process to testing data\n\n# Display the first few rows of the cleaned data\nprint(train_data[['text', 'clean_text']].head())\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T19:58:08.849663Z","iopub.execute_input":"2024-06-04T19:58:08.850002Z","iopub.status.idle":"2024-06-04T19:58:09.031606Z","shell.execute_reply.started":"2024-06-04T19:58:08.849972Z","shell.execute_reply":"2024-06-04T19:58:09.030346Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"                                                text  \\\n0  Our Deeds are the Reason of this #earthquake M...   \n1             Forest fire near La Ronge Sask. Canada   \n2  All residents asked to 'shelter in place' are ...   \n3  13,000 people receive #wildfires evacuation or...   \n4  Just got sent this photo from Ruby #Alaska as ...   \n\n                                          clean_text  \n0  our deeds are the reason of this #earthquake m...  \n1              forest fire near la ronge sask canada  \n2  all residents asked to shelter in place are be...  \n3   people receive #wildfires evacuation orders i...  \n4  just got sent this photo from ruby #alaska as ...  \n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Step 3: Preprocessing and Tokenization","metadata":{}},{"cell_type":"markdown","source":"### Tokenization and Padding/Truncation from bert-base-uncased \n\n- The bert-base-uncased tokenizer also has a padding/truncation feature built into it so we will use that so that we don't have to manually truncate and pad!","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef tokenize_texts(texts):\n    return tokenizer(\n        texts.tolist(),\n        max_length=64,\n        padding=True,\n        truncation=True,\n        return_tensors='tf'\n    )\n\ntrain_encodings = tokenize_texts(train_data['clean_text'])\ntest_encodings = tokenize_texts(test_data['clean_text'])\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T19:58:09.032900Z","iopub.execute_input":"2024-06-04T19:58:09.033265Z","iopub.status.idle":"2024-06-04T19:58:17.958597Z","shell.execute_reply.started":"2024-06-04T19:58:09.033233Z","shell.execute_reply":"2024-06-04T19:58:17.957659Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Analyzing the length of tokens to find the optimal maximum length for sequences\n\nBased on the results from this, we saw that the maximum sequence length (a sequence in this context is a single tweet) was around 40, and so we will pick a multiple of 2 for better computational performance. \n\nSo, I decided on 64!","metadata":{}},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n\n# # Tokenize the clean text without padding to get the length of each tweet\n# train_data['token_length'] = train_data['clean_text'].apply(lambda x: len(tokenizer.encode(x, add_special_tokens=True)))\n# test_data['token_length'] = test_data['clean_text'].apply(lambda x: len(tokenizer.encode(x, add_special_tokens=True)))\n\n# # Plot the distribution of token lengths\n# plt.hist(train_data['token_length'], bins=50, alpha=0.7, label='Train')\n# plt.hist(test_data['token_length'], bins=50, alpha=0.7, label='Test')\n# plt.axvline(x=128, color='r', linestyle='--', label='MAX_LEN = 128')\n# plt.xlabel('Token Length')\n# plt.ylabel('Frequency')\n# plt.legend()\n# plt.show()\n\n# # Display some statistics\n# print(\"Train token length statistics:\")\n# print(train_data['token_length'].describe())\n\n# print(\"\\nTest token length statistics:\")\n# print(test_data['token_length'].describe())\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T19:58:17.959844Z","iopub.execute_input":"2024-06-04T19:58:17.960160Z","iopub.status.idle":"2024-06-04T19:58:17.966238Z","shell.execute_reply.started":"2024-06-04T19:58:17.960133Z","shell.execute_reply":"2024-06-04T19:58:17.965136Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Prepare data for training\n\nThis includes:\n- Splitting data into training and validation \n- Converting data into a format that BERT can actually train on ","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\ntrain_labels = tf.convert_to_tensor(train_data['target'].values)\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(train_encodings),\n    train_labels\n))\n\n# Create a validation split  \nval_size = int(0.2 * len(train_data)) \nval_dataset = train_dataset.take(val_size)\ntrain_dataset = train_dataset.skip(val_size)\n\n# Batch and shuffle the datasets\nbatch_size = 32\n\ntrain_dataset = train_dataset.shuffle(10000).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\nval_dataset = val_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T19:58:17.967828Z","iopub.execute_input":"2024-06-04T19:58:17.968379Z","iopub.status.idle":"2024-06-04T19:58:19.583373Z","shell.execute_reply.started":"2024-06-04T19:58:17.968338Z","shell.execute_reply":"2024-06-04T19:58:19.582413Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Step 4: Building a model!!!","metadata":{}},{"cell_type":"markdown","source":"### Picking a Model Architecture\n\nI am going to pick BERT but I might play around with other pretrained models later. \n\nThis is an example of transfer learning, where I am taking a pretrained model (ex. BERT) and then training it on my specific data. No need to re-invent the wheel, especially since it will take long time to make a model from scratch and I might not get great results back since my training data size is not very good. The BERT model is training on SO MUCH data, so it's already very smart.\n\nI am using the BERT model and by doing import TFBertForSequenceClassification, I am using the model that adds a classification head to the BERT base model. Adding a layer to a pre-trained model is a crucial part of transfer learning, and by training the model on my data, I will be setting the weights of the new head layer of the model, which is where it learns about disaster tweets and how to classify them!","metadata":{}},{"cell_type":"code","source":"# from transformers import TFBertForSequenceClassification, BertConfig\n\n# config = BertConfig.from_pretrained('bert-base-uncased', num_labels=2)\n# model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T19:58:19.584903Z","iopub.execute_input":"2024-06-04T19:58:19.585350Z","iopub.status.idle":"2024-06-04T19:58:19.590438Z","shell.execute_reply.started":"2024-06-04T19:58:19.585309Z","shell.execute_reply":"2024-06-04T19:58:19.589345Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Compiling the Model","metadata":{}},{"cell_type":"code","source":"# model.compile(\n#     optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-8),\n#     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n#     metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]\n# )\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T19:58:19.592108Z","iopub.execute_input":"2024-06-04T19:58:19.592560Z","iopub.status.idle":"2024-06-04T19:58:19.601474Z","shell.execute_reply.started":"2024-06-04T19:58:19.592509Z","shell.execute_reply":"2024-06-04T19:58:19.600267Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Step 5: Training the Model","metadata":{}},{"cell_type":"markdown","source":"### Train the model\n","metadata":{}},{"cell_type":"code","source":"# history = model.fit(\n#     train_dataset,\n#     epochs=3,\n#     validation_data=val_dataset\n# )\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T19:58:19.603165Z","iopub.execute_input":"2024-06-04T19:58:19.603654Z","iopub.status.idle":"2024-06-04T19:58:19.612493Z","shell.execute_reply.started":"2024-06-04T19:58:19.603618Z","shell.execute_reply":"2024-06-04T19:58:19.610925Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from transformers import TFBertForSequenceClassification, BertConfig\n\nstrategy = tf.distribute.MirroredStrategy()\n\nwith strategy.scope():\n  # Model configuration and creation\n  config = BertConfig.from_pretrained('bert-base-uncased', num_labels=2)\n  model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n\n  # Model compilation\n  model.compile(\n      optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-8),\n      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n      metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]\n  )\n\n  # Model training\n  history = model.fit(\n      train_dataset,\n      epochs=3,\n      validation_data=val_dataset\n  )\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T19:58:19.614210Z","iopub.execute_input":"2024-06-04T19:58:19.614595Z","iopub.status.idle":"2024-06-04T20:09:45.795133Z","shell.execute_reply.started":"2024-06-04T19:58:19.614562Z","shell.execute_reply":"2024-06-04T20:09:45.793835Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nAll PyTorch model weights were used when initializing TFBertForSequenceClassification.\n\nSome weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\nWARNING: AutoGraph could not transform <function infer_framework at 0x7b1f8f5455a0> and will run it as-is.\nCause: for/else statement not yet supported\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1717531269.057470    7667 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"167/167 [==============================] - 385s 1s/step - loss: 0.4678 - accuracy: 0.7848 - val_loss: 0.3876 - val_accuracy: 0.8410\nEpoch 2/3\n167/167 [==============================] - 150s 899ms/step - loss: 0.3317 - accuracy: 0.8692 - val_loss: 0.4134 - val_accuracy: 0.8244\nEpoch 3/3\n167/167 [==============================] - 146s 877ms/step - loss: 0.2409 - accuracy: 0.9099 - val_loss: 0.5054 - val_accuracy: 0.8204\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Step 6: Evaluating and Submitting the Results","metadata":{}},{"cell_type":"markdown","source":"### Predict on the test set","metadata":{}},{"cell_type":"code","source":"test_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(test_encodings)\n)).batch(32)\n\npredictions = model.predict(test_dataset).logits\npredicted_labels = tf.argmax(predictions, axis=1).numpy()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T20:09:45.797606Z","iopub.execute_input":"2024-06-04T20:09:45.797973Z","iopub.status.idle":"2024-06-04T20:10:18.916454Z","shell.execute_reply.started":"2024-06-04T20:09:45.797942Z","shell.execute_reply":"2024-06-04T20:10:18.914922Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"102/102 [==============================] - 33s 65ms/step\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Prepare the Submission File:","metadata":{}},{"cell_type":"code","source":"# Create a submission DataFrame\nsubmission = pd.DataFrame({'id': test_data['id'], 'target': predicted_labels})\nsubmission.to_csv('submission_3_kaggle.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T20:10:18.919998Z","iopub.execute_input":"2024-06-04T20:10:18.920379Z","iopub.status.idle":"2024-06-04T20:10:18.936948Z","shell.execute_reply.started":"2024-06-04T20:10:18.920349Z","shell.execute_reply":"2024-06-04T20:10:18.935404Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Step 7: Iterating and Improving","metadata":{}},{"cell_type":"markdown","source":"Hyperparameter Tuning:\n- Experiment with different hyperparameters such as learning rate, batch size, and the number of epochs to improve the modelâ€™s performance.\n\nData Augmentation:\n- Consider using data augmentation techniques to increase the diversity of your training data.\n\nModel Ensembles:\n- Combine the predictions from multiple models to improve overall performance.","metadata":{}}]}