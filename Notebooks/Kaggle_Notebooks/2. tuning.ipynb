{"cells":[{"cell_type":"markdown","metadata":{},"source":["# This notebook attempts to use the trainer API from the transformer library in order to use Optuna, but that ended up following a guide for PyTorch, so this code didn't work properly for my TensorFlow Models"]},{"cell_type":"markdown","metadata":{},"source":["# Step 0: Kaggle Set Up"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T14:07:20.248510Z","iopub.status.busy":"2024-06-05T14:07:20.248155Z","iopub.status.idle":"2024-06-05T14:07:20.609821Z","shell.execute_reply":"2024-06-05T14:07:20.608946Z","shell.execute_reply.started":"2024-06-05T14:07:20.248475Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/nlp-getting-started/sample_submission.csv\n","/kaggle/input/nlp-getting-started/train.csv\n","/kaggle/input/nlp-getting-started/test.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","        \n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["## Step 1: pip install dependencies and set global variables"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T14:07:20.612554Z","iopub.status.busy":"2024-06-05T14:07:20.611894Z","iopub.status.idle":"2024-06-05T14:07:20.615988Z","shell.execute_reply":"2024-06-05T14:07:20.615180Z","shell.execute_reply.started":"2024-06-05T14:07:20.612519Z"},"trusted":true},"outputs":[],"source":["# %pip install pandas numpy tensorflow transformers scikit-learn matplotlib\n","\n","# # #python.exe -m pip install --upgrade pip\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Set Global random seed to make sure we can replicate any model that we create (no randomness)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T14:07:20.617281Z","iopub.status.busy":"2024-06-05T14:07:20.617015Z","iopub.status.idle":"2024-06-05T14:07:37.849389Z","shell.execute_reply":"2024-06-05T14:07:37.848421Z","shell.execute_reply.started":"2024-06-05T14:07:20.617236Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-06-05 14:07:22.666947: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-06-05 14:07:22.667039: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-06-05 14:07:22.829458: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import random\n","import tensorflow as tf\n","import numpy as np\n","import os\n","from transformers import set_seed\n","\n","\n","\n","np.random.seed(42)\n","random.seed(42)\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","set_seed(42)\n","\n","os.environ['TF_DETERMINISTIC_OPS'] = '1'"]},{"cell_type":"markdown","metadata":{},"source":["## Step 2: Exploring and Understanding the data"]},{"cell_type":"markdown","metadata":{},"source":["### Loading Data"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T14:07:37.851691Z","iopub.status.busy":"2024-06-05T14:07:37.850739Z","iopub.status.idle":"2024-06-05T14:07:37.925348Z","shell.execute_reply":"2024-06-05T14:07:37.924388Z","shell.execute_reply.started":"2024-06-05T14:07:37.851655Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["   id keyword location                                               text  \\\n","0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n","1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n","2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n","3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n","4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n","\n","   target  \n","0       1  \n","1       1  \n","2       1  \n","3       1  \n","4       1  \n"]}],"source":["import pandas as pd\n","\n","# Load the training data\n","train_data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n","test_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n","\n","# Display the first few rows of the training data\n","print(train_data.head())"]},{"cell_type":"markdown","metadata":{},"source":["### Data Cleaning\n","\n","I had a hard choice of whether or not to delete hashtags, but after inspecting the data, I saw that there were so many hashtags and hashtags are a crucial part of tweets so I decided that I want to keep them and then do the extra work of preprecessing them later on when I preprocess the data.\n","\n","I might go and remove hashtags in the future, to see how it affects the performance. So, if you see that I decided to remove the hashtags, then now you know why!"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T14:07:37.928301Z","iopub.status.busy":"2024-06-05T14:07:37.927977Z","iopub.status.idle":"2024-06-05T14:07:38.085041Z","shell.execute_reply":"2024-06-05T14:07:38.084078Z","shell.execute_reply.started":"2024-06-05T14:07:37.928265Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["                                                text  \\\n","0  Our Deeds are the Reason of this #earthquake M...   \n","1             Forest fire near La Ronge Sask. Canada   \n","2  All residents asked to 'shelter in place' are ...   \n","3  13,000 people receive #wildfires evacuation or...   \n","4  Just got sent this photo from Ruby #Alaska as ...   \n","\n","                                          clean_text  \n","0  our deeds are the reason of this #earthquake m...  \n","1              forest fire near la ronge sask canada  \n","2  all residents asked to shelter in place are be...  \n","3   people receive #wildfires evacuation orders i...  \n","4  just got sent this photo from ruby #alaska as ...  \n"]}],"source":["import re # Regular Expression\n","\n","def clean_text(text):\n","    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n","    text = re.sub(r'@\\w+', '', text)     # Remove mentions\n","    text = re.sub(r'\\d+', '', text)      # Remove numbers\n","    text = re.sub(r'[^\\w\\s#]', '', text)  # Remove punctuation except hashtags\n","    text = text.lower()                  # Convert to lowercase\n","    return text\n","\n","train_data['clean_text'] = train_data['text'].apply(clean_text) # Apply the data cleaning process to training data\n","test_data['clean_text'] = test_data['text'].apply(clean_text)# Apply the data cleaning process to testing data\n","\n","# Display the first few rows of the cleaned data\n","print(train_data[['text', 'clean_text']].head())\n"]},{"cell_type":"markdown","metadata":{},"source":["## Step 3: Preprocessing and Tokenization"]},{"cell_type":"markdown","metadata":{},"source":["### Tokenization and Padding/Truncation from bert-base-uncased \n","\n","- The bert-base-uncased tokenizer also has a padding/truncation feature built into it so we will use that so that we don't have to manually truncate and pad!"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T14:07:38.086290Z","iopub.status.busy":"2024-06-05T14:07:38.085981Z","iopub.status.idle":"2024-06-05T14:07:46.815002Z","shell.execute_reply":"2024-06-05T14:07:46.814199Z","shell.execute_reply.started":"2024-06-05T14:07:38.086240Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6f18790266e14d0eb38e2c2b6d564cdc","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"834e2fb4f52342049c33e01e5e335c2b","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c618869175f74628bfeddfd672170842","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c6edfad540ea45ebb111d629e0c27b53","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","def tokenize_texts(texts):\n","    return tokenizer(\n","        texts.tolist(),\n","        max_length=64,\n","        padding=True,\n","        truncation=True,\n","        return_tensors='tf'\n","    )\n","\n","train_encodings = tokenize_texts(train_data['clean_text'])\n","test_encodings = tokenize_texts(test_data['clean_text'])\n"]},{"cell_type":"markdown","metadata":{},"source":["### Analyzing the length of tokens to find the optimal maximum length for sequences\n","\n","Based on the results from this, we saw that the maximum sequence length (a sequence in this context is a single tweet) was around 40, and so we will pick a multiple of 2 for better computational performance. \n","\n","So, I decided on 64!"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T14:07:46.816417Z","iopub.status.busy":"2024-06-05T14:07:46.816096Z","iopub.status.idle":"2024-06-05T14:07:46.820830Z","shell.execute_reply":"2024-06-05T14:07:46.819954Z","shell.execute_reply.started":"2024-06-05T14:07:46.816392Z"},"trusted":true},"outputs":[],"source":["# import matplotlib.pyplot as plt\n","\n","# # Tokenize the clean text without padding to get the length of each tweet\n","# train_data['token_length'] = train_data['clean_text'].apply(lambda x: len(tokenizer.encode(x, add_special_tokens=True)))\n","# test_data['token_length'] = test_data['clean_text'].apply(lambda x: len(tokenizer.encode(x, add_special_tokens=True)))\n","\n","# # Plot the distribution of token lengths\n","# plt.hist(train_data['token_length'], bins=50, alpha=0.7, label='Train')\n","# plt.hist(test_data['token_length'], bins=50, alpha=0.7, label='Test')\n","# plt.axvline(x=128, color='r', linestyle='--', label='MAX_LEN = 128')\n","# plt.xlabel('Token Length')\n","# plt.ylabel('Frequency')\n","# plt.legend()\n","# plt.show()\n","\n","# # Display some statistics\n","# print(\"Train token length statistics:\")\n","# print(train_data['token_length'].describe())\n","\n","# print(\"\\nTest token length statistics:\")\n","# print(test_data['token_length'].describe())\n"]},{"cell_type":"markdown","metadata":{},"source":["### Prepare data for training\n","\n","This includes:\n","- Splitting data into training and validation \n","- Converting data into a format that BERT can actually train on "]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T14:07:46.822168Z","iopub.status.busy":"2024-06-05T14:07:46.821904Z","iopub.status.idle":"2024-06-05T14:07:48.600886Z","shell.execute_reply":"2024-06-05T14:07:48.600126Z","shell.execute_reply.started":"2024-06-05T14:07:46.822146Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","\n","train_labels = tf.convert_to_tensor(train_data['target'].values)\n","\n","train_dataset = tf.data.Dataset.from_tensor_slices((\n","    dict(train_encodings),\n","    train_labels\n","))\n","\n","# Create a validation split \n","val_size = int(0.3 * len(train_data))\n","val_dataset = train_dataset.take(val_size)\n","train_dataset = train_dataset.skip(val_size)\n","\n","# Batch and shuffle the datasets\n","batch_size = 32\n","\n","# train_dataset = train_dataset.shuffle(10000).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n","train_dataset = train_dataset.shuffle(10000) # removed batch and prefetch here since I will do that in my optuna tuning\n","val_dataset = val_dataset.batch(batch_size) # batch but don't prefetch since it says online that it might cause some randomness\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Step 4: Building a model!!!"]},{"cell_type":"markdown","metadata":{},"source":["### Picking a Model Architecture\n","\n","I am going to pick BERT but I might play around with other pretrained models later. \n","\n","This is an example of transfer learning, where I am taking a pretrained model (ex. BERT) and then training it on my specific data. No need to re-invent the wheel, especially since it will take long time to make a model from scratch and I might not get great results back since my training data size is not very good. The BERT model is training on SO MUCH data, so it's already very smart.\n","\n","I am using the BERT model and by doing import TFBertForSequenceClassification, I am using the model that adds a classification head to the BERT base model. Adding a layer to a pre-trained model is a crucial part of transfer learning, and by training the model on my data, I will be setting the weights of the new head layer of the model, which is where it learns about disaster tweets and how to classify them!"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T14:07:48.602179Z","iopub.status.busy":"2024-06-05T14:07:48.601918Z","iopub.status.idle":"2024-06-05T14:07:48.605912Z","shell.execute_reply":"2024-06-05T14:07:48.605035Z","shell.execute_reply.started":"2024-06-05T14:07:48.602157Z"},"trusted":true},"outputs":[],"source":["# from transformers import TFBertForSequenceClassification, BertConfig\n","\n","# config = BertConfig.from_pretrained('bert-base-uncased', num_labels=2)\n","# model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Compiling the Model"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T14:07:48.607095Z","iopub.status.busy":"2024-06-05T14:07:48.606812Z","iopub.status.idle":"2024-06-05T14:07:48.616150Z","shell.execute_reply":"2024-06-05T14:07:48.615268Z","shell.execute_reply.started":"2024-06-05T14:07:48.607065Z"},"trusted":true},"outputs":[],"source":["# model.compile(\n","#     optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-8),\n","#     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","#     metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]\n","# )\n"]},{"cell_type":"markdown","metadata":{},"source":["## Step 5: Training the Model"]},{"cell_type":"markdown","metadata":{},"source":["### Train the model\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T14:07:48.617441Z","iopub.status.busy":"2024-06-05T14:07:48.617171Z","iopub.status.idle":"2024-06-05T14:07:48.625647Z","shell.execute_reply":"2024-06-05T14:07:48.624916Z","shell.execute_reply.started":"2024-06-05T14:07:48.617413Z"},"trusted":true},"outputs":[],"source":["# history = model.fit(\n","#     train_dataset,\n","#     epochs=3,\n","#     validation_data=val_dataset\n","# )\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T14:09:10.519240Z","iopub.status.busy":"2024-06-05T14:09:10.518855Z","iopub.status.idle":"2024-06-05T14:09:10.580152Z","shell.execute_reply":"2024-06-05T14:09:10.578910Z","shell.execute_reply.started":"2024-06-05T14:09:10.519200Z"},"trusted":true},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'Transfomer'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mTransfomer\u001b[39;00m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptuna_hp_space\u001b[39m(trial):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m      5\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_float(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1e-6\u001b[39m, \u001b[38;5;241m1e-4\u001b[39m, log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      6\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mper_device_train_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_categorical(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mper_device_train_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m]),\n\u001b[1;32m      7\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_train_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_train_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m      8\u001b[0m     }\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Transfomer'"]}],"source":["# from transformers import (AutoModelForSequenceClassification, AutoTokenizer,\n","#                           Trainer, TrainingArguments)\n","\n","# def optuna_hp_space(trial):\n","#     return {\n","#       \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n","#       \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32, 64, 128]),\n","#       \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 1, 2),\n","#     }\n","\n","\n","# def model_init(trial):\n","#   # Assuming model_args is defined elsewhere with model details\n","#   return AutoModelForSequenceClassification.from_pretrained(\n","#       model_args.model_name_or_path,\n","#       from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","#       config=config,\n","#       cache_dir=model_args.cache_dir,\n","#       revision=model_args.model_revision,\n","#       token=True if model_args.use_auth_token else None,\n","#   )\n","\n","\n","# # Define your training arguments (assuming training_args is set elsewhere)\n","# trainer = transformers.Trainer(\n","#     model=None,  # Set by model_init\n","#     args=training_args,\n","#     train_dataset=small_train_dataset,\n","#     eval_dataset=small_eval_dataset,\n","#     compute_metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy'), tf.keras.metrics.F1Score(num_classes=2)],\n","#     tokenizer=tokenizer,\n","#     model_init=model_init,\n","#     data_collator=data_collator,\n","# )\n","\n","# # Perform hyperparameter search with Optuna\n","# best_trials = trainer.hyperparameter_search(\n","#     direction=[\"maximize\"],\n","#     backend=\"optuna\",\n","#     hp_space=optuna_hp_space,\n","#     n_trials=2,\n","# )\n","\n","# # Access results from best_trials (if needed)\n"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T14:19:31.525260Z","iopub.status.busy":"2024-06-05T14:19:31.524401Z","iopub.status.idle":"2024-06-05T14:19:33.329596Z","shell.execute_reply":"2024-06-05T14:19:33.328157Z","shell.execute_reply.started":"2024-06-05T14:19:31.525217Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.2/metrics/f1/f1.py\n","You can avoid this message in future by passing the argument `trust_remote_code=True`.\n","Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n","\n","Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"ename":"AttributeError","evalue":"'TFBertForSequenceClassification' object has no attribute 'to'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 63\u001b[0m\n\u001b[1;32m     59\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m metric\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mpredictions, references\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[0;32m---> 63\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#tokenizer=tokenizer,\u001b[39;49;00m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#data_collator=data_collator,\u001b[39;49;00m\n\u001b[1;32m     72\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m best_trials \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mhyperparameter_search(\n\u001b[1;32m     76\u001b[0m     direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     77\u001b[0m     backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptuna\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m     compute_objective\u001b[38;5;241m=\u001b[39mcompute_objective\n\u001b[1;32m     81\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:528\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES\n\u001b[1;32m    527\u001b[0m ):\n\u001b[0;32m--> 528\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:775\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[0;32m--> 775\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n\u001b[1;32m    776\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","\u001b[0;31mAttributeError\u001b[0m: 'TFBertForSequenceClassification' object has no attribute 'to'"]}],"source":["from transformers import (TFBertForSequenceClassification, BertConfig,\n","                          Trainer, TrainingArguments)\n","\n","from datasets import load_metric\n","\n","def optuna_hp_space(trial):\n","    return {\n","        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n","        \"batch_size\": trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128]),\n","        \"num_epochs\": trial.suggest_int(\"num_epochs\", 1, 3),\n","    }\n","\n","def model_init():\n","    \"\"\"\n","    Creates a non-compiled TFBertForSequenceClassification model.\n","\n","    Args:\n","      learning_rate: Learning rate for the optimizer.\n","      num_train_epochs: Number of training epochs.\n","      per_device_train_batch_size: Train batch size per device.\n","\n","    Returns:\n","      A TFBertForSequenceClassification model.\n","    \"\"\"\n","    config = BertConfig.from_pretrained('bert-base-uncased', num_labels=2)\n","    model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n","    return model\n","\n","\n","# def model_init(trial):\n","#     return AutoModelForSequenceClassification.from_pretrained(\n","#         model_args.model_name_or_path,\n","#         from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","#         config=config,\n","#         cache_dir=model_args.cache_dir,\n","#         revision=model_args.model_revision,\n","#         token=True if model_args.use_auth_token else None,\n","#     )\n","\n","# Improved training arguments for Optuna tuning (consider adjusting eval_steps)\n","training_args = TrainingArguments(\n","    output_dir=\"/kaggle/working/\",\n","    evaluation_strategy=\"epoch\",  # Evaluate after each epoch\n","    # Consider adjusting eval_steps based on training speed and desired frequency (e.g., for faster training with 10k samples)\n","    # eval_steps=(len(your_train_dataset) // per_device_train_batch_size),  # Evaluate after each epoch\n","    disable_tqdm=True,\n","    # Include Early Stopping for efficiency (optional)\n","    #early_stopping_patience=3,  # Stop training if validation F1 doesn't improve for 3 epochs\n",")\n","\n","\n","# Load the F1 metric\n","metric = load_metric(\"f1\")\n","\n","# Define the compute_metrics function\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    return metric.compute(predictions=predictions, references=labels)\n","\n","\n","trainer = Trainer(\n","    model=None,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    compute_metrics=compute_metrics,\n","    #tokenizer=tokenizer,\n","    model_init=model_init,\n","    #data_collator=data_collator,\n",")\n","\n","\n","best_trials = trainer.hyperparameter_search(\n","    direction=\"maximize\",\n","    backend=\"optuna\",\n","    hp_space=optuna_hp_space,\n","    n_trials=2,\n","    compute_objective=compute_objective\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-05T14:07:48.912406Z","iopub.status.idle":"2024-06-05T14:07:48.912753Z","shell.execute_reply":"2024-06-05T14:07:48.912586Z","shell.execute_reply.started":"2024-06-05T14:07:48.912573Z"},"trusted":true},"outputs":[],"source":["# from transformers import TFBertForSequenceClassification, AutoTokenizer\n","# from transformers import TrainingArguments, Trainer\n","# from optuna import create_study, Trial\n","# import tensorflow as tf  # Import tensorflow for strategy\n","\n","# # Define your objective function (to minimize loss)\n","# def objective(trial):\n","#     # Load model and tokenizer (assuming you have model_name defined)\n","#     model_name = \"bert-base-uncased\"  # Replace with your desired model\n","#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n","#     model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","\n","#     # Suggest hyperparameters from search space\n","#     learning_rate = trial.suggest_float(\"learning_rate\", low=1e-5, high=5e-5)\n","#     num_train_epochs = trial.suggest_int(\"num_train_epochs\", low=1, high=2)\n","#     per_device_train_batch_size = trial.suggest_int(\"per_device_train_batch_size\", low=8, high=32)\n","#     dropout_rate = trial.suggest_float(\"dropout_rate\", low=0.1, high=0.3)  # Dropout rate search space\n","#     adam_epsilon = trial.suggest_float(\"adam_epsilon\", low=1e-8, high=1e-6)  # Adam epsilon search space\n","\n","#     # Define the MirroredStrategy for multi-gpu training on Kaggle\n","#     strategy = tf.distribute.MirroredStrategy()\n","\n","#     # Wrap training logic with strategy.scope()\n","#     with strategy.scope():\n","#         # Prepare training arguments with suggested hyperparameters\n","#         training_args = TrainingArguments(\n","#             output_dir=\"/kaggle/working/\",\n","#             per_device_train_batch_size=per_device_train_batch_size,\n","#             learning_rate=learning_rate,\n","#             num_train_epochs=num_train_epochs,\n","#             # ... other training arguments\n","#         )\n","\n","#         # Create batched and prefetched training dataset using the selected batch size\n","#         train_dataset2 = train_dataset.batch(per_device_train_batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","#         # Train the model using Trainer (replace with your training logic)\n","#         trainer = Trainer(\n","#             model=model,\n","#             args=training_args,\n","#             train_dataset=train_dataset,  # Replace with your training dataset\n","#             eval_dataset=val_dataset,  # Replace with your evaluation dataset\n","#             compute_metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')],  # Replace with your metric function\n","#         )\n","#         trainer.train()\n","\n","#         # Return evaluation metric (e.g., validation loss) to minimize\n","#         return trainer.evaluate()[\"eval_loss\"]\n","\n","# # Create Optuna study\n","# study = create_study(direction=\"minimize\")\n","\n","# # Optimize hyperparameters over a specified number of trials\n","# study.optimize(objective, n_trials=2)\n","\n","# # Get the best trial with its hyperparameters\n","# best_trial = study.best_trial\n","# print(f\"Best learning rate: {best_trial['learning_rate']}\")\n","# print(f\"Best num_train_epochs: {best_trial['num_train_epochs']}\")\n","# print(f\"Best per_device_train_batch_size: {best_trial['per_device_train_batch_size']}\")\n","# print(f\"Best dropout rate: {best_trial['dropout_rate']}\")\n","# print(f\"Best adam_epsilon: {best_trial['adam_epsilon']}\")\n","\n","# # # Load the model with the best hyperparameters\n","# # best_model_name = f\"best_model_{study.study_id}\"  # Create unique name\n","# # model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","# # model.load_weight(best_model_name)  # Assuming you've saved the best model weights\n","\n","# # # Prepare your Kaggle test data (preprocessing, tokenization)\n","# # test_data = your_kaggle_test_data  # Replace with your test data preparation logic\n","# # test_encodings = tokenizer(test_data, padding=\"max_length\", truncation=True)  # Tokenize test data\n","\n","# # # Make predictions on Kaggle test data\n","# # predictions = model.predict(test_encodings)[\"logits\"].argmax(-1)  # Get predicted class indices\n","\n","# # # Prepare your Kaggle submission format (e.g., pandas dataframe)\n","# # submission_df = your_submission_df  # Replace with your submission dataframe creation logic\n","# # submission_df[\"predicted_label\"] = predictions\n","\n","# # # Submit your predictions to the Kaggle competition\n","# # submission_df.to_csv(\"submission.csv\", index=False)  # Replace with your submission logic\n","\n","# # print(f\"Submitted predictions using the best model with hyperparameters:\")\n","# # for key, value in best_trial.params.items():\n","# #   print(f\"{key}: {value}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Step 6: Evaluating and Submitting the Results"]},{"cell_type":"markdown","metadata":{},"source":["### Predict on the test set"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-05T14:07:48.914471Z","iopub.status.idle":"2024-06-05T14:07:48.914775Z","shell.execute_reply":"2024-06-05T14:07:48.914637Z","shell.execute_reply.started":"2024-06-05T14:07:48.914625Z"},"trusted":true},"outputs":[],"source":["test_dataset = tf.data.Dataset.from_tensor_slices((\n","    dict(test_encodings)\n",")).batch(32)\n","\n","predictions = model.predict(test_dataset).logits\n","predicted_labels = tf.argmax(predictions, axis=1).numpy()\n"]},{"cell_type":"markdown","metadata":{},"source":["### Prepare the Submission File:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-05T14:07:48.915755Z","iopub.status.idle":"2024-06-05T14:07:48.916079Z","shell.execute_reply":"2024-06-05T14:07:48.915929Z","shell.execute_reply.started":"2024-06-05T14:07:48.915915Z"},"trusted":true},"outputs":[],"source":["# Create a submission DataFrame\n","submission = pd.DataFrame({'id': test_data['id'], 'target': predicted_labels})\n","submission.to_csv('submission_3_kaggle.csv', index=False)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Step 7: Iterating and Improving"]},{"cell_type":"markdown","metadata":{},"source":["Hyperparameter Tuning:\n","- Experiment with different hyperparameters such as learning rate, batch size, and the number of epochs to improve the modelâ€™s performance.\n","\n","Data Augmentation:\n","- Consider using data augmentation techniques to increase the diversity of your training data.\n","\n","Model Ensembles:\n","- Combine the predictions from multiple models to improve overall performance."]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":869809,"sourceId":17777,"sourceType":"competition"}],"dockerImageVersionId":30732,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
