{"cells":[{"cell_type":"markdown","metadata":{},"source":["# This notebook tests out using the validation accuracy for the Optuna Trial, and also uses the keyword argument"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-12T19:59:02.519149Z","iopub.status.busy":"2024-06-12T19:59:02.518886Z","iopub.status.idle":"2024-06-12T19:59:02.891656Z","shell.execute_reply":"2024-06-12T19:59:02.890668Z","shell.execute_reply.started":"2024-06-12T19:59:02.519124Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/certification/BaltimoreCyberTrustRoot.crt.pem\n","/kaggle/input/nlp-getting-started/sample_submission.csv\n","/kaggle/input/nlp-getting-started/train.csv\n","/kaggle/input/nlp-getting-started/test.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T19:59:02.893409Z","iopub.status.busy":"2024-06-12T19:59:02.893003Z","iopub.status.idle":"2024-06-12T19:59:02.910845Z","shell.execute_reply":"2024-06-12T19:59:02.909944Z","shell.execute_reply.started":"2024-06-12T19:59:02.893382Z"},"trusted":true},"outputs":[],"source":["# import numpy as np\n","# import pandas as pd\n","# import random\n","# import os\n","# import re\n","# from transformers import set_seed, BertTokenizer, TFBertForSequenceClassification, BertConfig\n","# import tensorflow as tf\n","# %pip install evaluate\n","# import evaluate\n","# import optuna\n","# from sklearn.model_selection import train_test_split\n","# from sklearn.metrics import accuracy_score\n","# import json\n","\n","# # Install necessary packages for Azure SQL connection\n","# %pip install mysql-connector-python \n","# %pip install PyMySQL\n","\n","# # Set random seeds for reproducibility\n","# np.random.seed(42)\n","# random.seed(42)\n","# tf.random.set_seed(42)\n","# set_seed(42)\n","# os.environ['TF_DETERMINISTIC_OPS'] = '1'\n","\n","# # Load the training data\n","# train_data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n","# kaggle_test_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n","\n","# # Split the data into 75% training and 25% validation sets\n","# train_data, val_data = train_test_split(train_data, test_size=0.25, random_state=42, stratify=train_data['target'])\n","\n","# # Clean the text data\n","# def clean_text(text):\n","#     text = re.sub(r'http\\S+', '', text)  # Remove URLs\n","#     text = re.sub(r'@\\w+', '', text)     # Remove mentions\n","#     text = re.sub(r'\\d+', '', text)      # Remove numbers\n","#     text = re.sub(r'[^\\w\\s#]', '', text)  # Remove punctuation except hashtags\n","#     text = text.lower()                  # Convert to lowercase\n","#     return text\n","\n","# train_data['clean_text'] = train_data['text'].apply(clean_text)\n","# val_data['clean_text'] = val_data['text'].apply(clean_text)\n","# kaggle_test_data['clean_text'] = kaggle_test_data['text'].apply(clean_text)\n","\n","# # Function to combine keyword and text\n","# def combine_keyword_and_text(row):\n","#     keyword = str(row['keyword']) if pd.notna(row['keyword']) else ''\n","#     text = row['clean_text']\n","#     return '[CLS] ' + keyword + ' [SEP] ' + text + ' [SEP]'\n","\n","# # Apply the function to combine keyword and text\n","# train_data['combined_text'] = train_data.apply(combine_keyword_and_text, axis=1)\n","# val_data['combined_text'] = val_data.apply(combine_keyword_and_text, axis=1)\n","# kaggle_test_data['combined_text'] = kaggle_test_data.apply(combine_keyword_and_text, axis=1)\n","\n","# # Tokenize the text data\n","# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# def tokenize_texts(texts):\n","#     return tokenizer(\n","#         texts.tolist(),\n","#         max_length=64,\n","#         padding=True,\n","#         truncation=True,\n","#         return_tensors='tf'\n","#     )\n","\n","# # Encode the combined text data\n","# train_encodings = tokenize_texts(train_data['combined_text'])\n","# val_encodings = tokenize_texts(val_data['combined_text'])\n","# kaggle_test_encodings = tokenize_texts(kaggle_test_data['combined_text'])\n","\n","# train_labels = tf.convert_to_tensor(train_data['target'].values)\n","# val_labels = tf.convert_to_tensor(val_data['target'].values)\n","\n","# # Load the F1 metric from the evaluate library\n","# metric = evaluate.load(\"f1\", trust_remote_code=True)\n","\n","# def compute_metrics(predictions, labels):\n","#     predictions = np.argmax(predictions, axis=1)\n","#     f1 = metric.compute(predictions=predictions, references=labels)['f1']\n","#     accuracy = accuracy_score(labels, predictions)\n","#     return {'f1': f1, 'accuracy': accuracy}\n","\n","# def create_tf_dataset(encodings, labels, batch_size):\n","#     dataset = tf.data.Dataset.from_tensor_slices((encodings, labels))\n","#     return dataset.shuffle(10000).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","# # Define precision and recall metrics outside of the custom metric function\n","# precision = tf.keras.metrics.Precision()\n","# recall = tf.keras.metrics.Recall()\n","\n","# def f1_score(y_true, y_pred):\n","#     # Convert logits to predicted labels\n","#     y_pred = tf.argmax(y_pred, axis=1)\n","    \n","#     # Ensure true labels are in integer format\n","#     y_true = tf.cast(y_true, tf.int64)\n","    \n","#     # Update the state of precision and recall\n","#     precision.update_state(y_true, y_pred)\n","#     recall.update_state(y_true, y_pred)\n","    \n","#     # Compute precision and recall values\n","#     precision_result = precision.result()\n","#     recall_result = recall.result()\n","    \n","#     # Compute F1 score\n","#     f1 = 2 * ((precision_result * recall_result) / (precision_result + recall_result + tf.keras.backend.epsilon()))\n","    \n","#     return f1\n","\n","# strategy = tf.distribute.MirroredStrategy()\n","\n","# # Directory to save models\n","# model_save_dir = './saved_models'\n","# os.makedirs(model_save_dir, exist_ok=True)\n","\n","# # Track top 3 models\n","# top_n_models = []\n","\n","# def objective(trial):\n","#     learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True)\n","#     batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n","#     num_epochs = trial.suggest_int(\"num_epochs\", 1, 10)\n","#     dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n","#     weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.1)\n","#     lr_scheduler_type = trial.suggest_categorical(\"lr_scheduler_type\", [\"constant\", \"linear\", \"cosine\", \"cosine_with_restarts\"])\n","\n","#     num_gpus = strategy.num_replicas_in_sync\n","\n","#     train_dataset = create_tf_dataset(dict(train_encodings), train_labels, batch_size // num_gpus)\n","#     val_dataset = create_tf_dataset(dict(val_encodings), val_labels, batch_size // num_gpus)\n","#     kaggle_test_dataset = tf.data.Dataset.from_tensor_slices(dict(kaggle_test_encodings)).batch(batch_size // num_gpus).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","#     with strategy.scope():\n","#         config = BertConfig.from_pretrained('bert-base-uncased', num_labels=2, hidden_dropout_prob=dropout_rate)\n","#         model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n","\n","#         if lr_scheduler_type == \"linear\":\n","#             lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n","#                 initial_learning_rate=learning_rate,\n","#                 decay_steps=10000,\n","#                 end_learning_rate=0.0,\n","#                 power=1.0\n","#             )\n","#         elif lr_scheduler_type == \"cosine\":\n","#             lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n","#                 initial_learning_rate=learning_rate,\n","#                 decay_steps=10000\n","#             )\n","#         elif lr_scheduler_type == \"cosine_with_restarts\":\n","#             lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n","#                 initial_learning_rate=learning_rate,\n","#                 first_decay_steps=1000\n","#             )\n","#         else:\n","#             lr_schedule = learning_rate\n","\n","#         optimizer = tf.keras.optimizers.experimental.AdamW(\n","#             learning_rate=lr_schedule,\n","#             weight_decay=weight_decay,\n","#             epsilon=1e-8\n","#         )\n","\n","#         model.compile(optimizer=optimizer, \n","#                       loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n","#                       metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy'), f1_score])\n","\n","#     model.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset, verbose=1)\n","\n","#     # Evaluate on validation set\n","#     val_loss, val_accuracy, val_f1_score = model.evaluate(val_dataset, verbose=1)\n","#     print(f\"f1 score: {val_f1_score} and accuracy: {val_accuracy}\")\n","    \n","#     avg_score = (val_accuracy + val_f1_score) / 2\n","\n","#     if len(top_n_models) < 3 or avg_score > min(top_n_models, key=lambda x: x[1])[1]:  # Top-3 method\n","#         # Fine-tune the model on the validation dataset\n","#         fine_tune_encodings = tokenize_texts(val_data['combined_text'])\n","#         fine_tune_labels = tf.convert_to_tensor(val_data['target'].values)\n","#         fine_tune_dataset = tf.data.Dataset.from_tensor_slices((\n","#             dict(fine_tune_encodings),\n","#             fine_tune_labels\n","#         )).batch(batch_size // num_gpus).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","#         # Calculate the ratio of training data size to epochs since we shouldn't be fine-tuning on the same number of epoch since we are fine-tuning on a smaller amount of data (validation is smaller than training)\n","#         training_data_size = len(train_data)\n","#         fine_tune_data_size = len(val_data)\n","#         fine_tune_epochs = max(1, round((fine_tune_data_size / training_data_size) * num_epochs))\n","\n","#         model.fit(fine_tune_dataset, epochs=fine_tune_epochs, verbose=1)\n","        \n","#         # Save model after fine-tuning\n","#         model_save_path = os.path.join(model_save_dir, f\"{studyName}_model_trial_{trial.number}_val_accuracy_{val_accuracy:.4f}\")\n","#         model.save(model_save_path, save_format=\"tf\")\n","#         top_n_models.append((trial.number, avg_score))\n","#         top_n_models.sort(key=lambda x: x[1], reverse=True)\n","#         if len(top_n_models) > 3:\n","#             top_n_models.pop()\n","\n","#         # Make predictions on the Kaggle test dataset\n","#         kaggle_test_predictions = model.predict(kaggle_test_dataset).logits\n","#         kaggle_test_predicted_labels = tf.argmax(kaggle_test_predictions, axis=1).numpy()\n","\n","#         # Create a submission DataFrame\n","#         submission = pd.DataFrame({'id': kaggle_test_data['id'], 'target': kaggle_test_predicted_labels})\n","        \n","#         # Save the submission\n","#         submission_file = f\"{studyName}_model_trial_{trial.number}_accuracy_{val_accuracy:.4f}_avg_score_{avg_score:.4f}_f1_{val_f1_score:.4f}\" + '_submission.csv'  # Corrected naming convention\n","#         submission_path = os.path.join(model_save_dir, submission_file)\n","#         submission.to_csv(submission_path, index=False)\n","#         print(f\"Predictions saved for model: {model_save_path}\")\n","\n","#     return val_accuracy # from my experiments, I can see that val_accuracy is much better at predicting the real score than f1 or average score\n","\n","# # Define your Optuna study, using the MySQL connection string\n","# optuna_storage = 'mysql+pymysql://<username>:<password>@<host>/<database>?ssl_ca=<path_to_CA_cert>&ssl_verify_cert=true'\n","\n","# from kaggle_secrets import UserSecretsClient\n","# user_secrets = UserSecretsClient()\n","# db_password = user_secrets.get_secret(\"DB_PASSWORD\")# This uses the secrets inside of Kaggle so I don't have to explicitly type my password out in code\n","\n","# # Example with your details (replace '<password>' with your real password and '<database>' with your database name)\n","# optuna_storage = f'mysql+pymysql://MichaelAzure:{db_password}@kaggle-third-sql.mysql.database.azure.com/kaggle_disaster_database?ssl_ca=/kaggle/input/certification&ssl_verify_cert=true'\n","\n","# studyName = 'disaster_withKeyword_2'\n","# study = optuna.create_study(study_name=studyName, # name of the study\n","#                             storage=optuna_storage,  # URL for the mySQL schema\n","#                             direction='maximize', # maximize the log loss\n","#                             load_if_exists=True, # makes it so that if the study_name already exists in the schema, then it will append the new trials with the old trials and essentially resume the study. It will also remember the previous trials so it really is resuming the study\n","#                             )\n","\n","# study.optimize(objective, n_trials=10)\n","\n","# print(\"Best trial:\")\n","# trial = study.best_trial\n","# print(f\"  Value: {trial.value}\")\n","# print(\"  Params: \")\n","# for key, value in trial.params.items():\n","#     print(f\"    {key}: {value}\")\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T19:59:02.912844Z","iopub.status.busy":"2024-06-12T19:59:02.912360Z","iopub.status.idle":"2024-06-13T00:01:30.423846Z","shell.execute_reply":"2024-06-13T00:01:30.422831Z","shell.execute_reply.started":"2024-06-12T19:59:02.912819Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-06-12 19:59:10.937216: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-06-12 19:59:10.937373: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-06-12 19:59:11.110148: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid, fd = os.forkpty()\n"]},{"name":"stdout","output_type":"stream","text":["Collecting evaluate\n","  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n","Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.19.2)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\n","Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.1)\n","Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.3.1)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.23.2)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n","Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n","Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m845.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: evaluate\n","Successfully installed evaluate-0.4.2\n","Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid, fd = os.forkpty()\n"]},{"name":"stdout","output_type":"stream","text":["Collecting mysql-connector-python\n","  Downloading mysql_connector_python-8.4.0-cp310-cp310-manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n","Downloading mysql_connector_python-8.4.0-cp310-cp310-manylinux_2_17_x86_64.whl (19.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: mysql-connector-python\n","Successfully installed mysql-connector-python-8.4.0\n","Note: you may need to restart the kernel to use updated packages.\n","Collecting PyMySQL\n","  Downloading PyMySQL-1.1.1-py3-none-any.whl.metadata (4.4 kB)\n","Downloading PyMySQL-1.1.1-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m650.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: PyMySQL\n","Successfully installed PyMySQL-1.1.1\n","Note: you may need to restart the kernel to use updated packages.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"14a36ed67e914d76b8cabb744cff4c7e","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b611300f32084b5bac4ccf978093ec69","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a36d5c363d8041598020f0490edc2491","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a220f444e0864423a2ac3fc6e6a3062e","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"88aafaff83aa4a8a80c7795b7aa1b10a","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[I 2024-06-12 20:00:19,690] Using an existing study with name 'disaster_withKeyword_0' instead of creating a new one.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9be390a0ef014a96be30bcb0a3964c6c","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n","\n","Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/4\n","WARNING: AutoGraph could not transform <function infer_framework at 0x7e49152bd3f0> and will run it as-is.\n","Cause: for/else statement not yet supported\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"]},{"name":"stderr","output_type":"stream","text":["WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","I0000 00:00:1718222573.372710     151 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"]},{"name":"stdout","output_type":"stream","text":["714/714 [==============================] - 495s 474ms/step - loss: 0.4513 - accuracy: 0.8045 - f1_score: 0.6930 - val_loss: 0.3934 - val_accuracy: 0.8262 - val_f1_score: 0.7592\n","Epoch 2/4\n","714/714 [==============================] - 294s 412ms/step - loss: 0.3510 - accuracy: 0.8590 - f1_score: 0.7822 - val_loss: 0.3832 - val_accuracy: 0.8482 - val_f1_score: 0.7921\n","Epoch 3/4\n","714/714 [==============================] - 291s 408ms/step - loss: 0.2632 - accuracy: 0.8982 - f1_score: 0.8060 - val_loss: 0.4405 - val_accuracy: 0.8319 - val_f1_score: 0.8159\n","Epoch 4/4\n","714/714 [==============================] - 290s 407ms/step - loss: 0.1900 - accuracy: 0.9284 - f1_score: 0.8259 - val_loss: 0.4608 - val_accuracy: 0.8414 - val_f1_score: 0.8343\n","238/238 [==============================] - 9s 38ms/step - loss: 0.4608 - accuracy: 0.8414 - f1_score: 0.8326\n","f1 score: 0.8326383829116821 and accuracy: 0.8413865566253662\n","238/238 [==============================] - 141s 388ms/step - loss: 0.4033 - accuracy: 0.8472 - f1_score: 0.8315\n","408/408 [==============================] - 35s 36ms/step\n","Predictions saved for model: ./saved_models/disaster_withKeyword_0_model_trial_40_avg_score_0.8370\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-06-12 20:27:46,216] Trial 40 finished with value: 0.8413865566253662 and parameters: {'learning_rate': 2.6591789877871176e-05, 'batch_size': 16, 'num_epochs': 4, 'dropout_rate': 0.21076530215288186, 'weight_decay': 0.041621162953394256, 'lr_scheduler_type': 'linear'}. Best is trial 38 with value: 0.855042040348053.\n","/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n","\n","Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/4\n","714/714 [==============================] - 458s 463ms/step - loss: 0.6455 - accuracy: 0.6430 - f1_score: 0.8067 - val_loss: 0.5853 - val_accuracy: 0.7463 - val_f1_score: 0.7883\n","Epoch 2/4\n","714/714 [==============================] - 294s 412ms/step - loss: 0.5690 - accuracy: 0.7280 - f1_score: 0.7812 - val_loss: 0.5537 - val_accuracy: 0.7805 - val_f1_score: 0.7733\n","Epoch 3/4\n","714/714 [==============================] - 292s 409ms/step - loss: 0.5451 - accuracy: 0.7492 - f1_score: 0.7663 - val_loss: 0.5193 - val_accuracy: 0.7847 - val_f1_score: 0.7613\n","Epoch 4/4\n","714/714 [==============================] - 292s 409ms/step - loss: 0.5219 - accuracy: 0.7665 - f1_score: 0.7604 - val_loss: 0.6408 - val_accuracy: 0.5441 - val_f1_score: 0.7542\n","238/238 [==============================] - 9s 36ms/step - loss: 0.6408 - accuracy: 0.5441 - f1_score: 0.7493\n","f1 score: 0.749294102191925 and accuracy: 0.5441176295280457\n","238/238 [==============================] - 141s 390ms/step - loss: 0.5811 - accuracy: 0.6870 - f1_score: 0.7458\n","408/408 [==============================] - 36s 36ms/step\n","Predictions saved for model: ./saved_models/disaster_withKeyword_0_model_trial_44_avg_score_0.6467\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-06-12 20:54:31,732] Trial 44 finished with value: 0.5441176295280457 and parameters: {'learning_rate': 5.998581165035042e-05, 'batch_size': 16, 'num_epochs': 4, 'dropout_rate': 0.3448118580981549, 'weight_decay': 0.0556684022626768, 'lr_scheduler_type': 'linear'}. Best is trial 38 with value: 0.855042040348053.\n","/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n","\n","Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/3\n","714/714 [==============================] - 459s 464ms/step - loss: 0.5751 - accuracy: 0.7253 - f1_score: 0.7404 - val_loss: 0.4640 - val_accuracy: 0.8162 - val_f1_score: 0.7381\n","Epoch 2/3\n","714/714 [==============================] - 295s 413ms/step - loss: 0.4768 - accuracy: 0.8014 - f1_score: 0.7393 - val_loss: 0.5234 - val_accuracy: 0.7925 - val_f1_score: 0.7406\n","Epoch 3/3\n","714/714 [==============================] - 294s 412ms/step - loss: 0.4142 - accuracy: 0.8374 - f1_score: 0.7428 - val_loss: 0.4390 - val_accuracy: 0.8272 - val_f1_score: 0.7449\n","238/238 [==============================] - 9s 37ms/step - loss: 0.4390 - accuracy: 0.8272 - f1_score: 0.7458\n","f1 score: 0.7458329200744629 and accuracy: 0.8272058963775635\n","238/238 [==============================] - 143s 392ms/step - loss: 0.4599 - accuracy: 0.8051 - f1_score: 0.7464\n","408/408 [==============================] - 37s 36ms/step\n","Predictions saved for model: ./saved_models/disaster_withKeyword_0_model_trial_49_avg_score_0.7865\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-06-12 21:16:31,776] Trial 49 finished with value: 0.8272058963775635 and parameters: {'learning_rate': 6.701943413848717e-05, 'batch_size': 16, 'num_epochs': 3, 'dropout_rate': 0.3144233823695152, 'weight_decay': 0.05201206366911133, 'lr_scheduler_type': 'linear'}. Best is trial 38 with value: 0.855042040348053.\n","/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n","\n","Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/7\n","714/714 [==============================] - 462s 464ms/step - loss: 0.4819 - accuracy: 0.7860 - f1_score: 0.7452 - val_loss: 0.4704 - val_accuracy: 0.8104 - val_f1_score: 0.7461\n","Epoch 2/7\n","714/714 [==============================] - 295s 413ms/step - loss: 0.3989 - accuracy: 0.8336 - f1_score: 0.7479 - val_loss: 0.3861 - val_accuracy: 0.8456 - val_f1_score: 0.7494\n","Epoch 3/7\n","714/714 [==============================] - 295s 413ms/step - loss: 0.3462 - accuracy: 0.8574 - f1_score: 0.7518 - val_loss: 0.4463 - val_accuracy: 0.8351 - val_f1_score: 0.7539\n","Epoch 4/7\n","714/714 [==============================] - 294s 412ms/step - loss: 0.3082 - accuracy: 0.8828 - f1_score: 0.7569 - val_loss: 0.4067 - val_accuracy: 0.8414 - val_f1_score: 0.7596\n","Epoch 5/7\n","714/714 [==============================] - 294s 412ms/step - loss: 0.2676 - accuracy: 0.9002 - f1_score: 0.7626 - val_loss: 0.5113 - val_accuracy: 0.8141 - val_f1_score: 0.7653\n","Epoch 6/7\n","714/714 [==============================] - 293s 411ms/step - loss: 0.2323 - accuracy: 0.9124 - f1_score: 0.7682 - val_loss: 0.5008 - val_accuracy: 0.8172 - val_f1_score: 0.7709\n","Epoch 7/7\n","714/714 [==============================] - 294s 411ms/step - loss: 0.1954 - accuracy: 0.9285 - f1_score: 0.7740 - val_loss: 0.5828 - val_accuracy: 0.8120 - val_f1_score: 0.7769\n","238/238 [==============================] - 10s 39ms/step - loss: 0.5828 - accuracy: 0.8120 - f1_score: 0.7770\n","f1 score: 0.7769673466682434 and accuracy: 0.8119747638702393\n","Epoch 1/2\n","238/238 [==============================] - 143s 391ms/step - loss: 0.4114 - accuracy: 0.8288 - f1_score: 0.7771\n","Epoch 2/2\n","238/238 [==============================] - 93s 391ms/step - loss: 0.3254 - accuracy: 0.8713 - f1_score: 0.7776\n","408/408 [==============================] - 38s 37ms/step\n","Predictions saved for model: ./saved_models/disaster_withKeyword_0_model_trial_53_avg_score_0.7945\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-06-12 21:59:49,578] Trial 53 finished with value: 0.8119747638702393 and parameters: {'learning_rate': 1.0268244061380491e-05, 'batch_size': 16, 'num_epochs': 7, 'dropout_rate': 0.28189874494549116, 'weight_decay': 0.028233376080364747, 'lr_scheduler_type': 'constant'}. Best is trial 38 with value: 0.855042040348053.\n","/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n","\n","Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/3\n","357/357 [==============================] - 394s 734ms/step - loss: 0.5650 - accuracy: 0.7150 - f1_score: 0.7749 - val_loss: 0.4338 - val_accuracy: 0.8220 - val_f1_score: 0.7740\n","Epoch 2/3\n","357/357 [==============================] - 217s 607ms/step - loss: 0.4336 - accuracy: 0.8163 - f1_score: 0.7739 - val_loss: 0.4077 - val_accuracy: 0.8346 - val_f1_score: 0.7743\n","Epoch 3/3\n","357/357 [==============================] - 214s 600ms/step - loss: 0.3982 - accuracy: 0.8385 - f1_score: 0.7748 - val_loss: 0.3945 - val_accuracy: 0.8440 - val_f1_score: 0.7754\n","119/119 [==============================] - 5s 43ms/step - loss: 0.3945 - accuracy: 0.8440 - f1_score: 0.7758\n","f1 score: 0.7758095860481262 and accuracy: 0.8440126180648804\n","119/119 [==============================] - 118s 572ms/step - loss: 0.4106 - accuracy: 0.8346 - f1_score: 0.7761\n","204/204 [==============================] - 31s 45ms/step\n","Predictions saved for model: ./saved_models/disaster_withKeyword_0_model_trial_57_avg_score_0.8099\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-06-12 22:17:33,740] Trial 57 finished with value: 0.8440126180648804 and parameters: {'learning_rate': 2.1962475793711526e-06, 'batch_size': 32, 'num_epochs': 3, 'dropout_rate': 0.1678198487403304, 'weight_decay': 0.03964298756358873, 'lr_scheduler_type': 'cosine'}. Best is trial 38 with value: 0.855042040348053.\n","/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n","\n","Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/8\n","357/357 [==============================] - 388s 711ms/step - loss: 0.5548 - accuracy: 0.7295 - f1_score: 0.7729 - val_loss: 0.4386 - val_accuracy: 0.8141 - val_f1_score: 0.7722\n","Epoch 2/8\n","357/357 [==============================] - 216s 604ms/step - loss: 0.4122 - accuracy: 0.8275 - f1_score: 0.7725 - val_loss: 0.3931 - val_accuracy: 0.8409 - val_f1_score: 0.7729\n","Epoch 3/8\n","357/357 [==============================] - 214s 600ms/step - loss: 0.3748 - accuracy: 0.8459 - f1_score: 0.7735 - val_loss: 0.3909 - val_accuracy: 0.8482 - val_f1_score: 0.7742\n","Epoch 4/8\n","357/357 [==============================] - 214s 601ms/step - loss: 0.3511 - accuracy: 0.8595 - f1_score: 0.7750 - val_loss: 0.3834 - val_accuracy: 0.8519 - val_f1_score: 0.7760\n","Epoch 5/8\n","357/357 [==============================] - 213s 598ms/step - loss: 0.3297 - accuracy: 0.8678 - f1_score: 0.7769 - val_loss: 0.3856 - val_accuracy: 0.8461 - val_f1_score: 0.7778\n","Epoch 6/8\n","357/357 [==============================] - 214s 598ms/step - loss: 0.3084 - accuracy: 0.8823 - f1_score: 0.7790 - val_loss: 0.3942 - val_accuracy: 0.8487 - val_f1_score: 0.7801\n","Epoch 7/8\n","357/357 [==============================] - 214s 599ms/step - loss: 0.2881 - accuracy: 0.8898 - f1_score: 0.7813 - val_loss: 0.4019 - val_accuracy: 0.8482 - val_f1_score: 0.7824\n","Epoch 8/8\n","357/357 [==============================] - 213s 598ms/step - loss: 0.2717 - accuracy: 0.8965 - f1_score: 0.7837 - val_loss: 0.4294 - val_accuracy: 0.8388 - val_f1_score: 0.7849\n","119/119 [==============================] - 6s 45ms/step - loss: 0.4294 - accuracy: 0.8388 - f1_score: 0.7850\n","f1 score: 0.7850436568260193 and accuracy: 0.838760495185852\n","Epoch 1/3\n","119/119 [==============================] - 116s 568ms/step - loss: 0.4027 - accuracy: 0.8367 - f1_score: 0.7852\n","Epoch 2/3\n","119/119 [==============================] - 67s 566ms/step - loss: 0.3611 - accuracy: 0.8592 - f1_score: 0.7854\n","Epoch 3/3\n","119/119 [==============================] - 67s 566ms/step - loss: 0.3372 - accuracy: 0.8697 - f1_score: 0.7858\n","204/204 [==============================] - 31s 45ms/step\n","Predictions saved for model: ./saved_models/disaster_withKeyword_0_model_trial_61_avg_score_0.8119\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-06-12 22:55:14,234] Trial 61 finished with value: 0.838760495185852 and parameters: {'learning_rate': 2.0052686207370684e-06, 'batch_size': 32, 'num_epochs': 8, 'dropout_rate': 0.11918522933012374, 'weight_decay': 0.012593218792030382, 'lr_scheduler_type': 'cosine'}. Best is trial 38 with value: 0.855042040348053.\n","/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n","\n","Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/3\n","357/357 [==============================] - 385s 708ms/step - loss: 0.5250 - accuracy: 0.7572 - f1_score: 0.7846 - val_loss: 0.4141 - val_accuracy: 0.8298 - val_f1_score: 0.7842\n","Epoch 2/3\n","357/357 [==============================] - 215s 603ms/step - loss: 0.4156 - accuracy: 0.8238 - f1_score: 0.7841 - val_loss: 0.4260 - val_accuracy: 0.8162 - val_f1_score: 0.7842\n","Epoch 3/3\n","357/357 [==============================] - 215s 602ms/step - loss: 0.3805 - accuracy: 0.8464 - f1_score: 0.7846 - val_loss: 0.3901 - val_accuracy: 0.8451 - val_f1_score: 0.7849\n","119/119 [==============================] - 6s 46ms/step - loss: 0.3901 - accuracy: 0.8451 - f1_score: 0.7851\n","f1 score: 0.7850964069366455 and accuracy: 0.8450630307197571\n","119/119 [==============================] - 117s 570ms/step - loss: 0.3992 - accuracy: 0.8351 - f1_score: 0.7852\n","204/204 [==============================] - 29s 45ms/step\n","Predictions saved for model: ./saved_models/disaster_withKeyword_0_model_trial_65_avg_score_0.8151\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-06-12 23:12:48,541] Trial 65 finished with value: 0.8450630307197571 and parameters: {'learning_rate': 2.547928471571237e-06, 'batch_size': 32, 'num_epochs': 3, 'dropout_rate': 0.16603241597597937, 'weight_decay': 0.043519840123813915, 'lr_scheduler_type': 'cosine'}. Best is trial 38 with value: 0.855042040348053.\n","/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n","\n","Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/3\n","357/357 [==============================] - 382s 714ms/step - loss: 0.5803 - accuracy: 0.7119 - f1_score: 0.7832 - val_loss: 0.4760 - val_accuracy: 0.7999 - val_f1_score: 0.7822\n","Epoch 2/3\n","357/357 [==============================] - 215s 602ms/step - loss: 0.4699 - accuracy: 0.7991 - f1_score: 0.7818 - val_loss: 0.4390 - val_accuracy: 0.8267 - val_f1_score: 0.7816\n","Epoch 3/3\n","357/357 [==============================] - 214s 599ms/step - loss: 0.4357 - accuracy: 0.8145 - f1_score: 0.7816 - val_loss: 0.4187 - val_accuracy: 0.8335 - val_f1_score: 0.7815\n","119/119 [==============================] - 5s 43ms/step - loss: 0.4187 - accuracy: 0.8335 - f1_score: 0.7816\n","f1 score: 0.7816374897956848 and accuracy: 0.8335084319114685\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-06-12 23:26:36,884] Trial 69 finished with value: 0.8335084319114685 and parameters: {'learning_rate': 2.6094906120759933e-06, 'batch_size': 32, 'num_epochs': 3, 'dropout_rate': 0.3063625550283354, 'weight_decay': 0.059021215691878, 'lr_scheduler_type': 'cosine'}. Best is trial 38 with value: 0.855042040348053.\n","All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n","\n","Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/4\n","357/357 [==============================] - 383s 710ms/step - loss: 0.5744 - accuracy: 0.7103 - f1_score: 0.7795 - val_loss: 0.4462 - val_accuracy: 0.8209 - val_f1_score: 0.7786\n","Epoch 2/4\n","357/357 [==============================] - 217s 607ms/step - loss: 0.4624 - accuracy: 0.8015 - f1_score: 0.7784 - val_loss: 0.4310 - val_accuracy: 0.8262 - val_f1_score: 0.7782\n","Epoch 3/4\n","357/357 [==============================] - 214s 600ms/step - loss: 0.4279 - accuracy: 0.8185 - f1_score: 0.7782 - val_loss: 0.4195 - val_accuracy: 0.8382 - val_f1_score: 0.7782\n","Epoch 4/4\n","357/357 [==============================] - 214s 598ms/step - loss: 0.4167 - accuracy: 0.8276 - f1_score: 0.7784 - val_loss: 0.4075 - val_accuracy: 0.8398 - val_f1_score: 0.7785\n","119/119 [==============================] - 6s 45ms/step - loss: 0.4075 - accuracy: 0.8398 - f1_score: 0.7786\n","f1 score: 0.7786238193511963 and accuracy: 0.8398109078407288\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-06-12 23:43:57,724] Trial 72 finished with value: 0.8398109078407288 and parameters: {'learning_rate': 1.6729605641553753e-06, 'batch_size': 32, 'num_epochs': 4, 'dropout_rate': 0.2483768870318064, 'weight_decay': 0.07619362531911153, 'lr_scheduler_type': 'linear'}. Best is trial 38 with value: 0.855042040348053.\n","All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n","\n","Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/3\n","357/357 [==============================] - 385s 709ms/step - loss: 0.4890 - accuracy: 0.7798 - f1_score: 0.7780 - val_loss: 0.4074 - val_accuracy: 0.8367 - val_f1_score: 0.7779\n","Epoch 2/3\n","357/357 [==============================] - 215s 603ms/step - loss: 0.3865 - accuracy: 0.8446 - f1_score: 0.7782 - val_loss: 0.3900 - val_accuracy: 0.8430 - val_f1_score: 0.7786\n","Epoch 3/3\n","357/357 [==============================] - 214s 600ms/step - loss: 0.3509 - accuracy: 0.8609 - f1_score: 0.7791 - val_loss: 0.3870 - val_accuracy: 0.8477 - val_f1_score: 0.7796\n","119/119 [==============================] - 5s 42ms/step - loss: 0.3870 - accuracy: 0.8477 - f1_score: 0.7797\n","f1 score: 0.7797385454177856 and accuracy: 0.8476890921592712\n","119/119 [==============================] - 116s 569ms/step - loss: 0.3895 - accuracy: 0.8393 - f1_score: 0.7799\n","204/204 [==============================] - 31s 45ms/step\n","Predictions saved for model: ./saved_models/disaster_withKeyword_0_model_trial_75_avg_score_0.8137\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-06-13 00:01:29,824] Trial 75 finished with value: 0.8476890921592712 and parameters: {'learning_rate': 3.7553667023808855e-06, 'batch_size': 32, 'num_epochs': 3, 'dropout_rate': 0.13351924015737263, 'weight_decay': 0.03451410709391993, 'lr_scheduler_type': 'cosine'}. Best is trial 38 with value: 0.855042040348053.\n"]},{"name":"stdout","output_type":"stream","text":["Best trial:\n","  Value: 0.855042040348053\n","  Params: \n","    learning_rate: 2.244399450654266e-05\n","    batch_size: 16\n","    num_epochs: 4\n","    dropout_rate: 0.3261984812303381\n","    weight_decay: 0.04024248953184716\n","    lr_scheduler_type: linear\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import random\n","import os\n","import re\n","from transformers import set_seed, BertTokenizer, TFBertForSequenceClassification, BertConfig\n","import tensorflow as tf\n","%pip install evaluate\n","import evaluate\n","import optuna\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","import json\n","\n","# Install necessary packages for Azure SQL connection\n","%pip install mysql-connector-python \n","%pip install PyMySQL\n","\n","# Set random seeds for reproducibility\n","np.random.seed(42)\n","random.seed(42)\n","tf.random.set_seed(42)\n","set_seed(42)\n","os.environ['TF_DETERMINISTIC_OPS'] = '1'\n","\n","# Load the training data\n","train_data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n","kaggle_test_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n","\n","# Split the data into 75% training and 25% validation sets\n","train_data, val_data = train_test_split(train_data, test_size=0.25, random_state=42, stratify=train_data['target'])\n","\n","# Clean the text data\n","def clean_text(text):\n","    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n","    text = re.sub(r'@\\w+', '', text)     # Remove mentions\n","    text = re.sub(r'\\d+', '', text)      # Remove numbers\n","    text = re.sub(r'[^\\w\\s#]', '', text)  # Remove punctuation except hashtags\n","    text = text.lower()                  # Convert to lowercase\n","    return text\n","\n","train_data['clean_text'] = train_data['text'].apply(clean_text)\n","val_data['clean_text'] = val_data['text'].apply(clean_text)\n","kaggle_test_data['clean_text'] = kaggle_test_data['text'].apply(clean_text)\n","\n","# Function to combine keyword and text\n","def combine_keyword_and_text(row):\n","    keyword = str(row['keyword']) if pd.notna(row['keyword']) else ''\n","    text = row['clean_text']\n","    return '[CLS] ' + keyword + ' [SEP] ' + text + ' [SEP]'\n","\n","# Apply the function to combine keyword and text\n","train_data['combined_text'] = train_data.apply(combine_keyword_and_text, axis=1)\n","val_data['combined_text'] = val_data.apply(combine_keyword_and_text, axis=1)\n","kaggle_test_data['combined_text'] = kaggle_test_data.apply(combine_keyword_and_text, axis=1)\n","\n","# Tokenize the text data\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","def tokenize_texts(texts):\n","    return tokenizer(\n","        texts.tolist(),\n","        max_length=64,\n","        padding=True,\n","        truncation=True,\n","        return_tensors='tf'\n","    )\n","\n","# Encode the combined text data\n","train_encodings = tokenize_texts(train_data['combined_text'])\n","val_encodings = tokenize_texts(val_data['combined_text'])\n","kaggle_test_encodings = tokenize_texts(kaggle_test_data['combined_text'])\n","\n","train_labels = tf.convert_to_tensor(train_data['target'].values)\n","val_labels = tf.convert_to_tensor(val_data['target'].values)\n","\n","# Load the F1 metric from the evaluate library\n","metric = evaluate.load(\"f1\", trust_remote_code=True)\n","\n","def compute_metrics(predictions, labels):\n","    predictions = np.argmax(predictions, axis=1)\n","    f1 = metric.compute(predictions=predictions, references=labels)['f1']\n","    accuracy = accuracy_score(labels, predictions)\n","    return {'f1': f1, 'accuracy': accuracy}\n","\n","def create_tf_dataset(encodings, labels, batch_size):\n","    dataset = tf.data.Dataset.from_tensor_slices((encodings, labels))\n","    return dataset.shuffle(10000).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","# Define precision and recall metrics outside of the custom metric function\n","precision = tf.keras.metrics.Precision()\n","recall = tf.keras.metrics.Recall()\n","\n","def f1_score(y_true, y_pred):\n","    # Convert logits to predicted labels\n","    y_pred = tf.argmax(y_pred, axis=1)\n","    \n","    # Ensure true labels are in integer format\n","    y_true = tf.cast(y_true, tf.int64)\n","    \n","    # Update the state of precision and recall\n","    precision.update_state(y_true, y_pred)\n","    recall.update_state(y_true, y_pred)\n","    \n","    # Compute precision and recall values\n","    precision_result = precision.result()\n","    recall_result = recall.result()\n","    \n","    # Compute F1 score\n","    f1 = 2 * ((precision_result * recall_result) / (precision_result + recall_result + tf.keras.backend.epsilon()))\n","    \n","    return f1\n","\n","strategy = tf.distribute.MirroredStrategy()\n","\n","# Directory to save models\n","model_save_dir = './saved_models'\n","os.makedirs(model_save_dir, exist_ok=True)\n","\n","# Track top 3 models\n","top_n_models = []\n","\n","def objective(trial):\n","    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True)\n","    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n","    num_epochs = trial.suggest_int(\"num_epochs\", 3, 10)\n","    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n","    weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.1)\n","    lr_scheduler_type = trial.suggest_categorical(\"lr_scheduler_type\", [\"constant\", \"linear\", \"cosine\", \"cosine_with_restarts\"])\n","\n","    num_gpus = strategy.num_replicas_in_sync\n","\n","    train_dataset = create_tf_dataset(dict(train_encodings), train_labels, batch_size // num_gpus)\n","    val_dataset = create_tf_dataset(dict(val_encodings), val_labels, batch_size // num_gpus)\n","    kaggle_test_dataset = tf.data.Dataset.from_tensor_slices(dict(kaggle_test_encodings)).batch(batch_size // num_gpus).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","    with strategy.scope():\n","        config = BertConfig.from_pretrained('bert-base-uncased', num_labels=2, hidden_dropout_prob=dropout_rate)\n","        model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n","\n","        if lr_scheduler_type == \"linear\":\n","            lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n","                initial_learning_rate=learning_rate,\n","                decay_steps=10000,\n","                end_learning_rate=0.0,\n","                power=1.0\n","            )\n","        elif lr_scheduler_type == \"cosine\":\n","            lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n","                initial_learning_rate=learning_rate,\n","                decay_steps=10000\n","            )\n","        elif lr_scheduler_type == \"cosine_with_restarts\":\n","            lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n","                initial_learning_rate=learning_rate,\n","                first_decay_steps=1000\n","            )\n","        else:\n","            lr_schedule = learning_rate\n","\n","        optimizer = tf.keras.optimizers.experimental.AdamW(\n","            learning_rate=lr_schedule,\n","            weight_decay=weight_decay,\n","            epsilon=1e-8\n","        )\n","\n","        model.compile(optimizer=optimizer, \n","                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n","                      metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy'), f1_score])\n","\n","    model.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset, verbose=1)\n","\n","    # Evaluate on validation set\n","    val_loss, val_accuracy, val_f1_score = model.evaluate(val_dataset, verbose=1)\n","    print(f\"f1 score: {val_f1_score} and accuracy: {val_accuracy}\")\n","    \n","    avg_score = (val_accuracy + val_f1_score) / 2\n","\n","    if len(top_n_models) < 3 or avg_score > min(top_n_models, key=lambda x: x[1])[1]:  # Top-3 method\n","        model_save_path = os.path.join(model_save_dir, f\"{studyName}_model_trial_{trial.number}val_accuracy{val_accuracy:.4f}\")\n","        model.save(model_save_path, save_format=\"tf\")\n","        top_n_models.append((trial.number, avg_score))\n","        top_n_models.sort(key=lambda x: x[1], reverse=True)\n","        if len(top_n_models) > 3:\n","            top_n_models.pop()\n","\n","        # Fine-tune the model on the validation dataset\n","        fine_tune_encodings = tokenize_texts(val_data['combined_text'])\n","        fine_tune_labels = tf.convert_to_tensor(val_data['target'].values)\n","        fine_tune_dataset = tf.data.Dataset.from_tensor_slices((\n","            dict(fine_tune_encodings),\n","            fine_tune_labels\n","        )).batch(batch_size // num_gpus).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","        # Calculate the ratio of training data size to epochs\n","        training_data_size = len(train_data)\n","        fine_tune_data_size = len(val_data)\n","        fine_tune_epochs = max(1, round((fine_tune_data_size / training_data_size) * num_epochs))\n","\n","        model.fit(fine_tune_dataset, epochs=fine_tune_epochs, verbose=1)\n","\n","        # Make predictions on the Kaggle test dataset\n","        kaggle_test_predictions = model.predict(kaggle_test_dataset).logits\n","        kaggle_test_predicted_labels = tf.argmax(kaggle_test_predictions, axis=1).numpy()\n","\n","        # Create a submission DataFrame\n","        submission = pd.DataFrame({'id': kaggle_test_data['id'], 'target': kaggle_test_predicted_labels})\n","        \n","        # Save the submission\n","        submission_file = f\"{studyName}_model_trial_{trial.number}_accuracy_{val_accuracy:.4f}_avg_score_{avg_score:.4f}_f1_{val_f1_score:.4f}\" + '_submission.csv'  # Corrected naming convention\n","        submission_path = os.path.join(model_save_dir, submission_file)\n","        submission.to_csv(submission_path, index=False)\n","        print(f\"Predictions saved for model: {model_save_path}\")\n","\n","    return val_accuracy # from my experiments, I can see that val_accuracy is much better at predicting the real score than f1 or average score\n","\n","# Define your Optuna study, using the MySQL connection string\n","optuna_storage = 'mysql+pymysql://<username>:<password>@<host>/<database>?ssl_ca=<path_to_CA_cert>&ssl_verify_cert=true'\n","\n","from kaggle_secrets import UserSecretsClient\n","user_secrets = UserSecretsClient()\n","db_password = user_secrets.get_secret(\"DB_PASSWORD\")# This uses the secrets inside of Kaggle so I don't have to explicitly type my password out in code\n","\n","# Example with your details (replace '<password>' with your real password and '<database>' with your database name)\n","optuna_storage = f'mysql+pymysql://MichaelAzure:{db_password}@kaggle-third-sql.mysql.database.azure.com/kaggle_disaster_database?ssl_ca=/kaggle/input/certification&ssl_verify_cert=true'\n","\n","studyName = 'disaster_withKeyword_0'\n","study = optuna.create_study(study_name=studyName, # name of the study\n","                            storage=optuna_storage,  # URL for the mySQL schema\n","                            direction='maximize', # maximize the log loss\n","                            load_if_exists=True, # makes it so that if the study_name already exists in the schema, then it will append the new trials with the old trials and essentially resume the study. It will also remember the previous trials so it really is resuming the study\n","                            )\n","\n","study.optimize(objective, n_trials=10)\n","\n","print(\"Best trial:\")\n","trial = study.best_trial\n","print(f\"  Value: {trial.value}\")\n","print(\"  Params: \")\n","for key, value in trial.params.items():\n","    print(f\"    {key}: {value}\")\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":869809,"sourceId":17777,"sourceType":"competition"},{"datasetId":5157167,"sourceId":8616390,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
