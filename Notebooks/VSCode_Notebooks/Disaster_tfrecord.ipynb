{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from transformers import set_seed, BertTokenizer, TFBertForSequenceClassification, BertConfig\n",
    "import tensorflow as tf\n",
    "%pip install optuna\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "set_seed(42)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "# Install necessary packages for Azure SQL connection\n",
    "%pip install mysql-connector-python \n",
    "%pip install PyMySQL\n",
    "\n",
    "# Initialize TPU\n",
    "try:\n",
    "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(resolver)\n",
    "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "    strategy = tf.distribute.TPUStrategy(resolver)\n",
    "    tpu_cores = strategy.num_replicas_in_sync\n",
    "    print(f\"TPU cores available: {tpu_cores}\")\n",
    "except ValueError:\n",
    "    print(\"TPU not found\")\n",
    "    raise SystemExit\n",
    "\n",
    "# Set fixed batch size and learning rate parameters\n",
    "base_learning_rate = 1e-5\n",
    "batch_size_per_core = 32\n",
    "tpu_cores = 8\n",
    "batch_size = batch_size_per_core * tpu_cores\n",
    "learning_rate = base_learning_rate * (batch_size / (batch_size_per_core * tpu_cores))\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "kaggle_test_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "\n",
    "# Split the data into 75% training and 25% validation sets\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.25, random_state=42, stratify=train_data['target'])\n",
    "\n",
    "# Clean the text data\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)     # Remove mentions\n",
    "    text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s#]', '', text)  # Remove punctuation except hashtags\n",
    "    text = text.lower()                  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "train_data['clean_text'] = train_data['text'].apply(clean_text)\n",
    "val_data['clean_text'] = val_data['text'].apply(clean_text)\n",
    "kaggle_test_data['clean_text'] = kaggle_test_data['text'].apply(clean_text)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_texts(texts):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        max_length=64,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "# # Convert dataframes to TFRecord files\n",
    "# def dataframe_to_tfrecord(dataframe, tfrecord_file, include_target=True):\n",
    "#     with tf.io.TFRecordWriter(tfrecord_file) as writer:\n",
    "#         for _, row in dataframe.iterrows():\n",
    "#             feature = {\n",
    "#                 'id': tf.train.Feature(int64_list=tf.train.Int64List(value=[row['id']])),\n",
    "#                 'clean_text': tf.train.Feature(bytes_list=tf.train.BytesList(value=[row['clean_text'].encode('utf-8')])),\n",
    "#             }\n",
    "#             if include_target:\n",
    "#                 feature['target'] = tf.train.Feature(int64_list=tf.train.Int64List(value=[row['target']]))\n",
    "#             example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "#             writer.write(example.SerializeToString())\n",
    "\n",
    "# dataframe_to_tfrecord(train_data[['id', 'clean_text', 'target']], 'train_data.tfrecord')\n",
    "# dataframe_to_tfrecord(val_data[['id', 'clean_text', 'target']], 'val_data.tfrecord')\n",
    "# dataframe_to_tfrecord(val_data[['id', 'clean_text', 'target']], 'fine_tune_data.tfrecord')\n",
    "# dataframe_to_tfrecord(kaggle_test_data[['id', 'clean_text']], 'kaggle_test_data.tfrecord', include_target=False)\n",
    "\n",
    "# Parse TFRecord files and create datasets\n",
    "def parse_tfrecord_fn(example, include_target=True):\n",
    "    feature_description = {\n",
    "        'id': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'clean_text': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    if include_target:\n",
    "        feature_description['target'] = tf.io.FixedLenFeature([], tf.int64)\n",
    "    \n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    example['clean_text'] = tf.strings.reduce_join(example['clean_text'])\n",
    "    \n",
    "    if include_target:\n",
    "        return example['clean_text'], example['target']\n",
    "    return example['clean_text']\n",
    "\n",
    "def load_tfrecord_dataset(file_pattern, batch_size, include_target=True, repeat=True):\n",
    "    files = tf.data.Dataset.list_files(file_pattern)\n",
    "    dataset = files.interleave(tf.data.TFRecordDataset, cycle_length=4)\n",
    "    dataset = dataset.map(lambda x: parse_tfrecord_fn(x, include_target), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    if repeat:\n",
    "        dataset = dataset.repeat()\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "train_tfrecord_dataset = load_tfrecord_dataset('/kaggle/input/tfrecord-disaster/train_data.tfrecord', batch_size)\n",
    "val_tfrecord_dataset = load_tfrecord_dataset('/kaggle/input/tfrecord-disaster/val_data.tfrecord', batch_size)\n",
    "fine_tune_tfrecord_dataset = load_tfrecord_dataset('/kaggle/input/tfrecord-disaster/fine_tune_data.tfrecord', batch_size)\n",
    "kaggle_test_tfrecord_dataset = load_tfrecord_dataset('/kaggle/input/tfrecord-disaster/kaggle_test_data.tfrecord', batch_size, include_target=False, repeat=False)  # No repeat for test dataset\n",
    "\n",
    "# Tokenize datasets\n",
    "def tokenize_tfrecord_dataset(dataset, include_target=True):\n",
    "    def tokenize_fn(text, target=None):\n",
    "        encodings = tokenizer(\n",
    "            [str(t, 'utf-8') for t in text.numpy()],\n",
    "            max_length=64,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        if include_target:\n",
    "            return encodings['input_ids'], target\n",
    "        return encodings['input_ids']\n",
    "\n",
    "    def map_fn(text, target=None):\n",
    "        if include_target:\n",
    "            input_ids, target = tf.py_function(tokenize_fn, [text, target], [tf.int32, tf.int64])\n",
    "            input_ids.set_shape([None, 64])\n",
    "            target.set_shape([None])\n",
    "            return {'input_ids': input_ids}, target\n",
    "        input_ids = tf.py_function(tokenize_fn, [text], tf.int32)\n",
    "        input_ids.set_shape([None, 64])\n",
    "        return {'input_ids': input_ids}\n",
    "\n",
    "    if include_target:\n",
    "        return dataset.map(map_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset.map(map_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_tfrecord_dataset = tokenize_tfrecord_dataset(train_tfrecord_dataset)\n",
    "val_tfrecord_dataset = tokenize_tfrecord_dataset(val_tfrecord_dataset)\n",
    "fine_tune_tfrecord_dataset = tokenize_tfrecord_dataset(fine_tune_tfrecord_dataset)\n",
    "kaggle_test_tfrecord_dataset = tokenize_tfrecord_dataset(kaggle_test_tfrecord_dataset, include_target=False)\n",
    "\n",
    "# Directory to save models\n",
    "model_save_dir = './saved_models'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# File to store top 5 model predictions\n",
    "top_predictions_file = 'top_5_predictions.json'\n",
    "\n",
    "# Load existing top 5 predictions\n",
    "if os.path.exists(top_predictions_file):\n",
    "    with open(top_predictions_file, 'r') as file:\n",
    "        top_predictions = json.load(file)\n",
    "else:\n",
    "    top_predictions = []\n",
    "\n",
    "# Function to save top predictions\n",
    "def save_top_predictions(pre_fine_tuning_file, post_fine_tuning_file, val_accuracy, model_number):\n",
    "    global top_predictions\n",
    "\n",
    "    new_entry = {\n",
    "        'model_number': model_number,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'pre_fine_tuning_file': pre_fine_tuning_file,\n",
    "        'post_fine_tuning_file': post_fine_tuning_file\n",
    "    }\n",
    "\n",
    "    # Add the new entry and sort by validation accuracy\n",
    "    top_predictions.append(new_entry)\n",
    "    top_predictions = sorted(top_predictions, key=lambda x: x['val_accuracy'], reverse=True)\n",
    "\n",
    "    # If there are more than 5 entries, remove the one with the lowest accuracy\n",
    "    if len(top_predictions) > 5:\n",
    "        removed_entry = top_predictions.pop()\n",
    "        # Check if the files exist before attempting to remove them\n",
    "        if os.path.exists(removed_entry['pre_fine_tuning_file']):\n",
    "            os.remove(removed_entry['pre_fine_tuning_file'])\n",
    "            print(f\"File {removed_entry['pre_fine_tuning_file']} has been removed.\")\n",
    "        else:\n",
    "            print(f\"File {removed_entry['pre_fine_tuning_file']} does not exist and cannot be removed.\")\n",
    "        \n",
    "        if os.path.exists(removed_entry['post_fine_tuning_file']):\n",
    "            os.remove(removed_entry['post_fine_tuning_file'])\n",
    "            print(f\"File {removed_entry['post_fine_tuning_file']} has been removed.\")\n",
    "        else:\n",
    "            print(f\"File {removed_entry['post_fine_tuning_file']} does not exist and cannot be removed.\")\n",
    "\n",
    "    # Save the updated top predictions to file\n",
    "    with open(top_predictions_file, 'w') as file:\n",
    "        json.dump(top_predictions, file, indent=4)\n",
    "\n",
    "\n",
    "# Define steps per epoch and validation steps\n",
    "steps_per_epoch = len(train_data) // batch_size\n",
    "validation_steps = len(val_data) // batch_size\n",
    "fine_tune_steps_per_epoch = len(val_data) // batch_size  # Added step calculation for fine-tuning dataset\n",
    "\n",
    "def objective(trial):\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 1, 20)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.1)\n",
    "    lr_scheduler_type = trial.suggest_categorical(\"lr_scheduler_type\", [\"constant\", \"linear\", \"cosine\", \"cosine_with_restarts\"])\n",
    "    gradient_clip_norm = trial.suggest_float(\"gradient_clip_norm\", 0.0, 1.0)\n",
    "\n",
    "    with strategy.scope():\n",
    "        precision = tf.keras.metrics.Precision()\n",
    "        recall = tf.keras.metrics.Recall()\n",
    "\n",
    "        def f1_score_custom(y_true, y_pred):\n",
    "            y_pred = tf.argmax(y_pred, axis=1)\n",
    "            y_true = tf.cast(y_true, tf.int64)\n",
    "            precision.update_state(y_true, y_pred)\n",
    "            recall.update_state(y_true, y_pred)\n",
    "            precision_result = precision.result()\n",
    "            recall_result = recall.result()\n",
    "            f1 = 2 * ((precision_result * recall_result) / (precision_result + recall_result + tf.keras.backend.epsilon()))\n",
    "            return f1\n",
    "\n",
    "        config = BertConfig.from_pretrained('bert-base-uncased', num_labels=2, hidden_dropout_prob=dropout_rate)\n",
    "        model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
    "\n",
    "        if lr_scheduler_type == \"linear\":\n",
    "            lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "                initial_learning_rate=learning_rate,\n",
    "                decay_steps=10000,\n",
    "                end_learning_rate=0.0,\n",
    "                power=1.0\n",
    "            )\n",
    "        elif lr_scheduler_type == \"cosine\":\n",
    "            lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "                initial_learning_rate=learning_rate,\n",
    "                decay_steps=10000\n",
    "            )\n",
    "        elif lr_scheduler_type == \"cosine_with_restarts\":\n",
    "            lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "                initial_learning_rate=learning_rate,\n",
    "                first_decay_steps=1000\n",
    "            )\n",
    "        else:\n",
    "            lr_schedule = learning_rate\n",
    "\n",
    "        optimizer = tf.keras.optimizers.experimental.AdamW(\n",
    "            learning_rate=lr_schedule,\n",
    "            weight_decay=weight_decay,\n",
    "            epsilon=1e-8,\n",
    "            clipnorm=gradient_clip_norm\n",
    "        )\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer, \n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "            metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy'), f1_score_custom],\n",
    "            steps_per_execution=1\n",
    "        )\n",
    "\n",
    "    model.fit(train_tfrecord_dataset, epochs=num_epochs, validation_data=val_tfrecord_dataset, steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, verbose=1)\n",
    "\n",
    "    val_loss, val_accuracy, val_f1_score = model.evaluate(val_tfrecord_dataset, steps=validation_steps, verbose=1)\n",
    "    print(f\"f1 score: {val_f1_score} and accuracy: {val_accuracy}\")\n",
    "    \n",
    "    avg_score = (val_accuracy + val_f1_score) / 2\n",
    "\n",
    "    if len(top_predictions) < 5 or val_accuracy > min(top_predictions, key=lambda x: x['val_accuracy'])['val_accuracy']:\n",
    "        kaggle_test_predictions = model.predict(kaggle_test_tfrecord_dataset, steps=(len(kaggle_test_data) + batch_size - 1) // batch_size).logits\n",
    "        \n",
    "        # Ensure the prediction length matches the test data length\n",
    "        kaggle_test_predicted_labels = tf.argmax(kaggle_test_predictions, axis=1).numpy()[:len(kaggle_test_data)]\n",
    "        \n",
    "        pre_fine_tuning_predictions_file = os.path.join(model_save_dir, f\"{studyName}_model_trial_{trial.number}_accuracy_{val_accuracy:.4f}_avg_score_{avg_score:.4f}_f1_{val_f1_score:.4f}_pre_fine_tuning_submission.csv\") \n",
    "        submission = pd.DataFrame({'id': kaggle_test_data['id'], 'target': kaggle_test_predicted_labels})\n",
    "        submission.to_csv(pre_fine_tuning_predictions_file, index=False)\n",
    "\n",
    "        training_data_size = len(train_data)\n",
    "        fine_tune_data_size = len(val_data)\n",
    "        fine_tune_epochs = max(1, round((fine_tune_data_size / training_data_size) * num_epochs))\n",
    "\n",
    "        model.fit(fine_tune_tfrecord_dataset, epochs=fine_tune_epochs, steps_per_epoch=fine_tune_steps_per_epoch, verbose=1)  # Added steps_per_epoch\n",
    "\n",
    "        kaggle_test_predictions = model.predict(kaggle_test_tfrecord_dataset, steps=(len(kaggle_test_data) + batch_size - 1) // batch_size).logits\n",
    "        \n",
    "        # Ensure the prediction length matches the test data length\n",
    "        kaggle_test_predicted_labels = tf.argmax(kaggle_test_predictions, axis=1).numpy()[:len(kaggle_test_data)]\n",
    "        \n",
    "        post_fine_tuning_predictions_file = os.path.join(model_save_dir, f\"{studyName}_model_trial_{trial.number}_accuracy_{val_accuracy:.4f}_avg_score_{avg_score:.4f}_f1_{val_f1_score:.4f}_post_fine_tuning_submission.csv\") \n",
    "        submission = pd.DataFrame({'id': kaggle_test_data['id'], 'target': kaggle_test_predicted_labels})\n",
    "        submission.to_csv(post_fine_tuning_predictions_file, index=False)\n",
    "\n",
    "        save_top_predictions(pre_fine_tuning_predictions_file, post_fine_tuning_predictions_file, val_accuracy, trial.number)\n",
    "\n",
    "    return val_accuracy\n",
    "\n",
    "\n",
    "# Define your Optuna study, using the MySQL connection string\n",
    "optuna_storage = 'mysql+pymysql://<username>:<password>@<host>/<database>?ssl_ca=<path_to_CA_cert>&ssl_verify_cert=true'\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "db_password = user_secrets.get_secret(\"DB_PASSWORD\")\n",
    "\n",
    "optuna_storage = f'mysql+pymysql://MichaelAzure:{db_password}@kaggle-third-sql.mysql.database.azure.com/kaggle_disaster_database?ssl_ca=/kaggle/input/certification&ssl_verify_cert=true'\n",
    "\n",
    "studyName = 'disaster_tfrecord_BERT_1'\n",
    "study = optuna.create_study(study_name=studyName,\n",
    "                            storage=optuna_storage,\n",
    "                            direction='maximize',\n",
    "                            load_if_exists=True)\n",
    "\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value: {trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "output_top_predictions_file = '/kaggle/working/top_5_predictions.json'\n",
    "with open(output_top_predictions_file, 'w') as file:\n",
    "    json.dump(top_predictions, file, indent=4)\n",
    "\n",
    "print(f\"Top 5 predictions saved to {output_top_predictions_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from transformers import set_seed, BertTokenizer, TFBertForSequenceClassification, BertConfig\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "set_seed(42)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "# Install necessary packages for Azure SQL connection\n",
    "%pip install mysql-connector-python \n",
    "%pip install PyMySQL\n",
    "%pip install optuna\n",
    "\n",
    "import optuna\n",
    "\n",
    "# Initialize TPU\n",
    "try:\n",
    "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(resolver)\n",
    "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "    strategy = tf.distribute.TPUStrategy(resolver)\n",
    "    tpu_cores = strategy.num_replicas_in_sync\n",
    "    print(f\"TPU cores available: {tpu_cores}\")\n",
    "except ValueError:\n",
    "    print(\"TPU not found\")\n",
    "    raise SystemExit\n",
    "\n",
    "# Enable mixed precision\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# Set fixed batch size and learning rate parameters\n",
    "base_learning_rate = 1e-5\n",
    "batch_size_per_core = 32\n",
    "tpu_cores = 8\n",
    "batch_size = batch_size_per_core * tpu_cores\n",
    "learning_rate = base_learning_rate * (batch_size / (batch_size_per_core * tpu_cores))\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "kaggle_test_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "\n",
    "# Split the data into 75% training and 25% validation sets\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.25, random_state=42, stratify=train_data['target'])\n",
    "\n",
    "# Clean the text data\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)     # Remove mentions\n",
    "    text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s#]', '', text)  # Remove punctuation except hashtags\n",
    "    text = text.lower()                  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "train_data['clean_text'] = train_data['text'].apply(clean_text)\n",
    "val_data['clean_text'] = val_data['text'].apply(clean_text)\n",
    "kaggle_test_data['clean_text'] = kaggle_test_data['text'].apply(clean_text)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_texts(texts):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        max_length=64,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "# Parse TFRecord files and create datasets\n",
    "def parse_tfrecord_fn(example, include_target=True):\n",
    "    feature_description = {\n",
    "        'id': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'clean_text': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    if include_target:\n",
    "        feature_description['target'] = tf.io.FixedLenFeature([], tf.int64)\n",
    "    \n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    example['clean_text'] = tf.strings.reduce_join(example['clean_text'])\n",
    "    \n",
    "    if include_target:\n",
    "        return example['clean_text'], example['target']\n",
    "    return example['clean_text']\n",
    "\n",
    "def load_tfrecord_dataset(file_pattern, batch_size, include_target=True, repeat=True):\n",
    "    files = tf.data.Dataset.list_files(file_pattern)\n",
    "    dataset = files.interleave(tf.data.TFRecordDataset, cycle_length=4)\n",
    "    dataset = dataset.map(lambda x: parse_tfrecord_fn(x, include_target), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    if repeat:\n",
    "        dataset = dataset.repeat()\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "train_tfrecord_dataset = load_tfrecord_dataset('/kaggle/input/tfrecord-disaster/train_data.tfrecord', batch_size)\n",
    "val_tfrecord_dataset = load_tfrecord_dataset('/kaggle/input/tfrecord-disaster/val_data.tfrecord', batch_size)\n",
    "fine_tune_tfrecord_dataset = load_tfrecord_dataset('/kaggle/input/tfrecord-disaster/fine_tune_data.tfrecord', batch_size)\n",
    "kaggle_test_tfrecord_dataset = load_tfrecord_dataset('/kaggle/input/tfrecord-disaster/kaggle_test_data.tfrecord', batch_size, include_target=False, repeat=False)  # No repeat for test dataset\n",
    "\n",
    "# Tokenize datasets\n",
    "def tokenize_tfrecord_dataset(dataset, include_target=True):\n",
    "    def tokenize_fn(text, target=None):\n",
    "        encodings = tokenizer(\n",
    "            [str(t, 'utf-8') for t in text.numpy()],\n",
    "            max_length=64,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        if include_target:\n",
    "            return encodings['input_ids'], target\n",
    "        return encodings['input_ids']\n",
    "\n",
    "    def map_fn(text, target=None):\n",
    "        if include_target:\n",
    "            input_ids, target = tf.py_function(tokenize_fn, [text, target], [tf.int32, tf.int64])\n",
    "            input_ids.set_shape([None, 64])\n",
    "            target.set_shape([None])\n",
    "            return {'input_ids': input_ids}, target\n",
    "        input_ids = tf.py_function(tokenize_fn, [text], tf.int32)\n",
    "        input_ids.set_shape([None, 64])\n",
    "        return {'input_ids': input_ids}\n",
    "\n",
    "    if include_target:\n",
    "        return dataset.map(map_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset.map(map_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_tfrecord_dataset = tokenize_tfrecord_dataset(train_tfrecord_dataset)\n",
    "val_tfrecord_dataset = tokenize_tfrecord_dataset(val_tfrecord_dataset)\n",
    "fine_tune_tfrecord_dataset = tokenize_tfrecord_dataset(fine_tune_tfrecord_dataset)\n",
    "kaggle_test_tfrecord_dataset = tokenize_tfrecord_dataset(kaggle_test_tfrecord_dataset, include_target=False)\n",
    "\n",
    "# Directory to save models\n",
    "model_save_dir = './saved_models'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# File to store top 5 model predictions\n",
    "top_predictions_file = 'top_5_predictions.json'\n",
    "\n",
    "# Load existing top 5 predictions\n",
    "if os.path.exists(top_predictions_file):\n",
    "    with open(top_predictions_file, 'r') as file:\n",
    "        top_predictions = json.load(file)\n",
    "else:\n",
    "    top_predictions = []\n",
    "\n",
    "# Function to save top predictions\n",
    "def save_top_predictions(pre_fine_tuning_file, post_fine_tuning_file, val_accuracy, model_number):\n",
    "    global top_predictions\n",
    "\n",
    "    new_entry = {\n",
    "        'model_number': model_number,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'pre_fine_tuning_file': pre_fine_tuning_file,\n",
    "        'post_fine_tuning_file': post_fine_tuning_file\n",
    "    }\n",
    "\n",
    "    # Add the new entry and sort by validation accuracy\n",
    "    top_predictions.append(new_entry)\n",
    "    top_predictions = sorted(top_predictions, key=lambda x: x['val_accuracy'], reverse=True)\n",
    "\n",
    "    # If there are more than 5 entries, remove the one with the lowest accuracy\n",
    "    if len(top_predictions) > 5:\n",
    "        removed_entry = top_predictions.pop()\n",
    "        # Check if the files exist before attempting to remove them\n",
    "        if os.path.exists(removed_entry['pre_fine_tuning_file']):\n",
    "            os.remove(removed_entry['pre_fine_tuning_file'])\n",
    "            print(f\"File {removed_entry['pre_fine_tuning_file']} has been removed.\")\n",
    "        else:\n",
    "            print(f\"File {removed_entry['pre_fine_tuning_file']} does not exist and cannot be removed.\")\n",
    "        \n",
    "        if os.path.exists(removed_entry['post_fine_tuning_file']):\n",
    "            os.remove(removed_entry['post_fine_tuning_file'])\n",
    "            print(f\"File {removed_entry['post_fine_tuning_file']} has been removed.\")\n",
    "        else:\n",
    "            print(f\"File {removed_entry['post_fine_tuning_file']} does not exist and cannot be removed.\")\n",
    "\n",
    "    # Save the updated top predictions to file\n",
    "    with open(top_predictions_file, 'w') as file:\n",
    "        json.dump(top_predictions, file, indent=4)\n",
    "\n",
    "\n",
    "# Define steps per epoch and validation steps\n",
    "steps_per_epoch = len(train_data) // batch_size\n",
    "validation_steps = len(val_data) // batch_size\n",
    "fine_tune_steps_per_epoch = len(val_data) // batch_size  # Added step calculation for fine-tuning dataset\n",
    "\n",
    "def objective(trial):\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 1, 20)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.1)\n",
    "    lr_scheduler_type = trial.suggest_categorical(\"lr_scheduler_type\", [\"constant\", \"linear\", \"cosine\", \"cosine_with_restarts\"])\n",
    "    gradient_clip_norm = trial.suggest_float(\"gradient_clip_norm\", 0.0, 1.0)\n",
    "\n",
    "    with strategy.scope():\n",
    "        precision = tf.keras.metrics.Precision()\n",
    "        recall = tf.keras.metrics.Recall()\n",
    "\n",
    "        def f1_score_custom(y_true, y_pred):\n",
    "            y_pred = tf.argmax(y_pred, axis=1)\n",
    "            y_true = tf.cast(y_true, tf.int64)\n",
    "            precision.update_state(y_true, y_pred)\n",
    "            recall.update_state(y_true, y_pred)\n",
    "            precision_result = precision.result()\n",
    "            recall_result = recall.result()\n",
    "            f1 = 2 * ((precision_result * recall_result) / (precision_result + recall_result + tf.keras.backend.epsilon()))\n",
    "            return f1\n",
    "\n",
    "        config = BertConfig.from_pretrained('bert-base-uncased', num_labels=2, hidden_dropout_prob=dropout_rate)\n",
    "        model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
    "\n",
    "        if lr_scheduler_type == \"linear\":\n",
    "            lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "                initial_learning_rate=learning_rate,\n",
    "                decay_steps=10000,\n",
    "                end_learning_rate=0.0,\n",
    "                power=1.0\n",
    "            )\n",
    "        elif lr_scheduler_type == \"cosine\":\n",
    "            lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "                initial_learning_rate=learning_rate,\n",
    "                decay_steps=10000\n",
    "            )\n",
    "        elif lr_scheduler_type == \"cosine_with_restarts\":\n",
    "            lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "                initial_learning_rate=learning_rate,\n",
    "                first_decay_steps=1000\n",
    "            )\n",
    "        else:\n",
    "            lr_schedule = learning_rate\n",
    "\n",
    "        optimizer = tf.keras.optimizers.experimental.AdamW(\n",
    "            learning_rate=lr_schedule,\n",
    "            weight_decay=weight_decay,\n",
    "            epsilon=1e-8,\n",
    "            clipnorm=gradient_clip_norm\n",
    "        )\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer, \n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "            metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy'), f1_score_custom],\n",
    "            steps_per_execution=1\n",
    "        )\n",
    "\n",
    "    model.fit(train_tfrecord_dataset, epochs=num_epochs, validation_data=val_tfrecord_dataset, steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, verbose=1)\n",
    "\n",
    "    val_loss, val_accuracy, val_f1_score = model.evaluate(val_tfrecord_dataset, steps=validation_steps, verbose=1)\n",
    "    print(f\"f1 score: {val_f1_score} and accuracy: {val_accuracy}\")\n",
    "    \n",
    "    avg_score = (val_accuracy + val_f1_score) / 2\n",
    "\n",
    "    if len(top_predictions) < 5 or val_accuracy > min(top_predictions, key=lambda x: x['val_accuracy'])['val_accuracy']:\n",
    "        kaggle_test_predictions = model.predict(kaggle_test_tfrecord_dataset, steps=(len(kaggle_test_data) + batch_size - 1) // batch_size).logits\n",
    "        \n",
    "        # Ensure the prediction length matches the test data length\n",
    "        kaggle_test_predicted_labels = tf.argmax(kaggle_test_predictions, axis=1).numpy()[:len(kaggle_test_data)]\n",
    "        \n",
    "        pre_fine_tuning_predictions_file = os.path.join(model_save_dir, f\"{studyName}_model_trial_{trial.number}_accuracy_{val_accuracy:.4f}_avg_score_{avg_score:.4f}_f1_{val_f1_score:.4f}_pre_fine_tuning_submission.csv\") \n",
    "        submission = pd.DataFrame({'id': kaggle_test_data['id'], 'target': kaggle_test_predicted_labels})\n",
    "        submission.to_csv(pre_fine_tuning_predictions_file, index=False)\n",
    "\n",
    "        training_data_size = len(train_data)\n",
    "        fine_tune_data_size = len(val_data)\n",
    "        fine_tune_epochs = max(1, round((fine_tune_data_size / training_data_size) * num_epochs))\n",
    "\n",
    "        model.fit(fine_tune_tfrecord_dataset, epochs=fine_tune_epochs, steps_per_epoch=fine_tune_steps_per_epoch, verbose=1)  # Added steps_per_epoch\n",
    "\n",
    "        kaggle_test_predictions = model.predict(kaggle_test_tfrecord_dataset, steps=(len(kaggle_test_data) + batch_size - 1) // batch_size).logits\n",
    "        \n",
    "        # Ensure the prediction length matches the test data length\n",
    "        kaggle_test_predicted_labels = tf.argmax(kaggle_test_predictions, axis=1).numpy()[:len(kaggle_test_data)]\n",
    "        \n",
    "        post_fine_tuning_predictions_file = os.path.join(model_save_dir, f\"{studyName}_model_trial_{trial.number}_accuracy_{val_accuracy:.4f}_avg_score_{avg_score:.4f}_f1_{val_f1_score:.4f}_post_fine_tuning_submission.csv\") \n",
    "        submission = pd.DataFrame({'id': kaggle_test_data['id'], 'target': kaggle_test_predicted_labels})\n",
    "        submission.to_csv(post_fine_tuning_predictions_file, index=False)\n",
    "\n",
    "        save_top_predictions(pre_fine_tuning_predictions_file, post_fine_tuning_predictions_file, val_accuracy, trial.number)\n",
    "\n",
    "    return val_accuracy\n",
    "\n",
    "\n",
    "# Define your Optuna study, using the MySQL connection string\n",
    "optuna_storage = 'mysql+pymysql://<username>:<password>@<host>/<database>?ssl_ca=<path_to_CA_cert>&ssl_verify_cert=true'\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "db_password = user_secrets.get_secret(\"DB_PASSWORD\")\n",
    "\n",
    "optuna_storage = f'mysql+pymysql://MichaelAzure:{db_password}@kaggle-third-sql.mysql.database.azure.com/kaggle_disaster_database?ssl_ca=/kaggle/input/certification&ssl_verify_cert=true'\n",
    "\n",
    "studyName = 'disaster_tfrecord_BERT_mixed2_0'\n",
    "study = optuna.create_study(study_name=studyName,\n",
    "                            storage=optuna_storage,\n",
    "                            direction='maximize',\n",
    "                            load_if_exists=True)\n",
    "\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value: {trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "output_top_predictions_file = '/kaggle/working/top_5_predictions.json'\n",
    "with open(output_top_predictions_file, 'w') as file:\n",
    "    json.dump(top_predictions, file, indent=4)\n",
    "\n",
    "print(f\"Top 5 predictions saved to {output_top_predictions_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate whether the code properly uses mixed precision for TensorFlow, I'll outline the key steps and components needed for mixed precision training in TensorFlow. Then, you can compare these steps to your code to determine if anything is missing or needs adjustment.\n",
    "\n",
    "1. **Ensure Compatibility**: Check if your hardware (GPU) supports mixed precision. NVIDIA GPUs with compute capability 7.0 or higher (Volta, Turing, Ampere architectures) support mixed precision.\n",
    "\n",
    "2. **Update TensorFlow**: Ensure you're using TensorFlow 2.1 or newer, as mixed precision is officially supported from TensorFlow 2.1 onwards.\n",
    "\n",
    "3. **Enable Mixed Precision**: Use the `tf.keras.mixed_precision` API to set the global policy to 'mixed_float16'.\n",
    "\n",
    "   ```python\n",
    "   from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "   policy = mixed_precision.Policy('mixed_float16')\n",
    "   mixed_precision.set_policy(policy)\n",
    "   ```\n",
    "\n",
    "4. **Model Adjustments**:\n",
    "   - Ensure the model's output layer has a float32 dtype. Mixed precision can lead to loss in precision in the final outputs, so it's common practice to cast the last layer back to float32.\n",
    "   - Use `dtype='float32'` for loss scaling to maintain precision in loss calculations.\n",
    "\n",
    "5. **Optimizer Adjustments**:\n",
    "   - Wrap the optimizer in a `mixed_precision.LossScaleOptimizer`. This is crucial for preventing underflow in gradients when using float16.\n",
    "\n",
    "   ```python\n",
    "   optimizer = mixed_precision.LossScaleOptimizer(optimizer)\n",
    "   ```\n",
    "\n",
    "6. **Adjust Batch Size** (Optional): With mixed precision, you might be able to use a larger batch size due to reduced memory usage. However, this depends on your model and system.\n",
    "\n",
    "7. **Monitor Training**: Keep an eye on the training process. Mixed precision can sometimes lead to instability or convergence issues, so it's important to verify that the model trains as expected.\n",
    "\n",
    "If your code includes these steps, it's likely properly configured for mixed precision training in TensorFlow. If any of these steps are missing or incorrectly implemented, you might need to adjust your code accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from transformers import set_seed, BertTokenizer, TFBertForSequenceClassification, BertConfig\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "set_seed(42)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "# Install necessary packages for Azure SQL connection\n",
    "%pip install mysql-connector-python \n",
    "%pip install PyMySQL\n",
    "%pip install optuna\n",
    "\n",
    "import optuna\n",
    "\n",
    "# Initialize TPU\n",
    "try:\n",
    "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(resolver)\n",
    "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "    strategy = tf.distribute.TPUStrategy(resolver)\n",
    "    tpu_cores = strategy.num_replicas_in_sync\n",
    "    print(f\"TPU cores available: {tpu_cores}\")\n",
    "except ValueError:\n",
    "    print(\"TPU not found\")\n",
    "    raise SystemExit\n",
    "\n",
    "# Enable mixed precision\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# Set fixed batch size and learning rate parameters\n",
    "base_learning_rate = 1e-5\n",
    "batch_size_per_core = 32\n",
    "tpu_cores = 8\n",
    "batch_size = batch_size_per_core * tpu_cores\n",
    "learning_rate = base_learning_rate * (batch_size / (batch_size_per_core * tpu_cores))\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "kaggle_test_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "\n",
    "# Split the data into 75% training and 25% validation sets\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.25, random_state=42, stratify=train_data['target'])\n",
    "\n",
    "# Clean the text data\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)     # Remove mentions\n",
    "    text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s#]', '', text)  # Remove punctuation except hashtags\n",
    "    text = text.lower()                  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "train_data['clean_text'] = train_data['text'].apply(clean_text)\n",
    "val_data['clean_text'] = val_data['text'].apply(clean_text)\n",
    "kaggle_test_data['clean_text'] = kaggle_test_data['text'].apply(clean_text)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_texts(texts):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        max_length=64,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "# Parse TFRecord files and create datasets\n",
    "def parse_tfrecord_fn(example, include_target=True):\n",
    "    feature_description = {\n",
    "        'id': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'clean_text': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    if include_target:\n",
    "        feature_description['target'] = tf.io.FixedLenFeature([], tf.int64)\n",
    "    \n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    example['clean_text'] = tf.strings.reduce_join(example['clean_text'])\n",
    "    \n",
    "    if include_target:\n",
    "        return example['clean_text'], example['target']\n",
    "    return example['clean_text']\n",
    "\n",
    "def load_tfrecord_dataset(file_pattern, batch_size, include_target=True, repeat=True):\n",
    "    files = tf.data.Dataset.list_files(file_pattern)\n",
    "    dataset = files.interleave(tf.data.TFRecordDataset, cycle_length=4)\n",
    "    dataset = dataset.map(lambda x: parse_tfrecord_fn(x, include_target), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    if repeat:\n",
    "        dataset = dataset.repeat()\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "train_tfrecord_dataset = load_tfrecord_dataset('/kaggle/input/tfrecord-disaster/train_data.tfrecord', batch_size)\n",
    "val_tfrecord_dataset = load_tfrecord_dataset('/kaggle/input/tfrecord-disaster/val_data.tfrecord', batch_size)\n",
    "fine_tune_tfrecord_dataset = load_tfrecord_dataset('/kaggle/input/tfrecord-disaster/fine_tune_data.tfrecord', batch_size)\n",
    "kaggle_test_tfrecord_dataset = load_tfrecord_dataset('/kaggle/input/tfrecord-disaster/kaggle_test_data.tfrecord', batch_size, include_target=False, repeat=False)  # No repeat for test dataset\n",
    "\n",
    "# Tokenize datasets\n",
    "def tokenize_tfrecord_dataset(dataset, include_target=True):\n",
    "    def tokenize_fn(text, target=None):\n",
    "        encodings = tokenizer(\n",
    "            [str(t, 'utf-8') for t in text.numpy()],\n",
    "            max_length=64,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        if include_target:\n",
    "            return encodings['input_ids'], target\n",
    "        return encodings['input_ids']\n",
    "\n",
    "    def map_fn(text, target=None):\n",
    "        if include_target:\n",
    "            input_ids, target = tf.py_function(tokenize_fn, [text, target], [tf.int32, tf.int64])\n",
    "            input_ids.set_shape([None, 64])\n",
    "            target.set_shape([None])\n",
    "            return {'input_ids': input_ids}, target\n",
    "        input_ids = tf.py_function(tokenize_fn, [text], tf.int32)\n",
    "        input_ids.set_shape([None, 64])\n",
    "        return {'input_ids': input_ids}\n",
    "\n",
    "    if include_target:\n",
    "        return dataset.map(map_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset.map(map_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_tfrecord_dataset = tokenize_tfrecord_dataset(train_tfrecord_dataset)\n",
    "val_tfrecord_dataset = tokenize_tfrecord_dataset(val_tfrecord_dataset)\n",
    "fine_tune_tfrecord_dataset = tokenize_tfrecord_dataset(fine_tune_tfrecord_dataset)\n",
    "kaggle_test_tfrecord_dataset = tokenize_tfrecord_dataset(kaggle_test_tfrecord_dataset, include_target=False)\n",
    "\n",
    "# Custom wrapper to ensure the output layer is cast to float32\n",
    "class CustomBertForSequenceClassification(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super(CustomBertForSequenceClassification, self).__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        outputs = self.model(inputs)\n",
    "        logits = tf.cast(outputs.logits, tf.float32)\n",
    "        return logits\n",
    "\n",
    "# Directory to save models\n",
    "model_save_dir = './saved_models'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# File to store top 5 model predictions\n",
    "top_predictions_file = 'top_5_predictions.json'\n",
    "\n",
    "# Load existing top 5 predictions\n",
    "if os.path.exists(top_predictions_file):\n",
    "    with open(top_predictions_file, 'r') as file:\n",
    "        top_predictions = json.load(file)\n",
    "else:\n",
    "    top_predictions = []\n",
    "\n",
    "# Function to save top predictions\n",
    "def save_top_predictions(pre_fine_tuning_file, post_fine_tuning_file, val_accuracy, model_number):\n",
    "    global top_predictions\n",
    "\n",
    "    new_entry = {\n",
    "        'model_number': model_number,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'pre_fine_tuning_file': pre_fine_tuning_file,\n",
    "        'post_fine_tuning_file': post_fine_tuning_file\n",
    "    }\n",
    "\n",
    "    # Add the new entry and sort by validation accuracy\n",
    "    top_predictions.append(new_entry)\n",
    "    top_predictions = sorted(top_predictions, key=lambda x: x['val_accuracy'], reverse=True)\n",
    "\n",
    "    # If there are more than 5 entries, remove the one with the lowest accuracy\n",
    "    if len(top_predictions) > 5:\n",
    "        removed_entry = top_predictions.pop()\n",
    "        # Check if the files exist before attempting to remove them\n",
    "        if os.path.exists(removed_entry['pre_fine_tuning_file']):\n",
    "            os.remove(removed_entry['pre_fine_tuning_file'])\n",
    "            print(f\"File {removed_entry['pre_fine_tuning_file']} has been removed.\")\n",
    "        else:\n",
    "            print(f\"File {removed_entry['pre_fine_tuning_file']} does not exist and cannot be removed.\")\n",
    "        \n",
    "        if os.path.exists(removed_entry['post_fine_tuning_file']):\n",
    "            os.remove(removed_entry['post_fine_tuning_file'])\n",
    "            print(f\"File {removed_entry['post_fine_tuning_file']} has been removed.\")\n",
    "        else:\n",
    "            print(f\"File {removed_entry['post_fine_tuning_file']} does not exist and cannot be removed.\")\n",
    "\n",
    "    # Save the updated top predictions to file\n",
    "    with open(top_predictions_file, 'w') as file:\n",
    "        json.dump(top_predictions, file, indent=4)\n",
    "\n",
    "\n",
    "# Define steps per epoch and validation steps\n",
    "steps_per_epoch = len(train_data) // batch_size\n",
    "validation_steps = len(val_data) // batch_size\n",
    "fine_tune_steps_per_epoch = len(val_data) // batch_size  # Added step calculation for fine-tuning dataset\n",
    "\n",
    "def objective(trial):\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 1, 20)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.1)\n",
    "    lr_scheduler_type = trial.suggest_categorical(\"lr_scheduler_type\", [\"constant\", \"linear\", \"cosine\", \"cosine_with_restarts\"])\n",
    "    gradient_clip_norm = trial.suggest_float(\"gradient_clip_norm\", 0.0, 1.0)\n",
    "\n",
    "    with strategy.scope():\n",
    "        precision = tf.keras.metrics.Precision()\n",
    "        recall = tf.keras.metrics.Recall()\n",
    "\n",
    "        def f1_score_custom(y_true, y_pred):\n",
    "            y_pred = tf.argmax(y_pred, axis=1)\n",
    "            y_true = tf.cast(y_true, tf.int64)\n",
    "            precision.update_state(y_true, y_pred)\n",
    "            recall.update_state(y_true, y_pred)\n",
    "            precision_result = precision.result()\n",
    "            recall_result = recall.result()\n",
    "            f1 = 2 * ((precision_result * recall_result) / (precision_result + recall_result + tf.keras.backend.epsilon()))\n",
    "            return f1\n",
    "\n",
    "        config = BertConfig.from_pretrained('bert-base-uncased', num_labels=2, hidden_dropout_prob=dropout_rate)\n",
    "        model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
    "        model = CustomBertForSequenceClassification(model)  # Wrap with custom class\n",
    "\n",
    "        if lr_scheduler_type == \"linear\":\n",
    "            lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "                initial_learning_rate=learning_rate,\n",
    "                decay_steps=10000,\n",
    "                end_learning_rate=0.0,\n",
    "                power=1.0\n",
    "            )\n",
    "        elif lr_scheduler_type == \"cosine\":\n",
    "            lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "                initial_learning_rate=learning_rate,\n",
    "                decay_steps=10000\n",
    "            )\n",
    "        elif lr_scheduler_type == \"cosine_with_restarts\":\n",
    "            lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "                initial_learning_rate=learning_rate,\n",
    "                first_decay_steps=1000\n",
    "            )\n",
    "        else:\n",
    "            lr_schedule = learning_rate\n",
    "\n",
    "        from tensorflow.keras.mixed_precision import LossScaleOptimizer\n",
    "        optimizer = tf.keras.optimizers.experimental.AdamW(\n",
    "            learning_rate=lr_schedule,\n",
    "            weight_decay=weight_decay,\n",
    "            epsilon=1e-8,\n",
    "            clipnorm=gradient_clip_norm\n",
    "        )\n",
    "        optimizer = LossScaleOptimizer(optimizer)\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer, \n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "            metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy'), f1_score_custom],\n",
    "            steps_per_execution=1\n",
    "        )\n",
    "\n",
    "    log_dir = \"./logs/profile/\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, profile_batch='100,110')\n",
    "\n",
    "    model.fit(train_tfrecord_dataset, epochs=num_epochs, validation_data=val_tfrecord_dataset, steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, verbose=1, callbacks=[tensorboard_callback])\n",
    "\n",
    "    val_loss, val_accuracy, val_f1_score = model.evaluate(val_tfrecord_dataset, steps=validation_steps, verbose=1)\n",
    "    print(f\"f1 score: {val_f1_score} and accuracy: {val_accuracy}\")\n",
    "    \n",
    "    avg_score = (val_accuracy + val_f1_score) / 2\n",
    "\n",
    "    if len(top_predictions) < 5 or val_accuracy > min(top_predictions, key=lambda x: x['val_accuracy'])['val_accuracy']:\n",
    "        kaggle_test_predictions = model.predict(kaggle_test_tfrecord_dataset, steps=(len(kaggle_test_data) + batch_size - 1) // batch_size).logits\n",
    "        \n",
    "        # Ensure the prediction length matches the test data length\n",
    "        kaggle_test_predicted_labels = tf.argmax(kaggle_test_predictions, axis=1).numpy()[:len(kaggle_test_data)]\n",
    "        \n",
    "        pre_fine_tuning_predictions_file = os.path.join(model_save_dir, f\"{studyName}_model_trial_{trial.number}_accuracy_{val_accuracy:.4f}_avg_score_{avg_score:.4f}_f1_{val_f1_score:.4f}_pre_fine_tuning_submission.csv\") \n",
    "        submission = pd.DataFrame({'id': kaggle_test_data['id'], 'target': kaggle_test_predicted_labels})\n",
    "        submission.to_csv(pre_fine_tuning_predictions_file, index=False)\n",
    "\n",
    "        training_data_size = len(train_data)\n",
    "        fine_tune_data_size = len(val_data)\n",
    "        fine_tune_epochs = max(1, round((fine_tune_data_size / training_data_size) * num_epochs))\n",
    "\n",
    "        model.fit(fine_tune_tfrecord_dataset, epochs=fine_tune_epochs, steps_per_epoch=fine_tune_steps_per_epoch, verbose=1)  # Added steps_per_epoch\n",
    "\n",
    "        kaggle_test_predictions = model.predict(kaggle_test_tfrecord_dataset, steps=(len(kaggle_test_data) + batch_size - 1) // batch_size).logits\n",
    "        \n",
    "        # Ensure the prediction length matches the test data length\n",
    "        kaggle_test_predicted_labels = tf.argmax(kaggle_test_predictions, axis=1).numpy()[:len(kaggle_test_data)]\n",
    "        \n",
    "        post_fine_tuning_predictions_file = os.path.join(model_save_dir, f\"{studyName}_model_trial_{trial.number}_accuracy_{val_accuracy:.4f}_avg_score_{avg_score:.4f}_f1_{val_f1_score:.4f}_post_fine_tuning_submission.csv\") \n",
    "        submission = pd.DataFrame({'id': kaggle_test_data['id'], 'target': kaggle_test_predicted_labels})\n",
    "        submission.to_csv(post_fine_tuning_predictions_file, index=False)\n",
    "\n",
    "        save_top_predictions(pre_fine_tuning_predictions_file, post_fine_tuning_predictions_file, val_accuracy, trial.number)\n",
    "\n",
    "    return val_accuracy\n",
    "\n",
    "\n",
    "# Define your Optuna study, using the MySQL connection string\n",
    "optuna_storage = 'mysql+pymysql://<username>:<password>@<host>/<database>?ssl_ca=<path_to_CA_cert>&ssl_verify_cert=true'\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "db_password = user_secrets.get_secret(\"DB_PASSWORD\")\n",
    "\n",
    "optuna_storage = f'mysql+pymysql://MichaelAzure:{db_password}@kaggle-third-sql.mysql.database.azure.com/kaggle_disaster_database?ssl_ca=/kaggle/input/certification&ssl_verify_cert=true'\n",
    "\n",
    "studyName = 'disaster_tfrecord_BERT_mixed_0'\n",
    "study = optuna.create_study(study_name=studyName,\n",
    "                            storage=optuna_storage,\n",
    "                            direction='maximize',\n",
    "                            load_if_exists=True)\n",
    "\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value: {trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "output_top_predictions_file = '/kaggle/working/top_5_predictions.json'\n",
    "with open(output_top_predictions_file, 'w') as file:\n",
    "    json.dump(top_predictions, file, indent=4)\n",
    "\n",
    "print(f\"Top 5 predictions saved to {output_top_predictions_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
