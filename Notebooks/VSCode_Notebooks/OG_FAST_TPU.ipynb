{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import set_seed, BertTokenizer, TFBertForSequenceClassification, BertConfig\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import optuna\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "set_seed(42)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "# Initialize TPU\n",
    "try:\n",
    "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(resolver)\n",
    "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "    strategy = tf.distribute.TPUStrategy(resolver)\n",
    "    tpu_cores = strategy.num_replicas_in_sync\n",
    "    print(f\"TPU cores available: {tpu_cores}\")\n",
    "except ValueError:\n",
    "    print(\"TPU not found\")\n",
    "    raise SystemExit\n",
    "    \n",
    "# Set fixed batch size and learning rate parameters\n",
    "base_learning_rate = 1e-5\n",
    "batch_size_per_core = 32\n",
    "tpu_cores = 8\n",
    "batch_size = batch_size_per_core * tpu_cores\n",
    "learning_rate = base_learning_rate * (batch_size / (batch_size_per_core * tpu_cores))\n",
    "\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "kaggle_test_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "\n",
    "# Split the data into 75% training and 25% validation sets\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.25, random_state=42, stratify=train_data['target'])\n",
    "\n",
    "# Clean the text data\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)     # Remove mentions\n",
    "    text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s#]', '', text)  # Remove punctuation except hashtags\n",
    "    text = text.lower()                  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "train_data['clean_text'] = train_data['text'].apply(clean_text)\n",
    "val_data['clean_text'] = val_data['text'].apply(clean_text)\n",
    "kaggle_test_data['clean_text'] = kaggle_test_data['text'].apply(clean_text)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_texts(texts):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        max_length=64,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "# Define your TFRecord parsing and loading functions\n",
    "def parse_tfrecord_fn(example, include_target=True):\n",
    "    feature_description = {\n",
    "        'id': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'clean_text': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    if include_target:\n",
    "        feature_description['target'] = tf.io.FixedLenFeature([], tf.int64)\n",
    "    \n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    example['clean_text'] = tf.strings.reduce_join(example['clean_text'])\n",
    "    \n",
    "    if include_target:\n",
    "        return example['clean_text'], example['target']\n",
    "    return example['clean_text']\n",
    "\n",
    "def load_tfrecord_dataset(file_pattern, batch_size, include_target=True, repeat=True):\n",
    "    files = tf.data.Dataset.list_files(file_pattern)\n",
    "    dataset = files.interleave(tf.data.TFRecordDataset, cycle_length=4)\n",
    "    dataset = dataset.map(lambda x: parse_tfrecord_fn(x, include_target), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    if repeat:\n",
    "        dataset = dataset.repeat()\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "train_tfrecord_dataset = load_tfrecord_dataset('/kaggle/input/tfrecord-disaster/train_data.tfrecord', batch_size)\n",
    "val_tfrecord_dataset = load_tfrecord_dataset('/kaggle/input/tfrecord-disaster/val_data.tfrecord', batch_size)\n",
    "fine_tune_tfrecord_dataset = load_tfrecord_dataset('/kaggle/input/tfrecord-disaster/fine_tune_data.tfrecord', batch_size)\n",
    "kaggle_test_tfrecord_dataset = load_tfrecord_dataset('/kaggle/input/tfrecord-disaster/kaggle_test_data.tfrecord', batch_size, include_target=False, repeat=False)\n",
    "\n",
    "# Tokenize datasets\n",
    "def tokenize_tfrecord_dataset(dataset, include_target=True):\n",
    "    def tokenize_fn(text, target=None):\n",
    "        encodings = tokenizer(\n",
    "            [str(t, 'utf-8') for t in text.numpy()],\n",
    "            max_length=64,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        if include_target:\n",
    "            return encodings['input_ids'], target\n",
    "        return encodings['input_ids']\n",
    "\n",
    "    def map_fn(text, target=None):\n",
    "        if include_target:\n",
    "            input_ids, target = tf.py_function(tokenize_fn, [text, target], [tf.int32, tf.int64])\n",
    "            input_ids.set_shape([None, 64])\n",
    "            target.set_shape([None])\n",
    "            return {'input_ids': input_ids}, target\n",
    "        input_ids = tf.py_function(tokenize_fn, [text], tf.int32)\n",
    "        input_ids.set_shape([None, 64])\n",
    "        return {'input_ids': input_ids}\n",
    "\n",
    "    if include_target:\n",
    "        return dataset.map(map_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset.map(map_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_tfrecord_dataset = tokenize_tfrecord_dataset(train_tfrecord_dataset)\n",
    "val_tfrecord_dataset = tokenize_tfrecord_dataset(val_tfrecord_dataset)\n",
    "fine_tune_tfrecord_dataset = tokenize_tfrecord_dataset(fine_tune_tfrecord_dataset)\n",
    "kaggle_test_tfrecord_dataset = tokenize_tfrecord_dataset(kaggle_test_tfrecord_dataset, include_target=False)\n",
    "\n",
    "# Define and compile your model\n",
    "class CustomBertForSequenceClassification(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super(CustomBertForSequenceClassification, self).__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        outputs = self.model(inputs)\n",
    "        logits = tf.cast(outputs.logits, tf.float32)\n",
    "        return logits\n",
    "\n",
    "# Directory to save models\n",
    "model_save_dir = './saved_models'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# File to store top 5 model predictions\n",
    "top_predictions_file = 'top_5_predictions.json'\n",
    "\n",
    "# Load existing top 5 predictions\n",
    "if os.path.exists(top_predictions_file):\n",
    "    with open(top_predictions_file, 'r') as file:\n",
    "        top_predictions = json.load(file)\n",
    "else:\n",
    "    top_predictions = []\n",
    "\n",
    "# Function to save top predictions\n",
    "def save_top_predictions(pre_fine_tuning_file, post_fine_tuning_file, val_accuracy, model_number):\n",
    "    global top_predictions\n",
    "\n",
    "    new_entry = {\n",
    "        'model_number': model_number,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'pre_fine_tuning_file': pre_fine_tuning_file,\n",
    "        'post_fine_tuning_file': post_fine_tuning_file\n",
    "    }\n",
    "\n",
    "    # Add the new entry and sort by validation accuracy\n",
    "    top_predictions.append(new_entry)\n",
    "    top_predictions = sorted(top_predictions, key=lambda x: x['val_accuracy'], reverse=True)\n",
    "\n",
    "    # If there are more than 5 entries, remove the one with the lowest accuracy\n",
    "    if len(top_predictions) > 5:\n",
    "        removed_entry = top_predictions.pop()\n",
    "        # Check if the files exist before attempting to remove them\n",
    "        if os.path.exists(removed_entry['pre_fine_tuning_file']):\n",
    "            os.remove(removed_entry['pre_fine_tuning_file'])\n",
    "            print(f\"File {removed_entry['pre_fine_tuning_file']} has been removed.\")\n",
    "        else:\n",
    "            print(f\"File {removed_entry['pre_fine_tuning_file']} does not exist and cannot be removed.\")\n",
    "        \n",
    "        if os.path.exists(removed_entry['post_fine_tuning_file']):\n",
    "            os.remove(removed_entry['post_fine_tuning_file'])\n",
    "            print(f\"File {removed_entry['post_fine_tuning_file']} has been removed.\")\n",
    "        else:\n",
    "            print(f\"File {removed_entry['post_fine_tuning_file']} does not exist and cannot be removed.\")\n",
    "\n",
    "    # Save the updated top predictions to file\n",
    "    with open(top_predictions_file, 'w') as file:\n",
    "        json.dump(top_predictions, file, indent=4)\n",
    "    \n",
    "# Define steps per epoch and validation steps\n",
    "steps_per_epoch = len(train_data) // batch_size\n",
    "validation_steps = len(val_data) // batch_size\n",
    "fine_tune_steps_per_epoch = len(val_data) // batch_size  # Added step calculation for fine-tuning dataset\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 1, 20)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.1)\n",
    "    lr_scheduler_type = trial.suggest_categorical(\"lr_scheduler_type\", [\"constant\", \"linear\", \"cosine\", \"cosine_with_restarts\"])\n",
    "    gradient_clip_norm = trial.suggest_float(\"gradient_clip_norm\", 0.0, 1.0)\n",
    "\n",
    "    with strategy.scope():\n",
    "        precision = tf.keras.metrics.Precision()\n",
    "        recall = tf.keras.metrics.Recall()\n",
    "\n",
    "        def f1_score_custom(y_true, y_pred):\n",
    "            y_pred = tf.argmax(y_pred, axis=1)\n",
    "            y_true = tf.cast(y_true, tf.int64)\n",
    "            precision.update_state(y_true, y_pred)\n",
    "            recall.update_state(y_true, y_pred)\n",
    "            precision_result = precision.result()\n",
    "            recall_result = recall.result()\n",
    "            f1 = 2 * ((precision_result * recall_result) / (precision_result + recall_result + tf.keras.backend.epsilon()))\n",
    "            return f1\n",
    "\n",
    "        config = BertConfig.from_pretrained('bert-base-uncased', num_labels=2, hidden_dropout_prob=dropout_rate)\n",
    "        base_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
    "        model = CustomBertForSequenceClassification(base_model)\n",
    "\n",
    "        if lr_scheduler_type == \"linear\":\n",
    "            lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "                initial_learning_rate=learning_rate,\n",
    "                decay_steps=10000,\n",
    "                end_learning_rate=0.0,\n",
    "                power=1.0\n",
    "            )\n",
    "        elif lr_scheduler_type == \"cosine\":\n",
    "            lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "                initial_learning_rate=learning_rate,\n",
    "                decay_steps=10000\n",
    "            )\n",
    "        elif lr_scheduler_type == \"cosine_with_restarts\":\n",
    "            lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "                initial_learning_rate=learning_rate,\n",
    "                first_decay_steps=1000\n",
    "            )\n",
    "        else:\n",
    "            lr_schedule = learning_rate\n",
    "\n",
    "        optimizer = tf.keras.optimizers.AdamW(\n",
    "            learning_rate=lr_schedule,\n",
    "            weight_decay=weight_decay,\n",
    "            epsilon=1e-8,\n",
    "            clipnorm=gradient_clip_norm\n",
    "        )\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer, \n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "            metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy'), f1_score_custom],\n",
    "            steps_per_execution=1\n",
    "        )\n",
    "\n",
    "    checkpoint_filepath = './best_model.keras'\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=3,  # Number of epochs to wait for improvement before stopping\n",
    "        mode='max',\n",
    "        verbose=1,\n",
    "        restore_best_weights=True  # Restore model weights from the epoch with the best validation accuracy\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_tfrecord_dataset,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=val_tfrecord_dataset,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_steps=validation_steps,\n",
    "        callbacks=[checkpoint_callback, early_stopping_callback]\n",
    "    )\n",
    "\n",
    "    val_loss, val_accuracy, val_f1_score = model.evaluate(val_tfrecord_dataset, steps=validation_steps, verbose=1)\n",
    "    print(f\"f1 score: {val_f1_score} and accuracy: {val_accuracy}\")\n",
    "\n",
    "    avg_score = (val_accuracy + val_f1_score) / 2\n",
    "\n",
    "    if len(top_predictions) < 5 or val_accuracy > min(top_predictions, key=lambda x: x['val_accuracy'])['val_accuracy']:\n",
    "        kaggle_test_predictions = model.predict(kaggle_test_tfrecord_dataset, steps=(len(kaggle_test_data) + batch_size - 1) // batch_size).logits\n",
    "        \n",
    "        # Ensure the prediction length matches the test data length\n",
    "        kaggle_test_predicted_labels = tf.argmax(kaggle_test_predictions, axis=1).numpy()[:len(kaggle_test_data)]\n",
    "        \n",
    "        pre_fine_tuning_predictions_file = os.path.join(model_save_dir, f\"{studyName}_model_trial_{trial.number}_accuracy_{val_accuracy:.4f}_avg_score_{avg_score:.4f}_f1_{val_f1_score:.4f}_pre_fine_tuning_submission.csv\") \n",
    "        submission = pd.DataFrame({'id': kaggle_test_data['id'], 'target': kaggle_test_predicted_labels})\n",
    "        submission.to_csv(pre_fine_tuning_predictions_file, index=False)\n",
    "\n",
    "        training_data_size = len(train_data)\n",
    "        fine_tune_data_size = len(val_data)\n",
    "        fine_tune_epochs = max(1, round((fine_tune_data_size / training_data_size) * num_epochs))\n",
    "\n",
    "        model.fit(fine_tune_tfrecord_dataset, epochs=fine_tune_epochs, steps_per_epoch=fine_tune_steps_per_epoch, verbose=1)  # Added steps_per_epoch\n",
    "\n",
    "        kaggle_test_predictions = model.predict(kaggle_test_tfrecord_dataset, steps=(len(kaggle_test_data) + batch_size - 1) // batch_size).logits\n",
    "        \n",
    "        # Ensure the prediction length matches the test data length\n",
    "        kaggle_test_predicted_labels = tf.argmax(kaggle_test_predictions, axis=1).numpy()[:len(kaggle_test_data)]\n",
    "        \n",
    "        post_fine_tuning_predictions_file = os.path.join(model_save_dir, f\"{studyName}_model_trial_{trial.number}_accuracy_{val_accuracy:.4f}_avg_score_{avg_score:.4f}_f1_{val_f1_score:.4f}_post_fine_tuning_submission.csv\") \n",
    "        submission = pd.DataFrame({'id': kaggle_test_data['id'], 'target': kaggle_test_predicted_labels})\n",
    "        submission.to_csv(post_fine_tuning_predictions_file, index=False)\n",
    "\n",
    "        save_top_predictions(pre_fine_tuning_predictions_file, post_fine_tuning_predictions_file, val_accuracy, trial.number)\n",
    "\n",
    "    return val_accuracy\n",
    "\n",
    "# Define the Optuna study\n",
    "optuna_storage = 'mysql+pymysql://<username>:<password>@<host>/<database>?ssl_ca=<path_to_CA_cert>&ssl_verify_cert=true'\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "db_password = user_secrets.get_secret(\"DB_PASSWORD\")\n",
    "optuna_storage = f'mysql+pymysql://MichaelAzure:{db_password}@kaggle-third-sql.mysql.database.azure.com/kaggle_disaster_database?ssl_ca=/kaggle/input/certification&ssl_verify_cert=true'\n",
    "\n",
    "studyName = 'disaster_tfrecord_BERT_checkpoint_0'\n",
    "study = optuna.create_study(study_name=studyName, storage=optuna_storage, direction='maximize', load_if_exists=True)\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Access the best trial\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(f\"  Value: {best_trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Number of epochs actually run (can be less than the max due to early stopping)\n",
    "print(f\"Number of Epochs Run: {best_trial.number}\")\n",
    "\n",
    "# Load the best model\n",
    "best_model = tf.keras.models.load_model(checkpoint_filepath, custom_objects={'CustomBertForSequenceClassification': CustomBertForSequenceClassification})\n",
    "\n",
    "# Use the best model for predictions\n",
    "kaggle_test_predictions = best_model.predict(kaggle_test_tfrecord_dataset)\n",
    "kaggle_test_predicted_labels = tf.argmax(kaggle_test_predictions, axis=1).numpy()[:len(kaggle_test_data)]\n",
    "submission = pd.DataFrame({'id': kaggle_test_data['id'], 'target': kaggle_test_predicted_labels})\n",
    "submission.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import set_seed, BertTokenizer, TFBertForSequenceClassification, BertConfig\n",
    "import optuna\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "set_seed(42)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "# Initialize TPU\n",
    "try:\n",
    "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(resolver)\n",
    "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "    strategy = tf.distribute.TPUStrategy(resolver)\n",
    "    tpu_cores = strategy.num_replicas_in_sync\n",
    "    print(f\"TPU cores available: {tpu_cores}\")\n",
    "except ValueError:\n",
    "    print(\"TPU not found\")\n",
    "    raise SystemExit\n",
    "\n",
    "import tensorflow.keras.callbacks\n",
    "# Set fixed batch size and learning rate parameters\n",
    "base_learning_rate = 1e-5\n",
    "batch_size_per_core = 32\n",
    "tpu_cores = 8\n",
    "batch_size = batch_size_per_core * tpu_cores\n",
    "learning_rate = base_learning_rate * (batch_size / (batch_size_per_core * tpu_cores))\n",
    "\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "kaggle_test_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "\n",
    "# Split the data into 75% training and 25% validation sets\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.25, random_state=42, stratify=train_data['target'])\n",
    "\n",
    "# Clean the text data\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)     # Remove mentions\n",
    "    text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s#]', '', text)  # Remove punctuation except hashtags\n",
    "    text = text.lower()                  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "\n",
    "train_data['clean_text'] = train_data['text'].apply(clean_text)\n",
    "val_data['clean_text'] = val_data['text'].apply(clean_text)\n",
    "kaggle_test_data['clean_text'] = kaggle_test_data['text'].apply(clean_text)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_texts(texts):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        max_length=64,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "# Define your TFRecord parsing and loading functions\n",
    "def parse_tfrecord_fn(example, include_target=True):\n",
    "    feature_description = {\n",
    "        'id': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'clean_text': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    if include_target:\n",
    "        feature_description['target'] = tf.io.FixedLenFeature([], tf.int64)\n",
    "    \n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    example['clean_text'] = tf.strings.reduce_join(example['clean_text'])\n",
    "    \n",
    "    if include_target:\n",
    "        return example['clean_text'], example['target']\n",
    "    return example['clean_text']\n",
    "\n",
    "def load_tfrecord_dataset(file_pattern, batch_size, include_target=True, repeat=True):\n",
    "    files = tf.data.Dataset.list_files(file_pattern)\n",
    "    dataset = files.interleave(tf.data.TFRecordDataset, cycle_length=4)\n",
    "    dataset = dataset.map(lambda x: parse_tfrecord_fn(x, include_target), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    if repeat:\n",
    "        dataset = dataset.repeat()\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "train_tfrecord_dataset = load_tfrecord_dataset('/kaggle/input/tfrecord-disaster/train_data.tfrecord', batch_size)\n",
    "val_tfrecord_dataset = load_tfrecord_dataset('/kaggle/input/tfrecord-disaster/val_data.tfrecord', batch_size)\n",
    "fine_tune_tfrecord_dataset = load_tfrecord_dataset('/kaggle/input/tfrecord-disaster/fine_tune_data.tfrecord', batch_size)\n",
    "kaggle_test_tfrecord_dataset = load_tfrecord_dataset('/kaggle/input/tfrecord-disaster/kaggle_test_data.tfrecord', batch_size, include_target=False, repeat=False)\n",
    "\n",
    "# Tokenize datasets\n",
    "def tokenize_tfrecord_dataset(dataset, include_target=True):\n",
    "    def tokenize_fn(text, target=None):\n",
    "        encodings = tokenizer(\n",
    "            [str(t, 'utf-8') for t in text.numpy()],\n",
    "            max_length=64,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        if include_target:\n",
    "            return encodings['input_ids'], target\n",
    "        return encodings['input_ids']\n",
    "\n",
    "    def map_fn(text, target=None):\n",
    "        if include_target:\n",
    "            input_ids, target = tf.py_function(tokenize_fn, [text, target], [tf.int32, tf.int64])\n",
    "            input_ids.set_shape([None, 64])\n",
    "            target.set_shape([None])\n",
    "            return {'input_ids': input_ids}, target\n",
    "        input_ids = tf.py_function(tokenize_fn, [text], tf.int32)\n",
    "        input_ids.set_shape([None, 64])\n",
    "        return {'input_ids': input_ids}\n",
    "\n",
    "    if include_target:\n",
    "        return dataset.map(map_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset.map(map_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_tfrecord_dataset = tokenize_tfrecord_dataset(train_tfrecord_dataset)\n",
    "val_tfrecord_dataset = tokenize_tfrecord_dataset(val_tfrecord_dataset)\n",
    "fine_tune_tfrecord_dataset = tokenize_tfrecord_dataset(fine_tune_tfrecord_dataset)\n",
    "kaggle_test_tfrecord_dataset = tokenize_tfrecord_dataset(kaggle_test_tfrecord_dataset, include_target=False)\n",
    "\n",
    "# Define and compile your model\n",
    "class CustomBertForSequenceClassification(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super(CustomBertForSequenceClassification, self).__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        outputs = self.model(inputs)\n",
    "        logits = tf.cast(outputs.logits, tf.float32)\n",
    "        return logits\n",
    "\n",
    "# Directory to save models\n",
    "model_save_dir = './saved_models'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# File to store top 5 model predictions\n",
    "top_predictions_file = 'top_5_predictions.json'\n",
    "\n",
    "# Load existing top 5 predictions\n",
    "if os.path.exists(top_predictions_file):\n",
    "    with open(top_predictions_file, 'r') as file:\n",
    "        top_predictions = json.load(file)\n",
    "else:\n",
    "    top_predictions = []\n",
    "\n",
    "# Function to save top predictions\n",
    "def save_top_predictions(pre_fine_tuning_file, post_fine_tuning_file, val_accuracy, model_number):\n",
    "    global top_predictions\n",
    "\n",
    "    new_entry = {\n",
    "        'model_number': model_number,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'pre_fine_tuning_file': pre_fine_tuning_file,\n",
    "        'post_fine_tuning_file': post_fine_tuning_file\n",
    "    }\n",
    "\n",
    "    # Add the new entry and sort by validation accuracy\n",
    "    top_predictions.append(new_entry)\n",
    "    top_predictions = sorted(top_predictions, key=lambda x: x['val_accuracy'], reverse=True)\n",
    "\n",
    "    # If there are more than 5 entries, remove the one with the lowest accuracy\n",
    "    if len(top_predictions) > 5:\n",
    "        removed_entry = top_predictions.pop()\n",
    "        # Check if the files exist before attempting to remove them\n",
    "        if os.path.exists(removed_entry['pre_fine_tuning_file']):\n",
    "            os.remove(removed_entry['pre_fine_tuning_file'])\n",
    "            print(f\"File {removed_entry['pre_fine_tuning_file']} has been removed.\")\n",
    "        else:\n",
    "            print(f\"File {removed_entry['pre_fine_tuning_file']} does not exist and cannot be removed.\")\n",
    "        \n",
    "        if os.path.exists(removed_entry['post_fine_tuning_file']):\n",
    "            os.remove(removed_entry['post_fine_tuning_file'])\n",
    "            print(f\"File {removed_entry['post_fine_tuning_file']} has been removed.\")\n",
    "        else:\n",
    "            print(f\"File {removed_entry['post_fine_tuning_file']} does not exist and cannot be removed.\")\n",
    "\n",
    "    # Save the updated top predictions to file\n",
    "    with open(top_predictions_file, 'w') as file:\n",
    "        json.dump(top_predictions, file, indent=4)\n",
    "    \n",
    "# Define steps per epoch and validation steps\n",
    "steps_per_epoch = len(train_data) // batch_size\n",
    "validation_steps = len(val_data) // batch_size\n",
    "fine_tune_steps_per_epoch = len(val_data) // batch_size  # Added step calculation for fine-tuning dataset\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 1, 20)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.1)\n",
    "    lr_scheduler_type = trial.suggest_categorical(\"lr_scheduler_type\", [\"constant\", \"linear\", \"cosine\", \"cosine_with_restarts\"])\n",
    "    gradient_clip_norm = trial.suggest_float(\"gradient_clip_norm\", 0.0, 1.0)\n",
    "\n",
    "    with strategy.scope():\n",
    "        precision = tf.keras.metrics.Precision()\n",
    "        recall = tf.keras.metrics.Recall()\n",
    "\n",
    "        def f1_score_custom(y_true, y_pred):\n",
    "            y_pred = tf.argmax(y_pred, axis=1)\n",
    "            y_true = tf.cast(y_true, tf.int64)\n",
    "            precision.update_state(y_true, y_pred)\n",
    "            recall.update_state(y_true, y_pred)\n",
    "            precision_result = precision.result()\n",
    "            recall_result = recall.result()\n",
    "            f1 = 2 * ((precision_result * recall_result) / (precision_result + recall_result + tf.keras.backend.epsilon()))\n",
    "            return f1\n",
    "\n",
    "        config = BertConfig.from_pretrained('bert-base-uncased', num_labels=2, hidden_dropout_prob=dropout_rate)\n",
    "        base_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
    "        model = CustomBertForSequenceClassification(base_model)\n",
    "\n",
    "        if lr_scheduler_type == \"linear\":\n",
    "            lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "                initial_learning_rate=learning_rate,\n",
    "                decay_steps=10000,\n",
    "                end_learning_rate=0.0,\n",
    "                power=1.0\n",
    "            )\n",
    "        elif lr_scheduler_type == \"cosine\":\n",
    "            lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "                initial_learning_rate=learning_rate,\n",
    "                decay_steps=10000\n",
    "            )\n",
    "        elif lr_scheduler_type == \"cosine_with_restarts\":\n",
    "            lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "                initial_learning_rate=learning_rate,\n",
    "                first_decay_steps=1000\n",
    "            )\n",
    "        else:\n",
    "            lr_schedule = learning_rate\n",
    "\n",
    "        optimizer = tf.keras.optimizers.AdamW(\n",
    "            learning_rate=lr_schedule,\n",
    "            weight_decay=weight_decay,\n",
    "            epsilon=1e-8,\n",
    "            clipnorm=gradient_clip_norm\n",
    "        )\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer, \n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "            metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy'), f1_score_custom],\n",
    "            steps_per_execution=1\n",
    "        )\n",
    "\n",
    "#     checkpoint_filepath = './best_model.keras'\n",
    "#     checkpoint_callback = ModelCheckpoint(\n",
    "#         filepath=checkpoint_filepath,\n",
    "#         monitor='val_accuracy',\n",
    "#         save_best_only=True,\n",
    "#         save_weights_only=False,\n",
    "#         mode='max',\n",
    "#         verbose=1\n",
    "#     )\n",
    "\n",
    "#     early_stopping_callback = EarlyStopping(\n",
    "#         monitor='val_accuracy',\n",
    "#         patience=3,  # Number of epochs to wait for improvement before stopping\n",
    "#         mode='max',\n",
    "#         verbose=1,\n",
    "#         restore_best_weights=True  # Restore model weights from the epoch with the best validation accuracy\n",
    "#     )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_tfrecord_dataset,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=val_tfrecord_dataset,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_steps=validation_steps,\n",
    "        #callbacks=[checkpoint_callback, early_stopping_callback]\n",
    "    )\n",
    "\n",
    "    val_loss, val_accuracy, val_f1_score = model.evaluate(val_tfrecord_dataset, steps=validation_steps, verbose=1)\n",
    "    print(f\"f1 score: {val_f1_score} and accuracy: {val_accuracy}\")\n",
    "\n",
    "    avg_score = (val_accuracy + val_f1_score) / 2\n",
    "\n",
    "    if len(top_predictions) < 5 or val_accuracy > min(top_predictions, key=lambda x: x['val_accuracy'])['val_accuracy']:\n",
    "        kaggle_test_predictions = model.predict(kaggle_test_tfrecord_dataset, steps=(len(kaggle_test_data) + batch_size - 1) // batch_size).logits\n",
    "        \n",
    "        # Ensure the prediction length matches the test data length\n",
    "        kaggle_test_predicted_labels = tf.argmax(kaggle_test_predictions, axis=1).numpy()[:len(kaggle_test_data)]\n",
    "        \n",
    "        pre_fine_tuning_predictions_file = os.path.join(model_save_dir, f\"{studyName}_model_trial_{trial.number}_accuracy_{val_accuracy:.4f}_avg_score_{avg_score:.4f}_f1_{val_f1_score:.4f}_pre_fine_tuning_submission.csv\") \n",
    "        submission = pd.DataFrame({'id': kaggle_test_data['id'], 'target': kaggle_test_predicted_labels})\n",
    "        submission.to_csv(pre_fine_tuning_predictions_file, index=False)\n",
    "\n",
    "        training_data_size = len(train_data)\n",
    "        fine_tune_data_size = len(val_data)\n",
    "        fine_tune_epochs = max(1, round((fine_tune_data_size / training_data_size) * num_epochs))\n",
    "\n",
    "        model.fit(fine_tune_tfrecord_dataset, epochs=fine_tune_epochs, steps_per_epoch=fine_tune_steps_per_epoch, verbose=1)  # Added steps_per_epoch\n",
    "\n",
    "        kaggle_test_predictions = model.predict(kaggle_test_tfrecord_dataset, steps=(len(kaggle_test_data) + batch_size - 1) // batch_size).logits\n",
    "        \n",
    "        # Ensure the prediction length matches the test data length\n",
    "        kaggle_test_predicted_labels = tf.argmax(kaggle_test_predictions, axis=1).numpy()[:len(kaggle_test_data)]\n",
    "        \n",
    "        post_fine_tuning_predictions_file = os.path.join(model_save_dir, f\"{studyName}_model_trial_{trial.number}_accuracy_{val_accuracy:.4f}_avg_score_{avg_score:.4f}_f1_{val_f1_score:.4f}_post_fine_tuning_submission.csv\") \n",
    "        submission = pd.DataFrame({'id': kaggle_test_data['id'], 'target': kaggle_test_predicted_labels})\n",
    "        submission.to_csv(post_fine_tuning_predictions_file, index=False)\n",
    "\n",
    "        save_top_predictions(pre_fine_tuning_predictions_file, post_fine_tuning_predictions_file, val_accuracy, trial.number)\n",
    "\n",
    "    return val_accuracy\n",
    "\n",
    "# Define the Optuna study\n",
    "optuna_storage = 'mysql+pymysql://<username>:<password>@<host>/<database>?ssl_ca=<path_to_CA_cert>&ssl_verify_cert=true'\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "db_password = user_secrets.get_secret(\"DB_PASSWORD\")\n",
    "optuna_storage = f'mysql+pymysql://MichaelAzure:{db_password}@kaggle-third-sql.mysql.database.azure.com/kaggle_disaster_database?ssl_ca=/kaggle/input/certification&ssl_verify_cert=true'\n",
    "\n",
    "studyName = 'disaster_tfrecord_BERT_checkpoint_4'\n",
    "study = optuna.create_study(study_name=studyName, storage=optuna_storage, direction='maximize', load_if_exists=True)\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Access the best trial\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(f\"  Value: {best_trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Number of epochs actually run (can be less than the max due to early stopping)\n",
    "print(f\"Number of Epochs Run: {best_trial.number}\")\n",
    "\n",
    "# # Load the best model\n",
    "# best_model = tf.keras.models.load_model(checkpoint_filepath, custom_objects={'CustomBertForSequenceClassification': CustomBertForSequenceClassification})\n",
    "\n",
    "# # Use the best model for predictions\n",
    "# kaggle_test_predictions = best_model.predict(kaggle_test_tfrecord_dataset)\n",
    "# kaggle_test_predicted_labels = tf.argmax(kaggle_test_predictions, axis=1).numpy()[:len(kaggle_test_data)]\n",
    "# submission = pd.DataFrame({'id': kaggle_test_data['id'], 'target': kaggle_test_predicted_labels})\n",
    "# submission.to_csv('submission.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
