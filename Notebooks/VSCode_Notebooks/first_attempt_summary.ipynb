{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install keras\n",
    "\n",
    "\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import set_seed\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "set_seed(42)\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import keras\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "\n",
    "\n",
    "###########################################\n",
    "\n",
    "\n",
    "import re # Regular Expression\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)     # Remove mentions\n",
    "    text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s#]', '', text)  # Remove punctuation except hashtags\n",
    "    text = text.lower()                  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "train_data['clean_text'] = train_data['text'].apply(clean_text) # Apply the data cleaning process to training data\n",
    "test_data['clean_text'] = test_data['text'].apply(clean_text)# Apply the data cleaning process to testing data\n",
    "\n",
    "\n",
    "\n",
    "###########################################\n",
    "\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_texts(texts):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        max_length=64,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_texts(train_data['clean_text'])\n",
    "test_encodings = tokenize_texts(test_data['clean_text'])\n",
    "\n",
    "\n",
    "\n",
    "###########################################\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "train_labels = tf.convert_to_tensor(train_data['target'].values)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    "))\n",
    "\n",
    "# Create a validation split\n",
    "val_size = int(0.2 * len(train_data))\n",
    "val_dataset = train_dataset.take(val_size)\n",
    "train_dataset = train_dataset.skip(val_size)\n",
    "\n",
    "# Batch and shuffle the datasets\n",
    "batch_size = 128\n",
    "\n",
    "train_dataset = train_dataset.shuffle(10000).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "\n",
    "###########################################\n",
    "\n",
    "from transformers import TFBertForSequenceClassification, BertConfig\n",
    "\n",
    "config = BertConfig.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-8),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=1,\n",
    "    validation_data=val_dataset\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "###########################################\n",
    "\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings)\n",
    ")).batch(32)\n",
    "\n",
    "predictions = model.predict(test_dataset).logits\n",
    "predicted_labels = tf.argmax(predictions, axis=1).numpy()\n",
    "\n",
    "# Create a submission DataFrame\n",
    "submission = pd.DataFrame({'id': test_data['id'], 'target': predicted_labels})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "\n",
    "# predictions = model.predict(test_dataset).logits\n",
    "# predictions = tf.nn.softmax(predictions, axis=1)\n",
    "# predicted_labels = tf.argmax(predictions, axis=1).numpy()\n",
    "\n",
    "\n",
    "# ###########################################\n",
    "\n",
    "\n",
    "# submission = pd.DataFrame({'id': test_data['id'], 'target': predicted_labels})\n",
    "# submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
