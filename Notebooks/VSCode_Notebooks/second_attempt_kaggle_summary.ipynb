{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import set_seed\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "set_seed(42)\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "#########################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "test_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "\n",
    "# Display the first few rows of the training data\n",
    "print(train_data.head())\n",
    "\n",
    "#########################################################################################\n",
    "import re # Regular Expression\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)     # Remove mentions\n",
    "    text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s#]', '', text)  # Remove punctuation except hashtags\n",
    "    text = text.lower()                  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "train_data['clean_text'] = train_data['text'].apply(clean_text) # Apply the data cleaning process to training data\n",
    "test_data['clean_text'] = test_data['text'].apply(clean_text)# Apply the data cleaning process to testing data\n",
    "\n",
    "# Display the first few rows of the cleaned data\n",
    "print(train_data[['text', 'clean_text']].head())\n",
    "\n",
    "\n",
    "#########################################################################################\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_texts(texts):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        max_length=64,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_texts(train_data['clean_text'])\n",
    "test_encodings = tokenize_texts(test_data['clean_text'])\n",
    "\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Tokenize the clean text without padding to get the length of each tweet\n",
    "# train_data['token_length'] = train_data['clean_text'].apply(lambda x: len(tokenizer.encode(x, add_special_tokens=True)))\n",
    "# test_data['token_length'] = test_data['clean_text'].apply(lambda x: len(tokenizer.encode(x, add_special_tokens=True)))\n",
    "\n",
    "# # Plot the distribution of token lengths\n",
    "# plt.hist(train_data['token_length'], bins=50, alpha=0.7, label='Train')\n",
    "# plt.hist(test_data['token_length'], bins=50, alpha=0.7, label='Test')\n",
    "# plt.axvline(x=128, color='r', linestyle='--', label='MAX_LEN = 128')\n",
    "# plt.xlabel('Token Length')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Display some statistics\n",
    "# print(\"Train token length statistics:\")\n",
    "# print(train_data['token_length'].describe())\n",
    "\n",
    "# print(\"\\nTest token length statistics:\")\n",
    "# print(test_data['token_length'].describe())\n",
    "\n",
    "\n",
    "#########################################################################################\n",
    "import tensorflow as tf\n",
    "\n",
    "train_labels = tf.convert_to_tensor(train_data['target'].values)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    "))\n",
    "\n",
    "# Create a validation split\n",
    "val_size = int(0.2 * len(train_data))\n",
    "val_dataset = train_dataset.take(val_size)\n",
    "train_dataset = train_dataset.skip(val_size)\n",
    "\n",
    "# Batch and shuffle the datasets\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = train_dataset.shuffle(10000).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "from transformers import TFBertForSequenceClassification, BertConfig\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "  # Model configuration and creation\n",
    "  config = BertConfig.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "  model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
    "\n",
    "  # Model compilation\n",
    "  model.compile(\n",
    "      optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-8),\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]\n",
    "  )\n",
    "\n",
    "  # Model training\n",
    "  history = model.fit(\n",
    "      train_dataset,\n",
    "      epochs=3,\n",
    "      validation_data=val_dataset\n",
    "  )\n",
    "\n",
    "\n",
    "#########################################################################################\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings)\n",
    ")).batch(32)\n",
    "\n",
    "predictions = model.predict(test_dataset).logits\n",
    "predicted_labels = tf.argmax(predictions, axis=1).numpy()\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "# Create a submission DataFrame\n",
    "submission = pd.DataFrame({'id': test_data['id'], 'target': predicted_labels})\n",
    "submission.to_csv('submission_2_kaggle.csv', index=False)\n",
    "\n",
    "#########################################################################################\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
