{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\micha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.3.3)\n",
      "Requirement already satisfied: absl-py in c:\\users\\micha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras) (2.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\micha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras) (1.24.3)\n",
      "Requirement already satisfied: rich in c:\\users\\micha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras) (13.7.0)\n",
      "Requirement already satisfied: namex in c:\\users\\micha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\users\\micha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras) (3.10.0)\n",
      "Requirement already satisfied: optree in c:\\users\\micha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras) (0.11.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\micha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras) (0.3.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\micha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from optree->keras) (4.9.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\micha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\micha\\appdata\\roaming\\python\\python310\\site-packages (from rich->keras) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\micha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Micha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Micha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Micha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Micha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x000001E6A3955AB0> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function infer_framework at 0x000001E6A3955AB0> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:From c:\\Users\\Micha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:126: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Micha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:126: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "48/48 [==============================] - 383s 7s/step - loss: 0.5048 - accuracy: 0.7674 - val_loss: 0.4004 - val_accuracy: 0.8357\n",
      "Epoch 2/3\n"
     ]
    }
   ],
   "source": [
    "%pip install keras\n",
    "import pandas as pd\n",
    "import keras\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "\n",
    "\n",
    "###########################################\n",
    "\n",
    "\n",
    "import re # Regular Expression\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)     # Remove mentions\n",
    "    text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s#]', '', text)  # Remove punctuation except hashtags\n",
    "    text = text.lower()                  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "train_data['clean_text'] = train_data['text'].apply(clean_text) # Apply the data cleaning process to training data\n",
    "test_data['clean_text'] = test_data['text'].apply(clean_text)# Apply the data cleaning process to testing data\n",
    "\n",
    "\n",
    "\n",
    "###########################################\n",
    "\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_texts(texts):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        max_length=64,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_texts(train_data['clean_text'])\n",
    "test_encodings = tokenize_texts(test_data['clean_text'])\n",
    "\n",
    "\n",
    "\n",
    "###########################################\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "train_labels = tf.convert_to_tensor(train_data['target'].values)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    "))\n",
    "\n",
    "# Create a validation split\n",
    "val_size = int(0.2 * len(train_data))\n",
    "val_dataset = train_dataset.take(val_size)\n",
    "train_dataset = train_dataset.skip(val_size)\n",
    "\n",
    "# Batch and shuffle the datasets\n",
    "batch_size = 128\n",
    "\n",
    "train_dataset = train_dataset.shuffle(10000).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "\n",
    "###########################################\n",
    "\n",
    "from transformers import TFBertForSequenceClassification, BertConfig\n",
    "\n",
    "config = BertConfig.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-8),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=3,\n",
    "    validation_data=val_dataset\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "###########################################\n",
    "\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_data['padded_tokens'].tolist()).batch(128)\n",
    "\n",
    "predictions = model.predict(test_dataset).logits\n",
    "predicted_labels = tf.argmax(predictions, axis=1).numpy()\n",
    "\n",
    "# Create a submission DataFrame\n",
    "submission = pd.DataFrame({'id': test_data['id'], 'target': predicted_labels})\n",
    "submission.to_csv('data/submission.csv', index=False)\n",
    "\n",
    "\n",
    "# predictions = model.predict(test_dataset).logits\n",
    "# predictions = tf.nn.softmax(predictions, axis=1)\n",
    "# predicted_labels = tf.argmax(predictions, axis=1).numpy()\n",
    "\n",
    "\n",
    "# ###########################################\n",
    "\n",
    "\n",
    "# submission = pd.DataFrame({'id': test_data['id'], 'target': predicted_labels})\n",
    "# submission.to_csv('../data/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
