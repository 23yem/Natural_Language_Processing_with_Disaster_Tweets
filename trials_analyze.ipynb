{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this SQL code below to get a SQL table that you will export as a CSV and name trial.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT \n",
    "    studies.study_id,\n",
    "    studies.study_name,\n",
    "    trials.trial_id,\n",
    "    trials.number AS trial_number,\n",
    "    trial_params.param_name,\n",
    "    trial_params.param_value,\n",
    "    trial_values.value\n",
    "FROM \n",
    "    trials\n",
    "INNER JOIN \n",
    "    studies ON trials.study_id = studies.study_id\n",
    "INNER JOIN \n",
    "    trial_params ON trials.trial_id = trial_params.trial_id\n",
    "INNER JOIN \n",
    "    trial_values ON trials.trial_id = trial_values.trial_id\n",
    "WHERE \n",
    "    trial_values.value IS NOT NULL\n",
    "ORDER BY \n",
    "    studies.study_id, trials.number;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#pd.set_option('display.max_columns', None)  # Show all columns\n",
    "#pd.set_option('display.width', 1000)        # Set console width\n",
    "#pd.set_option('display.max_colwidth', 40)   # Limit the maximum width of each column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read from trials csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial_id</th>\n",
       "      <th>study_id</th>\n",
       "      <th>param_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>345.000000</td>\n",
       "      <td>345.000000</td>\n",
       "      <td>345.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>29.747826</td>\n",
       "      <td>7.695652</td>\n",
       "      <td>1.471554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>16.624186</td>\n",
       "      <td>2.866732</td>\n",
       "      <td>2.405142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.319917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>44.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>58.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         trial_id    study_id  param_value\n",
       "count  345.000000  345.000000   345.000000\n",
       "mean    29.747826    7.695652     1.471554\n",
       "std     16.624186    2.866732     2.405142\n",
       "min      1.000000    1.000000     0.000000\n",
       "25%     15.000000    8.000000     0.000027\n",
       "50%     30.000000    8.000000     0.319917\n",
       "75%     44.000000    8.000000     2.000000\n",
       "max     58.000000   16.000000    10.000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials = pd.read_csv(\"trials.csv\")\n",
    "trials.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide Each Unique Study into Its Own DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the DataFrame by 'study_id' and create a dictionary of DataFrames\n",
    "grouped = trials.groupby('study_name')\n",
    "dfs = {study_name: group for study_name, group in grouped}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Sorted DataFrame for Each Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'disaster_':     trial_id  study_id study_name         param_name  param_value  \\\n",
      "0         38        14  disaster_         batch_size     2.000000   \n",
      "1         38        14  disaster_       dropout_rate     0.459665   \n",
      "2         38        14  disaster_      learning_rate     0.000003   \n",
      "3         38        14  disaster_  lr_scheduler_type     3.000000   \n",
      "4         38        14  disaster_         num_epochs     3.000000   \n",
      "5         38        14  disaster_       weight_decay     0.086721   \n",
      "6         41        14  disaster_         batch_size     0.000000   \n",
      "7         41        14  disaster_       dropout_rate     0.414003   \n",
      "8         41        14  disaster_      learning_rate     0.000046   \n",
      "9         41        14  disaster_  lr_scheduler_type     2.000000   \n",
      "10        41        14  disaster_         num_epochs    10.000000   \n",
      "11        41        14  disaster_       weight_decay     0.010063   \n",
      "\n",
      "                    score  \n",
      "0   In Progress Or Failed  \n",
      "1   In Progress Or Failed  \n",
      "2   In Progress Or Failed  \n",
      "3   In Progress Or Failed  \n",
      "4   In Progress Or Failed  \n",
      "5   In Progress Or Failed  \n",
      "6   In Progress Or Failed  \n",
      "7   In Progress Or Failed  \n",
      "8   In Progress Or Failed  \n",
      "9   In Progress Or Failed  \n",
      "10  In Progress Or Failed  \n",
      "11  In Progress Or Failed  , 'disaster_2':     trial_id  study_id  study_name         param_name  param_value  \\\n",
      "24         2         2  disaster_2         batch_size     0.000000   \n",
      "55         7         2  disaster_2       dropout_rate     0.230677   \n",
      "47         5         2  disaster_2       weight_decay     0.077251   \n",
      "48         6         2  disaster_2         batch_size     1.000000   \n",
      "49         6         2  disaster_2       dropout_rate     0.164002   \n",
      "50         6         2  disaster_2      learning_rate     0.000023   \n",
      "51         6         2  disaster_2  lr_scheduler_type     3.000000   \n",
      "52         6         2  disaster_2         num_epochs     4.000000   \n",
      "53         6         2  disaster_2       weight_decay     0.000459   \n",
      "54         7         2  disaster_2         batch_size     0.000000   \n",
      "56         7         2  disaster_2      learning_rate     0.000004   \n",
      "25         2         2  disaster_2       dropout_rate     0.119921   \n",
      "57         7         2  disaster_2  lr_scheduler_type     3.000000   \n",
      "58         7         2  disaster_2         num_epochs     9.000000   \n",
      "59         7         2  disaster_2       weight_decay     0.018517   \n",
      "60         8         2  disaster_2         batch_size     2.000000   \n",
      "61         8         2  disaster_2       dropout_rate     0.193783   \n",
      "62         8         2  disaster_2      learning_rate     0.000042   \n",
      "63         8         2  disaster_2  lr_scheduler_type     1.000000   \n",
      "64         8         2  disaster_2         num_epochs     4.000000   \n",
      "46         5         2  disaster_2         num_epochs    10.000000   \n",
      "45         5         2  disaster_2  lr_scheduler_type     0.000000   \n",
      "44         5         2  disaster_2      learning_rate     0.000002   \n",
      "36         4         2  disaster_2         batch_size     0.000000   \n",
      "26         2         2  disaster_2      learning_rate     0.000083   \n",
      "27         2         2  disaster_2  lr_scheduler_type     3.000000   \n",
      "28         2         2  disaster_2         num_epochs    10.000000   \n",
      "29         2         2  disaster_2       weight_decay     0.042238   \n",
      "43         5         2  disaster_2       dropout_rate     0.123889   \n",
      "65         8         2  disaster_2       weight_decay     0.021603   \n",
      "37         4         2  disaster_2       dropout_rate     0.396617   \n",
      "38         4         2  disaster_2      learning_rate     0.000002   \n",
      "39         4         2  disaster_2  lr_scheduler_type     3.000000   \n",
      "40         4         2  disaster_2         num_epochs     8.000000   \n",
      "41         4         2  disaster_2       weight_decay     0.040438   \n",
      "42         5         2  disaster_2         batch_size     0.000000   \n",
      "35         3         2  disaster_2       weight_decay     0.086965   \n",
      "33         3         2  disaster_2  lr_scheduler_type     2.000000   \n",
      "32         3         2  disaster_2      learning_rate     0.000011   \n",
      "31         3         2  disaster_2       dropout_rate     0.427225   \n",
      "30         3         2  disaster_2         batch_size     0.000000   \n",
      "34         3         2  disaster_2         num_epochs     3.000000   \n",
      "\n",
      "                    score  \n",
      "24  In Progress Or Failed  \n",
      "55  In Progress Or Failed  \n",
      "47  In Progress Or Failed  \n",
      "48  In Progress Or Failed  \n",
      "49  In Progress Or Failed  \n",
      "50  In Progress Or Failed  \n",
      "51  In Progress Or Failed  \n",
      "52  In Progress Or Failed  \n",
      "53  In Progress Or Failed  \n",
      "54  In Progress Or Failed  \n",
      "56  In Progress Or Failed  \n",
      "25  In Progress Or Failed  \n",
      "57  In Progress Or Failed  \n",
      "58  In Progress Or Failed  \n",
      "59  In Progress Or Failed  \n",
      "60  In Progress Or Failed  \n",
      "61  In Progress Or Failed  \n",
      "62  In Progress Or Failed  \n",
      "63  In Progress Or Failed  \n",
      "64  In Progress Or Failed  \n",
      "46  In Progress Or Failed  \n",
      "45  In Progress Or Failed  \n",
      "44  In Progress Or Failed  \n",
      "36  In Progress Or Failed  \n",
      "26  In Progress Or Failed  \n",
      "27  In Progress Or Failed  \n",
      "28  In Progress Or Failed  \n",
      "29  In Progress Or Failed  \n",
      "43  In Progress Or Failed  \n",
      "65  In Progress Or Failed  \n",
      "37  In Progress Or Failed  \n",
      "38  In Progress Or Failed  \n",
      "39  In Progress Or Failed  \n",
      "40  In Progress Or Failed  \n",
      "41  In Progress Or Failed  \n",
      "42  In Progress Or Failed  \n",
      "35     0.7678244972577697  \n",
      "33     0.7678244972577697  \n",
      "32     0.7678244972577697  \n",
      "31     0.7678244972577697  \n",
      "30     0.7678244972577697  \n",
      "34     0.7678244972577697  , 'disaster_4':      trial_id  study_id  study_name         param_name  param_value  \\\n",
      "341        58         8  disaster_4       weight_decay     0.077944   \n",
      "227        35         8  disaster_4       weight_decay     0.026359   \n",
      "318        55         8  disaster_4         batch_size     0.000000   \n",
      "174        27         8  disaster_4         batch_size     0.000000   \n",
      "233        36         8  disaster_4       weight_decay     0.022332   \n",
      "..        ...       ...         ...                ...          ...   \n",
      "156        24         8  disaster_4         batch_size     0.000000   \n",
      "161        24         8  disaster_4       weight_decay     0.005657   \n",
      "160        24         8  disaster_4         num_epochs     5.000000   \n",
      "158        24         8  disaster_4      learning_rate     0.000001   \n",
      "159        24         8  disaster_4  lr_scheduler_type     0.000000   \n",
      "\n",
      "                     score  \n",
      "341  In Progress Or Failed  \n",
      "227  In Progress Or Failed  \n",
      "318  In Progress Or Failed  \n",
      "174  In Progress Or Failed  \n",
      "233  In Progress Or Failed  \n",
      "..                     ...  \n",
      "156     0.6165413533834587  \n",
      "161     0.6165413533834587  \n",
      "160     0.6165413533834587  \n",
      "158     0.6165413533834587  \n",
      "159     0.6165413533834587  \n",
      "\n",
      "[276 rows x 6 columns], 'disaster__':     trial_id  study_id  study_name         param_name  param_value  \\\n",
      "18        48        16  disaster__         batch_size     0.000000   \n",
      "19        48        16  disaster__       dropout_rate     0.102661   \n",
      "20        48        16  disaster__      learning_rate     0.000002   \n",
      "21        48        16  disaster__  lr_scheduler_type     3.000000   \n",
      "22        48        16  disaster__         num_epochs     2.000000   \n",
      "23        48        16  disaster__       weight_decay     0.058589   \n",
      "12        44        16  disaster__         batch_size     0.000000   \n",
      "13        44        16  disaster__       dropout_rate     0.267993   \n",
      "14        44        16  disaster__      learning_rate     0.000003   \n",
      "15        44        16  disaster__  lr_scheduler_type     3.000000   \n",
      "16        44        16  disaster__         num_epochs     2.000000   \n",
      "17        44        16  disaster__       weight_decay     0.055581   \n",
      "\n",
      "                    score  \n",
      "18  In Progress Or Failed  \n",
      "19  In Progress Or Failed  \n",
      "20  In Progress Or Failed  \n",
      "21  In Progress Or Failed  \n",
      "22  In Progress Or Failed  \n",
      "23  In Progress Or Failed  \n",
      "12     0.7692307692307692  \n",
      "13     0.7692307692307692  \n",
      "14     0.7692307692307692  \n",
      "15     0.7692307692307692  \n",
      "16     0.7692307692307692  \n",
      "17     0.7692307692307692  , 'disaster_third':      trial_id  study_id      study_name     param_name  param_value  \\\n",
      "342         1         1  disaster_third     batch_size     0.000000   \n",
      "343         1         1  disaster_third  learning_rate     0.000004   \n",
      "344         1         1  disaster_third     num_epochs     1.000000   \n",
      "\n",
      "                  score  \n",
      "342  0.7426326129666011  \n",
      "343  0.7426326129666011  \n",
      "344  0.7426326129666011  }\n"
     ]
    }
   ],
   "source": [
    "# Sort each DataFrame by the 'value' column in descending order\n",
    "sorted_dfs = {study_name: df.sort_values(by='score', ascending=False) for study_name, df in dfs.items()}\n",
    "print(sorted_dfs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DataFrame for Best Trials of Each Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'trial_number'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_72772\\1990414599.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbest_trial_dfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mstudy_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Sort the trials by their order of execution (trial_number)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mdf_sorted_by_trial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'trial_number'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Initialize the best value to a very small number\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mbest_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'inf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Micha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   6940\u001b[0m             )\n\u001b[0;32m   6941\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6942\u001b[0m             \u001b[1;31m# len(by) == 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6943\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6944\u001b[1;33m             \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6946\u001b[0m             \u001b[1;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6947\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Micha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1840\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1843\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1844\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'trial_number'"
     ]
    }
   ],
   "source": [
    "# Initialize a dictionary to hold the DataFrames of best trials for each study\n",
    "best_trial_dfs = {}\n",
    "\n",
    "for study_name, df in dfs.items():\n",
    "    # Sort the trials by their order of execution (trial_number)\n",
    "    df_sorted_by_trial = df.sort_values(by='trial_number')\n",
    "    \n",
    "    # Initialize the best value to a very small number\n",
    "    best_value = -float('inf')\n",
    "    \n",
    "    # List to keep track of rows that were best trials at their time\n",
    "    best_trials = []\n",
    "    \n",
    "    for _, row in df_sorted_by_trial.iterrows():\n",
    "        if row['value'] > best_value:\n",
    "            best_value = row['value']\n",
    "            best_trials.append(row)\n",
    "    \n",
    "    # Create a DataFrame from the best trials\n",
    "    best_trial_dfs[study_name] = pd.DataFrame(best_trials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_3_original_df = dfs[\"distributed_optimization_15\"]\n",
    "# study_3_sorted_df = sorted_dfs[3]\n",
    "# study_3_best_trials_df = best_trial_dfs[3]\n",
    "\n",
    "study_3_original_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_3_sorted_df = sorted_dfs[\"distributed_optimization_15\"]\n",
    "\n",
    "study_3_sorted_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_3_best_trials_df = best_trial_dfs[\"distributed_optimization_15\"]\n",
    "\n",
    "study_3_best_trials_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "trials = pd.read_csv(\"trials.csv\")\n",
    "trials.describe()\n",
    "\n",
    "# Group the DataFrame by 'study_id' and create a dictionary of DataFrames\n",
    "grouped = trials.groupby('study_name')\n",
    "dfs = {study_name: group for study_name, group in grouped}\n",
    "\n",
    "# Sort each DataFrame by the 'value' column in descending order\n",
    "sorted_dfs = {study_name: df.sort_values(by='value', ascending=True) for study_name, df in dfs.items()} # Make ascending = True if you want the lowest values at the top\n",
    "\n",
    "# Initialize a dictionary to hold the DataFrames of best trials for each study\n",
    "best_trial_dfs = {}\n",
    "\n",
    "for study_name, df in dfs.items():\n",
    "    # Sort the trials by their order of execution (trial_number)\n",
    "    df_sorted_by_trial = df.sort_values(by='trial_number')\n",
    "    \n",
    "    # Initialize the best value to a very small number\n",
    "    best_value = -float('inf')\n",
    "    \n",
    "    # List to keep track of rows that were best trials at their time\n",
    "    best_trials = []\n",
    "    \n",
    "    for _, row in df_sorted_by_trial.iterrows():\n",
    "        if row['value'] > best_value:\n",
    "            best_value = row['value']\n",
    "            best_trials.append(row)\n",
    "    \n",
    "    # Create a DataFrame from the best trials\n",
    "    best_trial_dfs[study_name] = pd.DataFrame(best_trials)\n",
    "\n",
    "\n",
    "study_3_original_df = dfs[\"distributed_optimization_15\"]\n",
    "# study_3_sorted_df = sorted_dfs[3]\n",
    "# study_3_best_trials_df = best_trial_dfs[3]\n",
    "\n",
    "study_3_original_df.head()\n",
    "\n",
    "study_3_sorted_df = sorted_dfs[\"distributed_optimization_15\"]\n",
    "\n",
    "study_3_sorted_df.head(20)\n",
    "\n",
    "study_3_best_trials_df = best_trial_dfs[\"distributed_optimization_15\"]\n",
    "\n",
    "study_3_best_trials_df.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
